{
  "blogPosts": [
    {
      "id": "/2022/08/17/helm-rollout",
      "metadata": {
        "permalink": "/blog/2022/08/17/helm-rollout",
        "editUrl": "https://github.com/kubevela/kubevela.io/tree/main/blog/2022-08-17-helm-rollout.md",
        "source": "@site/blog/2022-08-17-helm-rollout.md",
        "title": "The Canary Rollout of the Helm Chart Application Is Coming!",
        "description": "",
        "date": "2022-08-17T00:00:00.000Z",
        "formattedDate": "August 17, 2022",
        "tags": [
          {
            "label": "KubeVela",
            "permalink": "/blog/tags/kube-vela"
          },
          {
            "label": "use-case",
            "permalink": "/blog/tags/use-case"
          },
          {
            "label": "helm chart",
            "permalink": "/blog/tags/helm-chart"
          },
          {
            "label": "Canary Rollout",
            "permalink": "/blog/tags/canary-rollout"
          }
        ],
        "readingTime": 12.85,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Yike Wang",
            "title": "KubeVela Team",
            "url": "https://github.com/kubevela/kubevela",
            "imageURL": "https://KubeVela.io/img/logo.svg"
          }
        ],
        "frontMatter": {
          "title": "The Canary Rollout of the Helm Chart Application Is Coming!",
          "author": "Yike Wang",
          "author_title": "KubeVela Team",
          "author_url": "https://github.com/kubevela/kubevela",
          "author_image_url": "https://KubeVela.io/img/logo.svg",
          "tags": [
            "KubeVela",
            "use-case",
            "helm chart",
            "Canary Rollout"
          ],
          "description": "",
          "image": "https://raw.githubusercontent.com/oam-dev/KubeVela.io/main/docs/resources/KubeVela-03.png",
          "hide_table_of_contents": false
        },
        "nextItem": {
          "title": "How to migrate Existing Terraform Cloud Resources to KubeVela",
          "permalink": "/blog/2022/07/20/migrate-erraform-cloud-resources-to-kubeVela"
        }
      },
      "content": "## Background\r\n\r\nHelm is an application packaging and deployment tool of client side widely used in the cloud-native field. Its simple design and easy-to-use features have been recognized by users and formed its ecosystem. Up to now, thousands applications have been packaged using Helm Chart. Helm's design concept is very concise and can be summarized in the following two aspects:\r\n1.\tPackaging and templating complex Kubernetes APIs and then abstracting and simplifying them into small number of parameters\r\n\r\n2.\t**Giving Application Lifecycle Solutions:** Production, upload (hosting), versioning, distribution (discovery), and deployment.\r\n\r\n<!--truncate-->\r\n\r\nThese two design principles ensure that Helm is flexible and simple enough to cover all Kubernetes APIs, which solves the problem of one-off cloud-native application delivery. However, for enterprises with a certain scale, using Helm for continuous software delivery poses quite a challenge.\r\n## Challenges of the Continuous Delivery of Helm\r\nHelm was initially designed to ensure simplicity and ease of use instead of complex component orchestration. Therefore, **Helm delivers all resources to Kubernetes clusters during the application deployment. It is expected to solve application dependency and orchestration problems automatically using Kubernetes' final-state oriented self-healing capabilities.** Such a design may not be a problem during its first deployment, but it is too idealistic for the enterprise with a certain scale of production environment.\r\n\r\nOn the one hand, updating all resources when the application is upgraded may easily cause overall service interruption due to the short-term unavailability of some services. On the other hand, if there is a bug in the software, it cannot be rolled back in time, which may cause more trouble and make it difficult to control. In some serious scenarios, if some configurations in the production environment have been manually modified in O&M, the original modifications would be overwritten due to the one-off deployment of Helm. What's more, the previous versions of Helm may be inconsistent with the production environment, so it cannot be recovered using rollback. All of these add up to a larger area of failure.\r\n\r\n**Thus, with a certain scale, the grayscale and rollback capabilities of the software in the production environment are extremely important, and Helm itself cannot guarantee sufficient stability.**\r\n## How Do We Enable Canary-Rollout for Helm?\r\nTypically, a rigorous software upgrade process follows a similar process: It is roughly divided into three stages. The first stage upgrades small number of pods (such as 20% ) and switches a small amount of traffic to the new version. After completing this stage, the upgrade is paused first. After manual confirmation, continue the second phase, which is to upgrade a larger proportion of pods and traffic (such as 90% ), and pause again for manual confirmation. In the final stage, the full upgrade to the new version is completed, and the verification is completed, thus the entire rollout process is completed. If any exceptions, including business metrics, are found during the upgrade, such as an increase in the CPU or memory usage rate or excessive log requests error 500, you can roll back quickly.\r\n\r\n![image](/img/rollout-step.jpg)\r\n\r\nThe image above is a typical canary rollout scenario, *so how do we complete the above process for the Helm chart application?* There are two typical ways in the industry:\r\n1.\t**Modify the Helm chart to change workloads into two copies and expose different Helm parameters.** During the rollout, the images, number of pods, and traffic ratio of the two workloads are continuously modified to implement the canary rollout.\r\n2.\t**Modify the Helm chart to change the original basic workload to a custom workload with the same features and with phased rollout capabilities and expose the Helm parameters.** It's these canary rollout CRDs that are manipulated during the canary rollout.\r\n\r\nThe two solutions are complex with considerable modification costs, especially **when your Helm chart is a third-party component that cannot be modified or cannot maintain a Helm chart.** Even if the two are modified, there are still stability risks compared to the original simple workload model. The reason is that **Helm is only a package management tool, and it is incompatible with the canary rollout or workloads management.**\r\n\r\nWhen we have in-depth communication with large number of users in the community, we find that most users' applications are not complicated, among which the most are classic types (such as Deployment and StatefulSet). Therefore, through the powerful addon mechanism of [KubeVela](http://kubevela.net/) and the [OpenKruise](https://openkruise.io/) community, we have made a canary rollout KubeVela Addon for these qualified types. **This addon helps you easily complete the canary rollout of the Helm chart without any migration and modification.** Also, if your Helm chart is more complicated, you can customize an addon for your scenario to get the same experience.\r\nLet's take you through a practical example (using Deployment Workload as an example) to get a glimpse of the complete process.\r\n\r\n## Use KubeVela for Canary Rollout\r\n### Prepare the Environment\r\n\r\n- Install KubeVela\r\n\r\n```shell\r\n$ curl -fsSl https://static.kubevela.net/script/install-velad.sh | bash\r\nvelad install\r\n```\r\n\r\nSee [this document](https://kubevela.net/docs/install#1-install-velad) for more installation details.\r\n\r\n- Enable related addon\r\n\r\n```shell\r\n$ vela addon enable fluxcd\r\n$ vela addon enable ingress-nginx\r\n$ vela addon enable kruise-rollout\r\n$ vela addon enable velaux\r\n```\r\n\r\nIn this step, the following addons are started:\r\n1. The fluxcd addon helps us enable the capability of Helm delivery.\r\n2. The ingress-nginx addon is used to provide traffic management capabilities of canary rollout.\r\n3. The kruise-rollout provides canary rollout capability.\r\n4. The velaux addon provides interface operation and visualization.\r\n\r\n- Map the Nginx ingress-controller port to local\r\n\r\n```shell\r\n$ vela port-forward addon-ingress-nginx -n vela-system\r\n```\r\n\r\n### First Deployment\r\n\r\nRun the following command to deploy the Helm application for the first time. In this step, the deployment is done through KubeVela's CLI tool. If you are familiar with Kubernetes, you can also deploy through kubectl apply. The two work the same.\r\n\r\n```shell\r\ncat <<EOF | vela up -f -\r\napiVersion: core.oam.dev/v1beta1\r\nkind: Application\r\nmetadata:\r\n  name: canary-demo\r\n  annotations:\r\n    app.oam.dev/publishVersion: v1\r\nspec:\r\n  components:\r\n  - name: canary-demo\r\n    type: helm\r\n    properties:\r\n      repoType: \"helm\"\r\n      url: \"https://wangyikewxgm.github.io/my-charts/\"\r\n      chart: \"canary-demo\"\r\n      version: \"1.0.0\"\r\n    traits:\r\n    - type: kruise-rollout\r\n      properties:\r\n        canary:\r\n          # The first batch of Canary releases 20% Pods, and 20% traffic imported to the new version, require manual confirmation before subsequent releases are completed\r\n          steps:\r\n          - weight: 20\r\n          # The second batch of Canary releases 90% Pods, and 90% traffic imported to the new version.\r\n          - weight: 90\r\n          trafficRoutings:\r\n          - type: nginx\r\nEOF\r\n```\r\n\r\nIn the example above, we declare an application named canary-demo, which contains a Helm-type component (KubeVela also supports other types of component deployment), and the parameter of the component contains information (such as chart address and version).\r\n\r\nIn addition, we declare the kruise-rollout OAM trait for this component, which are the capabilities added to the component after the kruise-rollout addon is installed. The upgrade policy of Helm can be specified. In the first stage, 20% of pods and traffic are upgraded. After manual approve, 90% is upgraded. Finally, the full version is upgraded to the latest version.\r\n\r\n*Note:* We have [prepared a chart](https://github.com/wangyikewxgm/my-charts/tree/main/canary-demo) to demonstrate the intuitive effect (reflecting the version changes). The body of the Helm chart contains a Deployment and Ingress object, which is the most common scenario when the Helm chart is made. If your Helm chart is also equipped with the resources above, you can also use this example to canary rollout your helm chart.\r\n\r\nAfter the deployment is successful, we use the following command to access the gateway address in your cluster, and you will see the following result:\r\n\r\n```shell\r\n$ curl -H \"Host: canary-demo.com\" http://localhost:8080/version\r\nDemo: V1\r\n```\r\n\r\nIn addition, through the resource topology graph of VelaUX, we can see that the five V1 pods are all ready.\r\n\r\n![image](/img/helm-rollout-v1.jpg)\r\n\r\n### Upgrade an Application\r\nApply the following yaml to upgrade your application:\r\n\r\n```\r\ncat <<EOF | vela up -f -\r\napiVersion: core.oam.dev/v1beta1\r\nkind: Application\r\nmetadata:\r\n  name: canary-demo\r\n  annotations:\r\n    app.oam.dev/publishVersion: v2\r\nspec:\r\n  components:\r\n  - name: canary-demo\r\n    type: helm\r\n    properties:\r\n      repoType: \"helm\"\r\n      url: \"https://wangyikewxgm.github.io/my-charts/\"\r\n      chart: \"canary-demo\"\r\n      # Upgade to version 2.0.0\r\n      version: \"2.0.0\"\r\n    traits:\r\n    - type: kruise-rollout\r\n      properties:\r\n        canary:\r\n          # The first batch of Canary releases 20% Pods, and 20% traffic imported to the new version, require manual confirmation before subsequent releases are completed\r\n          steps:\r\n          - weight: 20\r\n          # The second batch of Canary releases 90% Pods, and 90% traffic imported to the new version.\r\n          - weight: 90\r\n          trafficRoutings:\r\n          - type: nginx\r\nEOF\r\n```\r\n\r\nWe noticed that the new application only has two changes compared to the first deployment:\r\n1.\tWe upgrade the annotation of the app.oam.dev/publishVersion from V1 to V2. This shows this revision is a new version.\r\n\r\n2.\tWe upgrade the version of the Helm chart to 2.0.0. The tag of the deployment image in this version of the chart is upgraded to V2.\r\n\r\nAfter a while, we will find that the upgrade process stops at the first batch we defined above. Only 20% of pods and traffic are upgraded. At this time, if you execute the command above to access the gateway multiple times, you will find that Demo: V1 and Demo: V2 appear alternately, and there is a probability of almost 20% of getting the result of Demo: V2.\r\n\r\n```shell\r\n$ curl -H \"Host: canary-demo.com\" http://localhost:8080/version\r\nDemo: V2\r\n```\r\n\r\nLooking at the topology status of the application resources again, you will see that the rollout CR created by the kruise-rollout trait has created a new version of the pod for us, while the five old version pods created by the previous workload remain unchanged.\r\n\r\n![image](/img/helm-rollout-v2.jpg)\r\n\r\nNext, we execute the following command through KubeVela's CLI to resume the upgrade through manual review:\r\n\r\n```shell\r\n$ vela workflow resume canary-demo\r\n```\r\n\r\nAfter a while, we saw that five new versions of pods were created through the resource topology diagram. At this time, if we visit the gateway again, we will find that the probability of Demo: V2 has increased significantly, which is close to 90%.\r\n### Quick Rollback\r\nGenerally, in the rollout of a real-world scenario, after a manual review, it is found that the status of the new version of the application is abnormal. You need to terminate the current upgrade and quickly roll back the application to the version before the upgrade starts.\r\n\r\nWe can execute the following command to suspend the current publishing workflow first:\r\n\r\n```shell\r\n$ vela workflow suspend canary-demo\r\nRollout default/canary-demo in cluster  suspended.\r\nSuccessfully suspend workflow: canary-demo\r\n```\r\n\r\nThen, roll back to the version before the rollout, which is V1:\r\n\r\n```shell\r\n$ vela workflow rollback canary-demo\r\nApplication spec rollback successfully.\r\nApplication status rollback successfully.\r\nRollout default/canary-demo in cluster  rollback.\r\nSuccessfully rollback rolloutApplication outdated revision cleaned up.\r\n```\r\n\r\nAt this time, when we access the gateway again, we will find that all the request results have returned to the V1 state:\r\n\r\n```shell\r\n$ curl -H \"Host: canary-demo.com\" http://localhost:8080/version\r\nDemo: V1\r\n```\r\n\r\nAt this time, we can see that all pods of the canary version have been deleted through the resource topology diagram, and from the beginning to the end, the five pods of V1, as the pods of the stable version, remain unchanged.\r\n\r\n![image](/img/helm-rollout-v1.jpg)\r\n\r\nIf you change the rollback operation above to resume the upgrade, the subsequent upgrade process will continue to complete the full rollout.\r\n\r\nPlease see [this link](https://kubevela.net/docs/tutorials/helm) for the complete operation procedure of the preceding demo.\r\n\r\nPlease refer to [this link](https://kubevela.net/docs/tutorials/k8s-object-rollout) if you want to directly use native Kubernetes resources to implement the process above.\r\n\r\nIn addition to Deployment, the kruise-rollout addon supports StatefulSet and OpenKruise CloneSet. If the workload types in your chart are one of the three types mentioned above, canary rollout can be implemented through the example above.\r\n\r\nI believe you can also notice that the example above is a nginx-Ingress-controller-based seven-layer traffic splitting scheme. In addition, we support [Kubernetes Gateway's API](https://gateway-api.sigs.k8s.io/) to support more gateway types and four-layer traffic splitting schemes.\r\n## How Can the Stability of the Rollout Be Guaranteed?\r\nAfter the first deployment, the kruise rollout addon (hereinafter referred to as rollout) listens to the resources deployed by the Helm chart, which are deployment, service, and ingress in the example. StatefulSet and OpenKruise Cloneset are also supported. The rollout will take over the subsequent upgrade actions of this deployment.\r\n\r\nDuring the upgrade, the new version of Helm takes effect first, and the deployment image is updated to V2. However, **the upgrade process of the deployment will be taken over by the rollout process from the controller-manager**, so the pods under the deployment will not be upgraded. At the same time, the rollout will copy a canary version of deployment, with V2 as the tag of the image, and create a service to filter the pods below it, together with an ingress pointing to this service. Finally, this ingress will receive the traffic of the canary version by setting the annotation corresponding to the ingress, thus enabling traffic splitting. For details, Please see [this link](https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#canary).\r\n\r\n**After all manual confirmation steps are passed, and the full rollout is completed, the rollout will return the upgrade control of deployment of the stable version to the controller-manager. At that time, the stable version of pods will be upgraded to the new version one after another. When the stable version pods are all ready, the canary version of deployment, service, and ingress will be destroyed one after another, thus ensuring that the request traffic will not hit the unready pod during the whole process nor cause abnormal requests.**\r\n\r\nAfter that, we will continue to iterate in the following areas to support more scenarios and bring a more stable and reliable upgrade experience:\r\n1.\tThe upgrade process is connected to KubeVela's workflow system, thus introducing a richer system of intermediate steps to expand the system and supporting functions (such as sending notifications through the workflow during the upgrade process). Even in the pause phase of each step, it can connect to the external observability system and automatically decide whether to continue publishing or roll back by checking indicators (such as logs or monitoring) to implement unattended publishing policies.\r\n2.\tIt integrates with more addons (such as istio) to support the traffic splitting solution of Service Mesh.\r\n3.\tIn addition to the percentage-based traffic splitting method, header-or cookie-based traffic splitting rules and features (such as blue-green publishing) are supported.\r\n\r\n## Summary\r\nAs mentioned earlier, Helm's canary rollout process is implemented by KubeVela through the addon system. fluxcd addon helps us deploy and manage the lifecycle of the Helm chart. The kruise-rollout addon helps us upgrade the workload pod and switch traffic during the upgrade process. By combining two addons, the full lifecycle management and canary upgrade of Helm applications are realized without any changes to your Helm chart. You can also [write addon](https://kubevela.io/docs/platform-engineers/addon/intro) for your scenarios to complete more special scenes or processes.\r\n\r\n**Based on KubeVela's powerful scalability, you can flexibly combine these addons and dynamically replace the underlying capabilities according to different platforms or environments without any changes to the upper-level applications.** For example, if you prefer to use argocd instead of fluxcd to implement the deployment of Helm applications, you can implement the same function by enabling the addon of argocd without any changes or migration at the upper-layer of Helm applications.\r\n\r\nNow, the KubeVela community has provided dozens of addons, which can help the platform expand its capabilities in observability, GitOps, FinOps, rollout, etc.\r\n\r\n![image](/img/addon-list.jpg)\r\n\r\nYou can find Addon's warehouse address [here](https://github.com/kubevela/catalog). If you are interested in addons, you are welcome to submit your custom addon and contribute new ecological capabilities to the community!"
    },
    {
      "id": "/2022/07/20/migrate-erraform-cloud-resources-to-kubeVela",
      "metadata": {
        "permalink": "/blog/2022/07/20/migrate-erraform-cloud-resources-to-kubeVela",
        "editUrl": "https://github.com/kubevela/kubevela.io/tree/main/blog/2022-07-20-migrate-erraform-cloud-resources-to-kubeVela.md",
        "source": "@site/blog/2022-07-20-migrate-erraform-cloud-resources-to-kubeVela.md",
        "title": "How to migrate Existing Terraform Cloud Resources to KubeVela",
        "description": "This blog discusses how to migrate existing Terraform cloud resources to KubeVela.",
        "date": "2022-07-20T00:00:00.000Z",
        "formattedDate": "July 20, 2022",
        "tags": [
          {
            "label": "KubeVela",
            "permalink": "/blog/tags/kube-vela"
          },
          {
            "label": "Terraform",
            "permalink": "/blog/tags/terraform"
          },
          {
            "label": "Kubernetes",
            "permalink": "/blog/tags/kubernetes"
          },
          {
            "label": "DevOps",
            "permalink": "/blog/tags/dev-ops"
          },
          {
            "label": "CNCF",
            "permalink": "/blog/tags/cncf"
          },
          {
            "label": "CI/CD",
            "permalink": "/blog/tags/ci-cd"
          },
          {
            "label": "Application delivery",
            "permalink": "/blog/tags/application-delivery"
          }
        ],
        "readingTime": 3.025,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Nan Li",
            "url": "https://github.com/loheagn",
            "imageURL": "https://avatars.githubusercontent.com/u/33423736"
          }
        ],
        "frontMatter": {
          "title": "How to migrate Existing Terraform Cloud Resources to KubeVela",
          "author": "Nan Li",
          "author_url": "https://github.com/loheagn",
          "author_image_url": "https://avatars.githubusercontent.com/u/33423736",
          "tags": [
            "KubeVela",
            "Terraform",
            "Kubernetes",
            "DevOps",
            "CNCF",
            "CI/CD",
            "Application delivery"
          ],
          "description": "This blog discusses how to migrate existing Terraform cloud resources to KubeVela.",
          "image": "https://raw.githubusercontent.com/oam-dev/KubeVela.io/main/docs/resources/KubeVela-03.png",
          "hide_table_of_contents": false
        },
        "prevItem": {
          "title": "The Canary Rollout of the Helm Chart Application Is Coming!",
          "permalink": "/blog/2022/08/17/helm-rollout"
        },
        "nextItem": {
          "title": "Helm Chart Delivery in Multi-Cluster",
          "permalink": "/blog/2022/07/07/helm-multi-cluster"
        }
      },
      "content": "You may have learned from [this blog](2022-06-27-terraform-integrate-with-vela.md) that we can use vela to manage cloud resources (like s3 bucket, AWS EIP and so on) via the terraform plugin. We can create an application which contains some cloud resource components and this application will generate these cloud resources, then we can use vela to manage them. \r\n\r\nSometimes we already have some Terraform cloud resources which may be created and managed by the Terraform binary or something else. In order to have [the benefits of using KubeVela to manage the cloud resources](2022-06-27-terraform-integrate-with-vela.md#part-1-glue-terraform-module-as-kubevela-capability) or just maintain consistency in the way you manage cloud resources, we may want to import these existing Terraform cloud resources into KubeVela and use vela to manage them. But if we just create an application which describes these cloud resources, the cloud resources will be recreated and may lead to errors. To fix this problem, we made [a simple `backup_restore` tool](https://github.com/kubevela/terraform-controller/tree/master/hack/tool/backup_restore). This blog will show you how to use the `backup_restore` tool to import your existing Terraform cloud resources into KubeVela.\r\n\r\n## Step 1: Create Terraform Cloud Resources\r\n\r\nSince we are going to demonstrate how to import an existing cloud resource into KubeVela, we need to create one first. If you already have such resources, you can skip this step.\r\n\r\nBefore start, make sure you have:\r\n\r\n- Installed [terraform CLI](https://www.terraform.io/downloads).\r\n- Have a Cloud Service credentials, in this article, we will use aws as example.\r\n- Learn the basic knowledge of [how to use terraform](https://www.terraform.io/language).\r\n\r\n<!--truncate-->\r\n\r\nLet's get started!\r\n\r\n1. Create an empty directory to start.\r\n\r\n    ```shell\r\n    mkdir -p cloud-resources\r\n    cd cloud-resources\r\n    ```\r\n\r\n2. Create a file named `main.tf` which will create a S3 bucket:\r\n\r\n    ```hcl\r\n    resource \"aws_s3_bucket\" \"bucket-acl\" {\r\n        bucket = var.bucket\r\n        acl    = var.acl\r\n    }\r\n\r\n    output \"RESOURCE_IDENTIFIER\" {\r\n        description = \"The identifier of the resource\"\r\n        value       = aws_s3_bucket.bucket-acl.bucket_domain_name\r\n    }\r\n\r\n    output \"BUCKET_NAME\" {\r\n        value       = aws_s3_bucket.bucket-acl.bucket_domain_name\r\n        description = \"The name of the S3 bucket\"\r\n    }\r\n\r\n    variable \"bucket\" {\r\n        description = \"S3 bucket name\"\r\n        default     = \"vela-website\"\r\n        type        = string\r\n    }\r\n\r\n    variable \"acl\" {\r\n        description = \"S3 bucket ACL\"\r\n        default     = \"private\"\r\n        type        = string\r\n    }\r\n    ```\r\n\r\n3. Configure the AWS Cloud provider credentials:\r\n\r\n    ```shell\r\n    export AWS_ACCESS_KEY_ID=\"your-accesskey-id\"\r\n    export AWS_SECRET_ACCESS_KEY=\"your-accesskey-secret\"\r\n    export AWS_DEFAULT_REGION=\"your-region-id\"\r\n    ```\r\n\r\n4. Set the variables in the `main.tf` file:\r\n\r\n    ```shell\r\n    export TF_VAR_acl=\"private\"; export TF_VAR_bucket=\"your-bucket-name\"\r\n    ```\r\n\r\n5. (Optional) Create a `backend.tf` to configure your Terraform backend. We just use the default local backend in this example.\r\n\r\n6. Run `terraform init` and `terraform apply` to create the S3 bucket:\r\n\r\n    ```shell\r\n    terraform init && terraform apply\r\n    ```\r\n\r\n7. Check the S3 bucket list to make sure the bucket is created successfully.\r\n\r\n8. Run `terraform state pull` to get the Terraform state of the cloud resource and store it into a local file:\r\n\r\n    ```shell\r\n    terraform state pull > state.json\r\n    ```\r\n\r\n## Step 2: Import Existing Terraform Cloud Resources into KubeVela\r\n\r\n1. Create the `application.yaml` file, please ensure that the description of each field of Component is consistent with your cloud resource configuration:\r\n\r\n    ```yaml\r\n    apiVersion: core.oam.dev/v1beta1\r\n    kind: Application\r\n    metadata:\r\n    name: app-aws-s3\r\n    spec:\r\n    components:\r\n        - name: sample-s3\r\n        type: aws-s3\r\n        properties:\r\n            bucket: vela-website-202110191745\r\n            acl: private\r\n            writeConnectionSecretToRef:\r\n            name: s3-conn\r\n    ```\r\n\r\n2. Get the [`backup_restore` tool](https://github.com/kubevela/terraform-controller/tree/master/hack/tool/backup_restore):\r\n\r\n    ```shell\r\n    git clone https://github.com/kubevela/terraform-controller.git\r\n    cd terraform-controller/hack/tool/backup_restore\r\n    ```\r\n\r\n3. Run the `restore` command:\r\n    ```shell\r\n    go run main.go restore --application <path/to/your/application.yaml> --component sample-s3 --state <path/to/your/state.json>\r\n    ```\r\n    The above command will resume the Terraform backend in the Kubernetes first and then create the application without recreating the S3 bucket.\r\n\r\nThat's all! You have successfully migrate the management of the S3 bucket to KubeVela!\r\n\r\n## What's more\r\n\r\nFor more information about the `backup_restore` tool, please read [the doc](https://github.com/kubevela/terraform-controller/blob/master/hack/tool/backup_restore/README.md). If you have any problem, issues and pull requests are always welcome."
    },
    {
      "id": "/2022/07/07/helm-multi-cluster",
      "metadata": {
        "permalink": "/blog/2022/07/07/helm-multi-cluster",
        "editUrl": "https://github.com/kubevela/kubevela.io/tree/main/blog/2022-07-07-helm-multi-cluster.md",
        "source": "@site/blog/2022-07-07-helm-multi-cluster.md",
        "title": "Helm Chart Delivery in Multi-Cluster",
        "description": "",
        "date": "2022-07-07T00:00:00.000Z",
        "formattedDate": "July 7, 2022",
        "tags": [
          {
            "label": "KubeVela",
            "permalink": "/blog/tags/kube-vela"
          },
          {
            "label": "use-case",
            "permalink": "/blog/tags/use-case"
          },
          {
            "label": "helm chart",
            "permalink": "/blog/tags/helm-chart"
          },
          {
            "label": "multi-cluster",
            "permalink": "/blog/tags/multi-cluster"
          }
        ],
        "readingTime": 7.705,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Jianbo Sun",
            "title": "KubeVela Team",
            "url": "https://github.com/kubevela/kubevela",
            "imageURL": "https://KubeVela.io/img/logo.svg"
          }
        ],
        "frontMatter": {
          "title": "Helm Chart Delivery in Multi-Cluster",
          "author": "Jianbo Sun",
          "author_title": "KubeVela Team",
          "author_url": "https://github.com/kubevela/kubevela",
          "author_image_url": "https://KubeVela.io/img/logo.svg",
          "tags": [
            "KubeVela",
            "use-case",
            "helm chart",
            "multi-cluster"
          ],
          "description": "",
          "image": "https://raw.githubusercontent.com/oam-dev/KubeVela.io/main/docs/resources/KubeVela-03.png",
          "hide_table_of_contents": false
        },
        "prevItem": {
          "title": "How to migrate Existing Terraform Cloud Resources to KubeVela",
          "permalink": "/blog/2022/07/20/migrate-erraform-cloud-resources-to-kubeVela"
        },
        "nextItem": {
          "title": "Glue Terraform Ecosystem into Kubernetes World",
          "permalink": "/blog/2022/06/27/terraform-integrate-with-vela"
        }
      },
      "content": "[Helm Charts](https://artifacthub.io/packages/search?kind=0) are very popular that you can find almost 10K different software packaged in this way. While in today's multi-cluster/hybrid cloud business environment, we often encounter these typical requirements: distribute to multiple specific clusters, specific group distributions according to business need, and differentiated configurations for multi-clusters.\r\n\r\nIn this blog, we'll introduce how to use [KubeVela](https://kubevela.io/) to do multi cluster delivery for Helm Charts.\r\n\r\nIf you don't have multi clusters, don't worry, we'll introduce from scratch with only Docker or Linux System required. You can also refer to the [basic helm chart delivery](https://kubevela.io/docs/tutorials/helm) in single cluster.\r\n\r\n\r\n<!--truncate-->\r\n\r\n## Prerequisites\r\n\r\n* Docker v20.10.5+ (runc >= v1.0.0-rc93), or Linux system\r\n* [VelaD](https://github.com/kubevela/velad), a lightweight deployment tool to set up KubeVela with Kubernetes.\r\n\r\n## Prepare Clusters\r\n\r\n> This section is preparation for multi-cluster, we will start from scratch for convenience. if you're already KubeVela users and have [multi-clusters joined](https://kubevela.io/docs/platform-engineers/system-operation/managing-clusters), you can skip this section.\r\n\r\n1. Install KubeVela control plane\r\n\r\n```shell\r\nvelad install\r\n```\r\n\r\n2. Export the KubeConfig for the newly created cluster\r\n\r\n```\r\nexport KUBECONFIG=$(velad kubeconfig --name default --host)\r\n```\r\n\r\nNow you have successfully installed KubeVela. You can join your cluster to kubevela by:\r\n\r\n```\r\nvela cluster join <path-to-kubeconfig-of-cluster> --name foo\r\n```\r\n\r\nVelaD can also provide K3s clusters for convenience.\r\n\r\n3. Create and Join a cluster created by velad named `foo`\r\n\r\n```shell\r\nvelad install --name foo --cluster-only\r\nvela cluster join $(velad kubeconfig --name foo --internal) --name foo\r\n```\r\n\r\nAs a fully extensible control plane, most of KubeVela's capabilities are pluggable. The following steps will guide you to install some addons for different capabilities.\r\n\r\n4. Enable velaux addon, it will provide UI console for KubeVela\r\n\r\n```shell\r\nvela addon enable velaux\r\n```\r\n\r\n5. Enable fluxcd addon for helm component delivery\r\n\r\n```shell\r\nvela addon enable fluxcd\r\n```\r\n\r\nIf you have already enabled the `fluxcd` addon before you joined the new cluster, you NEED to enable the addon for the newly joined cluster by:\r\n\r\n```\r\nvela addon enable fluxcd --clusters foo\r\n```\r\n\r\nFinally, we have finished all preparation, you can check the clusters joined:\r\n\r\n```console\r\n$ vela cluster ls\r\nCLUSTER\tALIAS\tTYPE           \tENDPOINT               \tACCEPTED\tLABELS\r\nlocal  \t     \tInternal       \t-                      \ttrue\r\nfoo    \t     \tX509Certificate\thttps://172.20.0.6:6443\ttrue\r\n```\r\n\r\nOne cluster named `local` is the KubeVela control plane, another one named `foo` is the cluster we just joined.\r\n\r\n## Deploy across multi clusters\r\n\r\nWe can use `topology` policy to specify the delivery topology for helm chart like the following command:\r\n\r\n```yaml\r\ncat <<EOF | vela up -f -\r\napiVersion: core.oam.dev/v1beta1\r\nkind: Application\r\nmetadata:\r\n  name: helm-hello\r\nspec:\r\n  components:\r\n    - name: hello\r\n      type: helm\r\n      properties:\r\n        repoType: \"helm\"\r\n        url: \"https://jhidalgo3.github.io/helm-charts/\"\r\n        chart: \"hello-kubernetes-chart\"\r\n        version: \"3.0.0\"\r\n  policies:\r\n    - name: foo-cluster-only\r\n      type: topology\r\n      properties:\r\n        clusters: [\"foo\"]\r\nEOF\r\n```\r\n\r\nThe `clusters` field of topology policy is a slice, you can specify multiple cluster names here.\r\nYou can also use label selector or specify namespace with that, refer to the [reference docs](https://kubevela.io/docs/end-user/policies/references#topology) for more details. \r\n\r\nAfter deployed, you can check the deployed application by:\r\n\r\n```shell\r\nvela status helm-hello\r\n```\r\n\r\nThe expected output should be as follows if deployed successfully:\r\n\r\n```console\r\nAbout:\r\n\r\n  Name:      \thelm-hello\r\n  Namespace: \tdefault\r\n  Created at:\t2022-06-09 19:14:57 +0800 CST\r\n  Status:    \trunning\r\n\r\nWorkflow:\r\n\r\n  mode: DAG\r\n  finished: true\r\n  Suspend: false\r\n  Terminated: false\r\n  Steps\r\n  - id:vtahj5zrz4\r\n    name:deploy-foo-cluster-only\r\n    type:deploy\r\n    phase:succeeded\r\n    message:\r\n\r\nServices:\r\n\r\n  - Name: hello\r\n    Cluster: foo  Namespace: default\r\n    Type: helm\r\n    Healthy Fetch repository successfully, Create helm release successfully\r\n    No trait applied\r\n```\r\n\r\nYou can check the deployed resource by:\r\n\r\n```\r\n$ vela status helm-hello --tree\r\nCLUSTER       NAMESPACE     RESOURCE             STATUS\r\nfoo       ─── default   ─┬─ HelmRelease/hello    updated\r\n                         └─ HelmRepository/hello updated\r\n```\r\n\r\nYou can also check the deployed resource by VelaUX.\r\n\r\n\r\n## Check Resources from UI console\r\n\r\nBy using the `velaux` UI console, you can get even more information with a unified experience for multi clusters. You can refer to [this doc](https://kubevela.io/docs/install#2-install-velaux) to learn how to visit VelaUX.\r\n\r\nWith the help of UI, you can:\r\n\r\n* Check pod status and event from different clusters:\r\n\r\n![resource-detail](/img/helm/helm-pod.jpg)\r\n\r\n* Check pod logs from different clusters:\r\n\r\n![resource-detail](/img/helm/helm-logs.jpg)\r\n\r\n* Check resource topology and status:\r\n\r\n![resource-detail](/img/helm/helm-topology.jpg)\r\n\r\n\r\n## Deploy with override configurations\r\n\r\nIn some cases, we will deploy helm chart into different clusters with different values, then we can use the [override policy](https://kubevela.io/docs/end-user/policies/references#override).\r\n\r\nBelow is a complex example that we will deploy one helm chart into two clusters and specify different values for each cluster. Let's deploy it:\r\n\r\n```shell\r\ncat <<EOF | vela up -f -\r\napiVersion: core.oam.dev/v1beta1\r\nkind: Application\r\nmetadata:\r\n  name: helm-hello\r\nspec:\r\n  components:\r\n    - name: hello\r\n      type: helm\r\n      properties:\r\n        repoType: \"helm\"\r\n        url: \"https://jhidalgo3.github.io/helm-charts/\"\r\n        chart: \"hello-kubernetes-chart\"\r\n        version: \"3.0.0\"\r\n  policies:\r\n    - name: topology-local\r\n      type: topology\r\n      properties:\r\n        clusters: [\"local\"]\r\n    - name: topology-foo\r\n      type: topology\r\n      properties:\r\n        clusters: [\"foo\"]\r\n    - name: override-local\r\n      type: override\r\n      properties:\r\n        components:\r\n          - name: hello\r\n            properties:\r\n              values:\r\n                configs:\r\n                  MESSAGE: Welcome to Control Plane Cluster!\r\n    - name: override-foo\r\n      type: override\r\n      properties:\r\n        components:\r\n          - name: hello\r\n            properties:\r\n              values:\r\n                configs:\r\n                  MESSAGE: Welcome to Your New Foo Cluster!\r\n  workflow:\r\n    steps:\r\n      - name: deploy2local\r\n        type: deploy\r\n        properties:\r\n          policies: [\"topology-local\", \"override-local\"]\r\n      - name: manual-approval\r\n        type: suspend\r\n      - name: deploy2foo\r\n        type: deploy\r\n        properties:\r\n          policies: [\"topology-foo\", \"override-foo\"]\r\nEOF\r\n```\r\n\r\n> **Note: If you feel the policy and workflow is a bit complex, you can make them as an external object and just reference the object, the usage is the same with the [container delivery](https://kubevela.io/docs/case-studies/multi-cluster#use-policies-and-workflow-outside-the-application).**\r\n\r\nThe deploy process has three steps: \r\n\r\n- 1) deploy to local cluster;\r\n- 2) wait for manual approval;\r\n- 3) deploy to foo cluster.\r\n\r\nSo you will find it was suspended after the first step, just like follows:\r\n\r\n```\r\n$ vela status helm-hello\r\nAbout:\r\n\r\n  Name:      \thelm-hello\r\n  Namespace: \tdefault\r\n  Created at:\t2022-06-09 19:38:13 +0800 CST\r\n  Status:    \tworkflowSuspending\r\n\r\nWorkflow:\r\n\r\n  mode: StepByStep\r\n  finished: false\r\n  Suspend: true\r\n  Terminated: false\r\n  Steps\r\n  - id:ww4cydlvee\r\n    name:deploy2local\r\n    type:deploy\r\n    phase:succeeded\r\n    message:\r\n  - id:xj6hu97e1e\r\n    name:manual-approval\r\n    type:suspend\r\n    phase:succeeded\r\n    message:\r\n\r\nServices:\r\n\r\n  - Name: hello\r\n    Cluster: local  Namespace: default\r\n    Type: helm\r\n    Healthy Fetch repository successfully, Create helm release successfully\r\n    No trait applied\r\n```\r\n\r\nYou can check the helm chart deployed in control plane with the value \"Welcome to Control Plane Cluster!\".\r\n\r\n```\r\nvela port-forward helm-hello\r\n```\r\n\r\nIt will automatically prompt with your browser with the following page:\r\n\r\n![resource-detail](/img/helm/helm-c1.jpg)\r\n\r\nLet's continue the workflow as we have checked the deployment has succeeded.\r\n\r\n```shell\r\nvela workflow resume helm-hello\r\n```\r\n\r\nThen it will deploy to the foo cluster, you can check the resources with detailed information:\r\n\r\n```console\r\n$ vela status helm-hello --tree --detail\r\nCLUSTER       NAMESPACE     RESOURCE             STATUS    APPLY_TIME          DETAIL\r\nfoo       ─── default   ─┬─ HelmRelease/hello    updated   2022-06-09 19:38:13 Ready: True  Status: Release reconciliation succeeded  Age: 64s\r\n                         └─ HelmRepository/hello updated   2022-06-09 19:38:13 URL: https://jhidalgo3.github.io/helm-charts/  Age: 64s  Ready: True\r\n                                                                               Status: stored artifact for revision 'ab876069f02d779cb4b63587af1266464818ba3790c0ccd50337e3cdead44803'\r\nlocal     ─── default   ─┬─ HelmRelease/hello    updated   2022-06-09 19:38:13 Ready: True  Status: Release reconciliation succeeded  Age: 7m34s\r\n                         └─ HelmRepository/hello updated   2022-06-09 19:38:13 URL: https://jhidalgo3.github.io/helm-charts/  Age: 7m34s  Ready: True\r\n                                                                               Status: stored artifact for revision 'ab876069f02d779cb4b63587af1266464818ba3790c0ccd50337e3cdead44803'\r\n```\r\n\r\nUse port forward again:\r\n\r\n```shell\r\nvela port-forward helm-hello\r\n```\r\n\r\nThen it will prompt some selections:\r\n\r\n```\r\n? You have 2 deployed resources in your app. Please choose one:  [Use arrows to move, type to filter]\r\n> Cluster: foo | Namespace: default | Kind: HelmRelease | Name: hello\r\n  Cluster: local | Namespace: default | Kind: HelmRelease | Name: hello\r\n```\r\n\r\nChoose the option with cluster `foo`, then you'll see the result that has was overridden with new message.\r\n\r\n```console\r\n$ curl http://127.0.0.1:8080/\r\n...snip...\r\n      <div id=\"message\">\r\n  Welcome to Your New Foo Cluster!\r\n</div>\r\n...snip...\r\n```\r\n\r\n## Specify different value file for different environment\r\n\r\nYou can choose different value file present in a helm chart for different environment. eg:\r\n\r\nPlease make sure your local cluster have two namespaces \"test\" and \"prod\" which represent two environments in our example.\r\n\r\nWe use the chart `hello-kubernetes-chart` as an example.This chart has two values files. You can pull this chart and have a look all contains files in it:\r\n\r\n```yaml\r\n$ tree ./hello-kubernetes-chart\r\n./hello-kubernetes-chart\r\n├── Chart.yaml\r\n├── templates\r\n│ ├── NOTES.txt\r\n│ ├── _helpers.tpl\r\n│ ├── config-map.yaml\r\n│ ├── deployment.yaml\r\n│ ├── hpa.yaml\r\n│ ├── ingress.yaml\r\n│ ├── service.yaml\r\n│ ├── serviceaccount.yaml\r\n│ └── tests\r\n│ └── test-connection.yaml\r\n├── values-production.yaml\r\n└── values.yaml\r\n```\r\n\r\nAs we can see, there are values files `values.yaml` `values-production.yaml` in this chart.\r\n\r\n```yaml\r\ncat <<EOF | vela up -f -\r\napiVersion: core.oam.dev/v1beta1\r\nkind: Application\r\nmetadata:\r\n  name: hello-kubernetes\r\nspec:\r\n  components:\r\n    - name: hello-kubernetes\r\n      type: helm\r\n      properties:\r\n        repoType: \"helm\"\r\n        url: \"https://wangyikewxgm.github.io/my-charts/\"\r\n        chart: \"hello-kubernetes-chart\"\r\n        version: \"0.1.0\"\r\n\r\n  policies:\r\n    - name: topology-test\r\n      type: topology\r\n      properties:\r\n        clusters: [\"local\"]\r\n        namespace: \"test\"\r\n    - name: topology-prod\r\n      type: topology\r\n      properties:\r\n        clusters: [\"local\"]\r\n        namespace: \"prod\"\r\n    - name: override-prod\r\n      type: override\r\n      properties:\r\n        components:\r\n          - name: hello-kubernetes\r\n            properties:\r\n              valuesFiles:\r\n                - \"values-production.yaml\"\r\n  workflow:\r\n    steps:\r\n      - name: deploy2test\r\n        type: deploy\r\n        properties:\r\n          policies: [\"topology-test\"]\r\n      - name: deploy2prod\r\n        type: deploy\r\n        properties:\r\n          policies: [\"topology-prod\", \"override-prod\"]  \r\nEOF\r\n```\r\n\r\nAccess the endpoints of application:\r\n\r\n```yaml\r\nvela port-forward hello-kubernetes\r\n```\r\n\r\nIf you choose ```Cluster: local | Namespace: test | Kind: HelmRelease | Name: hello-kubernetes``` you will see:\r\n\r\n![image](/img/helm/helm-files-test.jpg)\r\n\r\nIf you choose ```Cluster: local | Namespace: prod | Kind: HelmRelease | Name: hello-kubernetes``` you will see:\r\n\r\n![image](/img/helm/helm-files-prod.jpg)\r\n\r\n## Clean up\r\n\r\nIf you're using velad for this demo, you can clean up very easily by:\r\n\r\n* Clean up the foo cluster\r\n```\r\nvelad uninstall -n foo\r\n```\r\n\r\n* Clean up the default cluster\r\n```\r\nvelad uninstall\r\n```\r\n\r\n## What's More?\r\n\r\nWith the help of KubeVela and its addon, you can get the capability of [Canary Rollout](https://kubevela.io/docs/tutorials/helm-rollout) for your helm charts!\r\n\r\nGo and ship Helm chart with KubeVela, makes deploying and operating applications across today's hybrid, multi-cloud environments easier, faster and more reliable."
    },
    {
      "id": "/2022/06/27/terraform-integrate-with-vela",
      "metadata": {
        "permalink": "/blog/2022/06/27/terraform-integrate-with-vela",
        "editUrl": "https://github.com/kubevela/kubevela.io/tree/main/blog/2022-06-27-terraform-integrate-with-vela.md",
        "source": "@site/blog/2022-06-27-terraform-integrate-with-vela.md",
        "title": "Glue Terraform Ecosystem into Kubernetes World",
        "description": "This article discusses how to Integrate terraform ecosystem with KubeVela.",
        "date": "2022-06-27T00:00:00.000Z",
        "formattedDate": "June 27, 2022",
        "tags": [
          {
            "label": "KubeVela",
            "permalink": "/blog/tags/kube-vela"
          },
          {
            "label": "Terraform",
            "permalink": "/blog/tags/terraform"
          },
          {
            "label": "Kubernetes",
            "permalink": "/blog/tags/kubernetes"
          },
          {
            "label": "DevOps",
            "permalink": "/blog/tags/dev-ops"
          },
          {
            "label": "CNCF",
            "permalink": "/blog/tags/cncf"
          },
          {
            "label": "CI/CD",
            "permalink": "/blog/tags/ci-cd"
          },
          {
            "label": "Application delivery",
            "permalink": "/blog/tags/application-delivery"
          }
        ],
        "readingTime": 7.88,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Jianbo Sun",
            "title": "KubeVela Team",
            "url": "https://github.com/kubevela/KubeVela",
            "imageURL": "https://KubeVela.io/img/logo.svg"
          }
        ],
        "frontMatter": {
          "title": "Glue Terraform Ecosystem into Kubernetes World",
          "author": "Jianbo Sun",
          "author_title": "KubeVela Team",
          "author_url": "https://github.com/kubevela/KubeVela",
          "author_image_url": "https://KubeVela.io/img/logo.svg",
          "tags": [
            "KubeVela",
            "Terraform",
            "Kubernetes",
            "DevOps",
            "CNCF",
            "CI/CD",
            "Application delivery"
          ],
          "description": "This article discusses how to Integrate terraform ecosystem with KubeVela.",
          "image": "https://raw.githubusercontent.com/oam-dev/KubeVela.io/main/docs/resources/KubeVela-03.png",
          "hide_table_of_contents": false
        },
        "prevItem": {
          "title": "Helm Chart Delivery in Multi-Cluster",
          "permalink": "/blog/2022/07/07/helm-multi-cluster"
        },
        "nextItem": {
          "title": "KubeVela 1.4 released, Make Application Delivery Safe, Foolproof, and Transparent",
          "permalink": "/blog/2022/06/21/release-1.4"
        }
      },
      "content": "If you're looking for something to glue Terraform ecosystem with the Kubernetes world, congratulations! You're getting exactly what you want in this blog.\r\n\r\nWe will introduce how to integrate terraform modules into KubeVela by fixing a real world problem -- \"Fixing the Developer Experience of Kubernetes Port Forwarding\" inspired by [article](https://inlets.dev/blog/2022/06/24/fixing-kubectl-port-forward.html) from Alex Ellis.\r\n\r\nIn general, this article will be divided into two parts:\r\n\r\n* Part.1 will introduce how to glue Terraform with KubeVela, it needs some basic knowledge of both Terraform and KubeVela. You can just skip this part if you don't want to extend KubeVela as a Developer.\r\n* Part.2 will introduce how KubeVela can 1) provision a Cloud ECS instance by KubeVela with public IP; 2) Use the ECS instance as a tunnel sever to provide public access for any container service within an intranet environment.\r\n\r\nOK, let's go!\r\n\r\n<!--truncate-->\r\n\r\n## Part 1. Glue Terraform Module as KubeVela Capability\r\n\r\nIn general, [KubeVela](https://kubevela.net/docs/) is a modern software delivery control plane, you may ask: \"What benefit from doing this\":\r\n\r\n1. The power of gluing Terraform with Kubernetes ecosystem including Helm Charts in one unified solution, that helps you to do GitOps, CI/CD integration and application lifecycle management.\r\n    - Thinking of deploy a product that includes Cloud Database, Container Service and several helm charts, now you can manage and deploy them together without switching to different tools.\r\n2. Declarative model for all the resources, KubeVela will run the reconcile loop until succeed.\r\n    - You won't be blocked by the network issues from terraform CLI.\r\n3. A powerful CUE based workflow that you can define any preferred steps in the application delivery process.\r\n    - You can compose the way you like, such as canary rollout, multi-clusters/multi-env promotion, notification.\r\n\r\nIf you're already a good hand of terraform, it's pretty easy for this integration.\r\n\r\n### Build your terraform module\r\n\r\n> This part can usually be skipped, if you already have a well-tested terraform module, \r\n\r\nBefore start, make sure you have:\r\n\r\n- Installed [terraform CLI](https://www.terraform.io/downloads).\r\n- Have a Cloud Service credentials, in this article, we will use Alibaba Cloud as example.\r\n- Learn the basic knowledge of [how to use terraform](https://www.terraform.io/language).\r\n\r\nHere's my terraform module( https://github.com/wonderflow/terraform-alicloud-ecs-instance ) for this demo.\r\n\r\n* Clone this module:\r\n\r\n```\r\ngit clone https://github.com/wonderflow/terraform-alicloud-ecs-instance.git\r\ncd terraform-alicloud-ecs-instance\r\n```\r\n\r\n* Initialize and download the latest stable version of the Alibaba Cloud provider:\r\n```shell\r\nterraform init\r\n```\r\n\r\n* Configure the Alibaba Cloud provider credentials:\r\n\r\n```shell\r\nexport ALICLOUD_ACCESS_KEY=\"your-accesskey-id\"\r\nexport ALICLOUD_SECRET_KEY=\"your-accesskey-secret\"\r\nexport ALICLOUD_REGION=\"your-region-id\"\r\n```\r\n\r\nYou can also create an `provider.tf` including the credentials instead:\r\n\r\n```hcl\r\nprovider \"alicloud\" {\r\n    access_key  = \"your-accesskey-id\"\r\n    secret_key   = \"your-accesskey-secret\"\r\n    region           = \"cn-hangzhou\"\r\n}\r\n```\r\n\r\n* Test creating the resources:\r\n```shell\r\nterraform apply -var-file=test/test.tfvars\r\n```\r\n\r\n* Destroy all resources of tests:\r\n\r\n```shell\r\nterraform destroy  -var-file=test/test.tfvars\r\n```\r\n\r\nYou can customize this module per your needs and push to github of your own.\r\n\r\n\r\n### Make the Terraform module as KubeVela Capability\r\n\r\nBefore start, make sure you have [installed kubevela control plane](https://kubevela.net/docs/install#1-install-velad), don't worry if you don't have Kubernetes cluster, velad is enough for the quick demo.\r\n\r\nWe'll use the terraform module we have already prepared just now.\r\n\r\n* Generate Component Definition\r\n\r\n```\r\nvela def init ecs --type component --provider alibaba --desc \"Terraform configuration for Alibaba Cloud Elastic Compute Service\" --git https://github.com/wonderflow/terraform-alicloud-ecs-instance.git > alibaba-ecs-def.yaml\r\n```\r\n\r\n> Change the git url with your own if you have customized.\r\n\r\n* Apply it to the vela control plane\r\n\r\n```\r\nvela kube apply -f alibaba-ecs-def.yaml\r\n```\r\n\r\n> `vela kube apply` works the same with `kubectl apply`.\r\n\r\nThen the extension of ECS module has been added, you can learn more details from [here](https://kubevela.net/docs/platform-engineers/components/component-terraform).\r\n\r\nWe have finished the integration, the end user can discover the capability immediately after the apply.\r\n\r\nThe end user can use following commands to check the parameters:\r\n\r\n```\r\nvela show alibaba-ecs\r\n```\r\n\r\nThey can also view it from website by launching:\r\n\r\n```\r\nvela show alibaba-ecs --web\r\n```\r\n\r\nThat's all of the integration needed.\r\n\r\n## Part 2. Fixing the Developer Experience of Kubernetes Port Forwarding\r\n\r\nIn this part, we will introduce a solution that you can expose any of your Kubernetes service to public with a specific port. The solution is composed by:\r\n\r\n1. KubeVela environment, you already have if you have practiced in part 1.\r\n2. Alibaba Cloud ECS, KubeVela will create a tiny ecs(`1u1g`) automatically by access key.\r\n3. [frp](https://github.com/fatedier/frp), KubeVela will launch this proxy both at server-side and client-side.\r\n\r\n### Prepare KubeVela environment\r\n\r\n* Install KubeVela\r\n\r\n```\r\ncurl -fsSl https://static.kubevela.net/script/install-velad.sh | bash\r\nvelad install\r\n```\r\n\r\nCheck [this doc](https://kubevela.net/docs/install#1-install-velad) to learn more details of installation.\r\n\r\n* Enable Terraform Addon and Alibaba Provider\r\n\r\n```\r\nvela addon enable terraform\r\nvela addon enable terraform-alibaba\r\n```\r\n\r\n* Add credentials as provider\r\n\r\n```\r\nvela provider add terraform-alibaba --ALICLOUD_ACCESS_KEY <\"your-accesskey-id\"> --ALICLOUD_SECRET_KEY \"your-accesskey-secret\" --ALICLOUD_REGION <your-region> --name terraform-alibaba-default\r\n```\r\n\r\nCheck [this doc](https://kubevela.net/docs/reference/addons/terraform) for more details about other clouds.\r\n\r\n### Launch a ECS with Public IP and Deploy the `frp` server \r\n\r\nAfter the environment prepared well, you can create an application as below.\r\n\r\n```yaml\r\ncat <<EOF | vela up -f -\r\n# YAML begins\r\napiVersion: core.oam.dev/v1beta1\r\nkind: Application\r\nmetadata:\r\n  name: ecs-demo\r\nspec:\r\n  components:\r\n    - name: ecs-demo\r\n      type: alibaba-ecs\r\n      properties:\r\n        providerRef:\r\n          name: terraform-alibaba-default\r\n        writeConnectionSecretToRef:\r\n          name: outputs-ecs          \r\n        name: \"test-terraform-vela-123\"\r\n        instance_type: \"ecs.n1.tiny\"\r\n        host_name: \"test-terraform-vela\"\r\n        password: \"Test-123456!\"\r\n        internet_max_bandwidth_out: \"10\"\r\n        associate_public_ip_address: \"true\"\r\n        instance_charge_type: \"PostPaid\"\r\n        user_data_url: \"https://raw.githubusercontent.com/wonderflow/terraform-alicloud-ecs-instance/master/frp.sh\"\r\n        ports:\r\n        - 8080\r\n        - 8081\r\n        - 8082\r\n        - 8083\r\n        - 9090\r\n        - 9091\r\n        - 9092\r\n        tags:\r\n          created_by: \"Terraform-of-KubeVela\"\r\n          created_from: \"module-tf-alicloud-ecs-instance\"\r\n# YAML ends\r\nEOF\r\n```\r\n\r\nThis application will deploy an ECS instance with a public ip, explanation of some useful fields:\r\n\r\n| Field | Usage  |\r\n|:----:|:---:|\r\n| providerRef | reference to the provider credentials we added |\r\n| writeConnectionSecretToRef | the outputs of terraform module will be written into the secret |\r\n| name | name of the ecs instance |\r\n| instance_type | ecs instance type |\r\n| host_name | hostname of the ecs |\r\n| password | password of the ecs instance, you can connect by `ssh` |\r\n| internet_max_bandwidth_out | internet bandwidth of the ecs instance |\r\n| associate_public_ip_address | create public IP or not |\r\n| instance_charge_type | the charge way of the resource |\r\n| user_data_url | the installation script after the ecs instance created, we have installed the frp server in the script |\r\n| ports | ports that will be allowd in the VPC and security group, 9090/9091 is must for frp server while others are preserved for client usage |\r\n| tags | tags of the ECS instance |\r\n\r\nYou can learn more fields by:\r\n\r\n```\r\nvela show alibaba-ecs\r\n```\r\n\r\nAfter applied, you can check the status and logs of the application by:\r\n\r\n```\r\nvela status ecs-demo\r\nvela logs ecs-demo\r\n```\r\n\r\nYou can get the secret from the terraform resource contains the output values.\r\n\r\nYou may already see the result in `vela logs`, you can also check the output information from Terraform by:\r\n\r\n```shell\r\n$ kubectl get secret outputs-ecs --template={{.data.this_public_ip}} | base64 --decode\r\n[\"121.196.106.174\"]\r\n```\r\n\r\n> KubeVela will soon support query resource like this https://github.com/kubevela/kubevela/issues/4268.\r\n\r\nAs a result, you can visit the frp server admin page on port `:9091`, the `admin` password is `vela123` in the script.\r\n\r\nBy now, we have finished the server part here.\r\n\r\n### Use frp client in KubeVela\r\n\r\nThe usage of frp client is very straight-forward, we can provide public IP for any of the service inside the cluster.\r\n\r\n![](../docs/resources/terraform-ecs.png)\r\n\r\n1. Deploy as standalone to proxy for any [Kubernetes Service](https://kubernetes.io/docs/concepts/services-networking/service/).\r\n\r\n```yaml\r\ncat <<EOF | vela up -f -\r\napiVersion: core.oam.dev/v1beta1\r\nkind: Application\r\nmetadata:\r\n  name: frp-proxy\r\nspec:\r\n  components:\r\n    - name: frp-proxy\r\n      type: worker\r\n      properties:\r\n        image: oamdev/frpc:0.43.0\r\n        env:\r\n          - name: server_addr\r\n            value: \"121.196.106.174\"\r\n          - name: server_port\r\n            value: \"9090\"\r\n          - name: local_port\r\n            value: \"80\"\r\n          - name: connect_name\r\n            value: \"velaux-service\"\r\n          - name: local_ip\r\n            value: \"velaux.vela-system\"\r\n          - name: remote_port\r\n            value: \"8083\"\r\nEOF\r\n```\r\n\r\nIn this case, we specify the `local_ip` by `velaux.vela-system`, which means we're visiting the Kubernetes Service with name `velaux` in the namespace `vela-system`.\r\n\r\nAs a result, you can visit [velaux](https://kubevela.io/docs/reference/addons/velaux) service from the public IP `121.196.106.174:8083`.\r\n\r\n2. Compose two components together for the same lifecycle.\r\n\r\n```yaml\r\ncat <<EOF | vela up -f -\r\n# YAML begins\r\napiVersion: core.oam.dev/v1beta1\r\nkind: Application\r\nmetadata:\r\n  name: composed-app\r\nspec:\r\n  components:\r\n    - name: web-new\r\n      type: webservice\r\n      properties:\r\n        image: oamdev/hello-world:v2\r\n        ports:\r\n          - port: 8000\r\n            expose: true\r\n    - name: frp-web\r\n      type: worker\r\n      properties:\r\n        image: oamdev/frpc:0.43.0\r\n        env:\r\n          - name: server_addr\r\n            value: \"121.196.106.174\"\r\n          - name: server_port\r\n            value: \"9090\"\r\n          - name: local_port\r\n            value: \"8000\"\r\n          - name: connect_name\r\n            value: \"composed-app\"\r\n          - name: local_ip\r\n            value: \"web-new.default\"\r\n          - name: remote_port\r\n            value: \"8082\"\r\nEOF\r\n```\r\n\r\nWow! Then you can visiting the `hello-world` by:\r\n\r\n```\r\ncurl 121.196.106.174:8082\r\n```\r\n\r\nThe `webservice` type component will generate a service with the name of the component automatically. The `frp-web` component will proxy the traffic to the service `web-new` in the `default` namespace which is exactly the service generated.\r\n\r\nWhen the application deleted, all of the resources defined in the same app are deleted together.\r\n\r\nYou can also compose the database together with them, then you can deliver all components needed in one time.\r\n\r\n### Clean Up\r\n\r\nYou can clean up all the applications in the demo by `vela delete`:\r\n\r\n```\r\nvela delete composed-app -y\r\nvela delete frp-proxy -y\r\nvela delete ecs-demo -y\r\n```\r\n\r\nI think you've learned how to use KubeVela in this scenario now, just try it in your environment!\r\n\r\n## What's more\r\n\r\nIn this blog, we have introduced the way to integrate Terraform module with KubeVela. It provides interesting use case that allow you to expose any of inner service to public.\r\n\r\nWhile KubeVela can do more things than that, go and discover it at [kubevela.io](https://kubevela.io/)!"
    },
    {
      "id": "/2022/06/21/release-1.4",
      "metadata": {
        "permalink": "/blog/2022/06/21/release-1.4",
        "editUrl": "https://github.com/kubevela/kubevela.io/tree/main/blog/2022-06-21-release-1.4.md",
        "source": "@site/blog/2022-06-21-release-1.4.md",
        "title": "KubeVela 1.4 released, Make Application Delivery Safe, Foolproof, and Transparent",
        "description": "This article discusses a brief history of KubeVela and the latest release of KubeVela 1.4.",
        "date": "2022-06-21T00:00:00.000Z",
        "formattedDate": "June 21, 2022",
        "tags": [
          {
            "label": "KubeVela",
            "permalink": "/blog/tags/kube-vela"
          },
          {
            "label": "release-note",
            "permalink": "/blog/tags/release-note"
          },
          {
            "label": "Kubernetes",
            "permalink": "/blog/tags/kubernetes"
          },
          {
            "label": "DevOps",
            "permalink": "/blog/tags/dev-ops"
          },
          {
            "label": "CNCF",
            "permalink": "/blog/tags/cncf"
          },
          {
            "label": "CI/CD",
            "permalink": "/blog/tags/ci-cd"
          },
          {
            "label": "Application delivery",
            "permalink": "/blog/tags/application-delivery"
          },
          {
            "label": "Role-Based Access Control",
            "permalink": "/blog/tags/role-based-access-control"
          }
        ],
        "readingTime": 12.02,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Jianbo Sun, Qingguo Zeng",
            "title": "KubeVela Team",
            "url": "https://github.com/kubevela/KubeVela",
            "imageURL": "https://KubeVela.io/img/logo.svg"
          }
        ],
        "frontMatter": {
          "title": "KubeVela 1.4 released, Make Application Delivery Safe, Foolproof, and Transparent",
          "author": "Jianbo Sun, Qingguo Zeng",
          "author_title": "KubeVela Team",
          "author_url": "https://github.com/kubevela/KubeVela",
          "author_image_url": "https://KubeVela.io/img/logo.svg",
          "tags": [
            "KubeVela",
            "release-note",
            "Kubernetes",
            "DevOps",
            "CNCF",
            "CI/CD",
            "Application delivery",
            "Role-Based Access Control"
          ],
          "description": "This article discusses a brief history of KubeVela and the latest release of KubeVela 1.4.",
          "image": "https://raw.githubusercontent.com/oam-dev/KubeVela.io/main/docs/resources/KubeVela-03.png",
          "hide_table_of_contents": false
        },
        "prevItem": {
          "title": "Glue Terraform Ecosystem into Kubernetes World",
          "permalink": "/blog/2022/06/27/terraform-integrate-with-vela"
        },
        "nextItem": {
          "title": "Trace and visualize the relationships between the kubernetes resources with KubeVela",
          "permalink": "/blog/2022/06/10/visualize-resources"
        }
      },
      "content": "[KubeVela](http://kubevela.net/) is a modern software delivery control panel. The goal is to make application deployment and O&M simpler, more agile, and more reliable in today's hybrid multi-cloud environment. Since the release of [Version 1.1](https://kubevela.net/blog/2021/10/08/blog-1.1), the KubeVela architecture has naturally solved the delivery problems of enterprises in the hybrid multi-cloud environments and has provided sufficient scalability based on the OAM model, which makes it win the favor of many enterprise developers. This also accelerates the iteration of KubeVela. \r\n\r\nIn [Version 1.2](https://kubevela.net/blog/2022/01/27/blog-1.2), we released an out-of-the-box visual console, which allows the end user to publish and manage diverse workloads through the interface. The release of [Version 1.3](https://kubevela.net/blog/2022/04/06/multi-cluster-management) improved the expansion system with the OAM model as the core and provides rich plug-in functions. It also provides users with a large number of enterprise-level functions, including LDAP permission authentication, and provides more convenience for enterprise integration. You can obtain more than 30 addons in the [addons registry](https://github.com/kubevela/catalog) of the KubeVela community. There are well-known CNCF projects (such as argocd, istio, and traefik), database middleware (such as Flink and MySQL), and hundreds of cloud vendor resources. \r\n\r\nIn Version 1.4, we focused on **making application delivery safe, foolproof, and transparent**. We added core functions, including multi-cluster permission authentication and authorization, a complex resource topology display, and a one-click installation control panel. We comprehensively strengthened the delivery security in multi-tenancy scenarios, improved the consistent experience of application development and delivery, and made the application delivery process more transparent. \r\n\r\n<!--truncate-->\r\n\r\n## Core Features\r\n\r\n### Out-of-the-Box Authentication and Authorization, Connecting to Kubernetes RBAC and Naturally Supporting Multiple Clusters\r\n\r\nAfter solving the challenges of architecture upgrade and extensibility, we have noticed that the security of application delivery is a problem in the entire industry that needs to be solved urgently. We found many security risks from the use cases:\r\n\r\n- In using traditional CI/CD, many users will directly embed the admin permission of the production cluster into the environment variable of CI and only have certain permission separation for which clusters are delivered to the most basic. CI systems are usually used intensively for development and testing, and it is easy to introduce uncontrolled risks. Once the CI system is attacked by hackers or some **man-made mis-operations** occur, it may lead to huge damage to centralized management and coarse-grained authority control. \r\n\r\n- A large number of CRD controllers rely on admin permissions to perform operations on cluster resources and do not impose constraints on API access. Kubernetes has rich RBAC control capabilities. However, due to the high threshold of permission management (which is also independent of the implementation of specific functions), most users do not care about the details. They only choose the default configuration and put it into production use. Controllers with high flexibility (such as the ability to distribute Helm Chart) can easily become targets of hacker attacks, such as embedding a YAML script in the helm to steal keys from other namespaces. \r\n\r\n\r\nKubeVela 1.4 has added **authentication and authorization capabilities and naturally supports a multi-cluster hybrid environment.** Each KubeVela platform administrator can customize any API permission combination in fine granularity, connect with the Kubernetes RBAC system, authorize these permission modules to developer users, and strictly restrict their permissions. They can also easily use the permission module preset on the KubeVela platform. For example, they can directly grant a user the permissions on a specific namespace of a cluster and the *read-only* permissions. This simplifies the learning costs and mental burdens of users and comprehensively strengthens the security of application delivery. The system automatically completes the underlying authorization and strictly verifies the scope and type of resources available for the project for users who use the UI, so the business layer RBAC permissions and the underlying Kubernetes RBAC system can be connected and work together to achieve security from the outside to the inside without expanding the permissions in any link. \r\n\r\n![alt](/img/release-1.4/impersonation-arch.jpg)\r\n\r\nSpecifically, after the platform administrator [authorizes a user](https://kubevela.net/docs/platform-engineers/auth/basic), the user's request will go through several stages (as shown in the figure). \r\n\r\n1. First, the webhook of KubeVela intercepts the user's request and sends the user's permission information (ServiceAccount) to the Application object. \r\n\r\n2. When the KubeVela Controller executes the deployment plan of the Application, it changes the permissions of the corresponding users based on the [impersonation mechanism (impersonate)](https://kubernetes.io/docs/reference/access-authn-authz/authentication/#user-impersonation) of Kubernetes. \r\n\r\n3. The KubeVela multi-cluster module (ClusterGateway) passes the corresponding permissions to the sub-cluster. The Kubernetes APIServer of the sub-cluster performs authentication based on the permissions of the sub-cluster. The permissions of the sub-cluster are created by the KubeVela authorization process. \r\n\r\n\r\nIn short, KubeVela's multi-cluster authentication and authorization ensure that the permissions of **each end user are strictly restricted and will not be amplified by the delivery system. At the same time, KubeVela's permissions are minimized**, and the entire user experience is simple. \r\n\r\nPlease read the official [Permission Authentication and Authorization](https://kubevela.net/docs/platform-engineers/auth/basic) to learn more about the operating mechanisms behind them. \r\n\r\n### Lightweight and Convenient Application Development Control Panel Offers a Consistent Experience for Local Development and Production Deployment\r\n\r\nWith the prosperity of the ecosystem, we have seen more developers begin to pay attention to cloud-native technology, but they often don't know how to get started. The following are the main reasons:\r\n\r\n- The application **development environment is inconsistent with the production environment**, and the experience is different. Cloud-native is a technology trend that has emerged in the last five or six years. It has developed rapidly, but most companies are still accustomed to developing a set of internal platforms shielding underlying technologies. As a result, even if ordinary business developers learn cloud-native technology, it is difficult to practice it in actual work. In the best case, they may have to reconnect the API and configuration, let alone the consistent experience. \r\n\r\n- **The deployment and use** of cloud-native technologies with Kubernetes as the core **is complicated**. It is expensive to purchase host services from cloud vendors just for the sake of getting started. Even if it takes a lot of effort to learn to deploy a set of available local environments, it is difficult to connect many cloud-native technologies to complete the entire CI/CD process, which involves a lot of knowledge in the field of operation and maintenance, and ordinary developers usually do not need to care about it and have rare chance to use it. \r\n\r\n\r\nWe have also observed in the community that more companies are beginning to realize that self-built platforms cannot keep up with the development of the community ecosystem. They hope to provide a consistent experience through KubeVela and OAM models without losing the scalability of the ecosystem. However, since KubeVela's control panel relies on Kubernetes, the threshold for getting started is still not low. In response to this problem, the community has been thinking and looking for solutions. We conclude that we need a tool that has these characteristics:\r\n\r\n- Only rely on the container environment (such as Docker) to deploy and run, **so every developer can easily obtain and use it**\r\n\r\n- The local development and production environment experience are consistent, and the configuration is reusable, which can simulate a hybrid multi-cluster environment.\r\n\r\n- A single binary package supports **offline deployment**, and the time for environment initialization does not exceed the time to drink a glass of water (three minutes).\r\n\r\n\r\nAfter several months of incubation, we can finally release this tool in 1.4: [VelaD](https://github.com/kubevela/velad/). D represents Daemon and Developer. It can help KubeVela run on a single machine and does not rely on any existing Kubernetes cluster. At the same time, it works as a lightweight application development control panel with KubeVela, helping developers obtain integrated development, testing, and delivery experiences and simplifying the complexity of cloud-native application deployment and management. \r\n\r\nYou can use the [Demo documentation](https://github.com/kubevela/velad/blob/main/docs/01.simple.md) to install and try this tool to learn more about the implementation details. It only takes three minutes to initialize the installation. \r\n\r\n![alt](/img/release-1.4/demo.gif)\r\n\r\n### Show Resource Topology and Status to Make the Delivery Process Transparent\r\n\r\nAnother big demand in application delivery is the transparent management of the resource delivery process. For example, many users in the community like to use Helm Chart to package a lot of complex YAML together. However, once there are problems in the deployment, it will be difficult to troubleshoot due to the overall black box (even if it is a small problem). Some problems include the underlying storage is not provided normally, the associated resources are not created normally, and the underlying configuration is incorrect. There are many types of resources (especially in the modern hybrid multi-cluster hybrid environment) and how to obtain effective information from them and solve problems is a challenge. \r\n\r\nIn Version 1.4, we added the resource topology query function to improve KubeVela's application-centric delivery experience. When developers initiate application delivery, they only need to care about simple and consistent APIs. When they need to troubleshoot problems or focus on the delivery process, they can use the resource topology feature to quickly obtain the orchestration relationships of resources in different clusters **from applications to the running status of Pod instances and automatically obtain the relationships of resources, including complex and black-box Helm Chart.** \r\n\r\n\r\n![resource graph](https://static.kubevela.net/images/1.4/resource-graph.jpg)\r\n\r\nThe application shown in the preceding figure is used as an example. A Redis cluster is delivered through the Helm Chart package. The first layer of the chart is the application name, the second layer is the cluster, and the third layer is the resources directly rendered by the application. The next third and fourth layers are the associated resources of the lower level tracked according to different resources. \r\n\r\nUsers can use graphs to observe the derived resources and their status during the application delivery process. The abnormal points are displayed in yellow or red and the specific reasons are displayed. Compared with the application shown in the following figure, it is a basic Webservice delivered to two clusters. Developers can find that the application creates Deployment and Service resources in the two clusters, respectively. Also, the Deployment resources in the ask-hongkong cluster are displayed in yellow since the Pod instance has not been fully started. \r\n\r\n![multiple-cluster-graph](https://static.kubevela.net/images/1.4/multiple-cluster-graph.jpg)\r\n\r\n\r\nThis feature also allows you to search, filter, and query using different clusters and components. This helps developers quickly identify problems and understand the delivery status of the underlying application at a very low threshold. \r\n\r\nPlease read the official blog *[Visualize the Topological Relationship of Multi-cluster Resources](https://kubevela.net/blog/2022/06/10/visualize-resources)* to learn more about the operation mechanism behind them. \r\n\r\n## Other Key Changes\r\n\r\nIn addition to the core functions and plug-in ecosystem, Version 1.4 also enhances core functions such as workflow:\r\n\r\n- You can configure field ignore rules to maintain the application status. This enables KubeVela to work with other controllers, such as HPA and Istio.\r\n \r\n- Application Resource Recycling supports settings based on the resource type. Currently, it supports settings based on the component name, component type, feature type, and resource type. \r\n\r\n- Workflows support sub-steps. Sub-steps support concurrent execution, which accelerates the delivery of resources in multi-cluster high availability scenarios. \r\n\r\n- You can pause a workflow step for a certain period. After that, the workflow automatically continues. \r\n\r\n- Resource deployment and recycling support follows the component dependency rule settings and supports sequential deployment and recycling of resources. \r\n\r\n- Workflow steps support conditional judgment. Currently, the *if: always* rule is supported, which means the step is executed under any circumstances, thus supporting deployment failure notification. \r\n\r\n- You can set the deployment scope for O&M features to separate O&M features from the deployment status of components. O&M features can be independently deployed in the control cluster. \r\n\r\n\r\nThanks to the continued contributions and efforts from more than 30 organizations and individuals in China and internationally (such as Alibaba Cloud, China Merchants Bank, and Napptive), more than 200 functional features and repairs were completed in a short period of two months, which has made this iteration excellent. \r\n\r\nPlease see the [release details](https://github.com/kubevela/kubevela/releases/tag/v1.4.0) for more information. \r\n\r\n## Addon Ecosystem\r\n\r\nOur plug-in ecology is also rapidly expanding because of the improvement of the 1.3 addon system:\r\n\r\n- Updated fluxcd addon supports OCI registry, which allows you to select different values files in the chart during deployment \r\n\r\n- The cert-manager addon is added to automatically manage Kubernetes certificates. \r\n\r\n- Adds a flink-kubernetes-operator addon to deliver flink workloads. \r\n\r\n- The kruise-rollout addon is added to support various release policies (such as canary release). \r\n\r\n- The pyroscope addon is added to support continuous performance tuning.\r\n \r\n- The traefik plug-in is added to support configuration API Gateway.\r\n \r\n- The vegeta addon is added to support automated stress testing of workloads. \r\n\r\n- The argocd addon is added to support ArgoCD-based Helm delivery and GitOps.\r\n \r\n- The Dapr addon is added to support the O&M capabilities of Dapr subscription and publishing. \r\n\r\n- The istio addon is added to support Istio-based gateway capabilities and traffic canaries. \r\n\r\n- The mysql-operator addon is added to support the deployment of highly available distributed mysql databases. \r\n\r\nDevelopers are welcome to participate in the community and [create addon](https://kubevela.net/docs/platform-engineers/addon/intro) to extend KubeVela's system capabilities. \r\n\r\n![undefined](https://static.kubevela.net/images/1.4/addon-list-new.jpg) \r\n\r\n## How Can You Participate in the Community?\r\n\r\n![alt](/img/release-1.4/open-source.png)\r\n\r\nKubeVela is an open-source, worldwide, Top-Level Project in the CNCF Foundation. There are more than 300 domestic and international contributors and more than 40 community members and maintainers. It is a bilingual international operation mode with more than 4000 community members, including code, documents, and community communication. \r\n\r\nIf you are interested in participating in the open-source community, we welcome you to join the KubeVela community. You can learn more about the methods of participating in the open-source community through the [developer documentation of the KubeVela community](https://kubevela.io/docs/contributor/overview). The engineers of the community will guide you. \r\n\r\n## Recent Planning\r\n\r\nKubeVela will continue to evolve around an iterative cycle of two months. We will focus on these three dimensions in the next release:\r\n\r\n- **Observability** will provide end-to-end rich application insights around logs, metrics, and tracing dimensions to lay a solid foundation for the stability and intelligence of application delivery. \r\n\r\n- **Workflow Delivery Capabilities** will provide richer frameworks and integration capabilities, including custom step timeout, context information-based condition judgment, and branch workflow, and connect CI/CD, providing users with richer use cases and scenarios. \r\n\r\n- **Application (Including Plug-Ins) Management Ability:** You can disable and restart applications. You can import, export, and upload applications to the application market. \r\n\r\n\r\nIf you want to learn more about planning and become a contributor or partner, you can contact us by participating in [community communication](https://github.com/kubevela/community). We are looking forward to hearing from you!"
    },
    {
      "id": "/2022/06/10/visualize-resources",
      "metadata": {
        "permalink": "/blog/2022/06/10/visualize-resources",
        "editUrl": "https://github.com/kubevela/kubevela.io/tree/main/blog/2022-06-10-visualize-resources.md",
        "source": "@site/blog/2022-06-10-visualize-resources.md",
        "title": "Trace and visualize the relationships between the kubernetes resources with KubeVela",
        "description": "",
        "date": "2022-06-10T00:00:00.000Z",
        "formattedDate": "June 10, 2022",
        "tags": [
          {
            "label": "KubeVela",
            "permalink": "/blog/tags/kube-vela"
          },
          {
            "label": "Observable",
            "permalink": "/blog/tags/observable"
          }
        ],
        "readingTime": 7.68,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Qingguo Zeng",
            "url": "https://github.com/barnettZQG",
            "imageURL": "https://avatars.githubusercontent.com/u/18493394?v=4"
          }
        ],
        "frontMatter": {
          "title": "Trace and visualize the relationships between the kubernetes resources with KubeVela",
          "author": "Qingguo Zeng",
          "author_url": "https://github.com/barnettZQG",
          "author_image_url": "https://avatars.githubusercontent.com/u/18493394?v=4",
          "tags": [
            "KubeVela",
            "Observable"
          ],
          "description": "",
          "image": "https://raw.githubusercontent.com/oam-dev/KubeVela.io/main/docs/resources/KubeVela-03.png",
          "hide_table_of_contents": false
        },
        "prevItem": {
          "title": "KubeVela 1.4 released, Make Application Delivery Safe, Foolproof, and Transparent",
          "permalink": "/blog/2022/06/21/release-1.4"
        },
        "nextItem": {
          "title": "Build flexible abstraction for any Kubernetes Resources with CUE and KubeVela",
          "permalink": "/blog/2022/05/30/abstraction-kubevela"
        }
      },
      "content": "One of the biggest requests from KubeVela community is to provide a transparent delivery process for resources in the application. For example, many users prefer to use Helm Chart to package a lot of complex YAML, but once there is any issue during the deployment, such as the underlying storage can not be provided normally, the associated resources are not created normally, or the underlying configuration is incorrect, etc., even a small problem will be a huge threshold for troubleshooting due to the black box of Helm chart. Especially in the modern hybrid multi-cluster environment, there is a wide range of resources, how to obtain effective information and solve the problem? This can be a very big challenge.\r\n\r\n![resource graph](https://static.kubevela.net/images/1.4/resource-graph.jpg)\r\n\r\nAs shown in the figure above, KubeVela has offered a real-time observation resource topology graph for applications, which further improves KubeVela's application-centric delivery experience. Developers only need to care about simple and consistent APIs when initiating application delivery. When they need to troubleshoot problems or pay attention to the delivery process, they can use the resource topology graph to quickly obtain the arrangement relationship of resources in different clusters, from the application to the running status of the Pod instance. Automatically obtain resource relationships, including complex and black-box Helm Charts.\r\n\r\nIn this post, we will describe how this new feature of KubeVela is implemented and works, and the roadmap for this feature.\r\n\r\n<!--truncate-->\r\n\r\n## Application resource composition\r\n\r\nIn KubeVela, an application consists of multiple components and traits and is associated with delivery workflow and delivery policy configuration. The application configuration generates Kubernetes resources through rendering and inspection and applies them to the target cluster. Take a simple application as an example:\r\n\r\n```yaml\r\napiVersion: core.oam.dev/v1beta1\r\nkind: Application\r\nmetadata:\r\n  name: first-vela-app\r\nspec:\r\n  components:\r\n    - name: express-server\r\n      type: webservice\r\n      properties:\r\n        image: oamdev/hello-world\r\n        ports: \r\n         - port: 8000\r\n           expose: false\r\n```\r\n\r\nBased on the above configuration, a `Deployment` resource will be rendered and deployed to the target cluster, if we slightly add some configuration, such as:\r\n\r\n```yaml\r\napiVersion: core.oam.dev/v1beta1\r\nkind: Application\r\nmetadata:\r\n  name: first-vela-app\r\nspec:\r\n  components:\r\n    - name: express-server\r\n      type: webservice\r\n      properties:\r\n        image: oamdev/hello-world\r\n        ports: \r\n         - port: 8000\r\n           expose:  true\r\n      traits:\r\n        - type: gateway\r\n          properties:\r\n            domain: testsvc.example.com\r\n            http:\r\n              \"/\": 8000\r\n```\r\n\r\nIn the above configuration, we set the `expose` field of port 8000 to true, and add a trait with type `gateway`. The application will render three resources `Deployment` + `Service` + `Ingress`.\r\n\r\nAs mentioned above, the resources directly rendered by the application are called <b>Direct Resources</b>, and they will be stored in `ResourceTracker` as version records at the same time, and these resources can be obtained by direct indexing. When these resources are delivered to the target cluster, taking the `Deployment` resource as an example, the lower-level resource `ReplicaSet` will be generated, and then the lower-level resource `Pod` will be derived. These secondary derived resources from direct resources are called <b>Indirect Resources</b>. An application resource tree consists of direct resources and indirect resources, that work together to run dynamic applications at scale.\r\n\r\n## Trace and Visualize resources’ relationships\r\n\r\nThe relationship chain `Deployment` => `ReplicaSet` => `Pod` described in the previous chapter is a resource cascade relationship, which is also the introductory knowledge of the Kubernetes system. We can build this relationship chain very easily through experience. Where `Deployment` is a direct resource, which we can index to based on (application & component & cluster) conditions. Next, we mainly build the relationship between the indirect resources.\r\n\r\nIn Kubernetes, the concept of Owner Reference is designed to record the relationship between resources. For example, in the following use case, the Pod resource records its parent resource ReplicaSet.\r\n\r\n```yaml\r\napiVersion: v1\r\nkind: Pod\r\nmetadata:\r\n  name: kruise-rollout-controller-manager-696bdb76f8-92rsn\r\n  ownerReferences:\r\n  - apiVersion: apps/v1\r\n    blockOwnerDeletion: true\r\n    controller: true\r\n    kind: ReplicaSet\r\n    name: kruise-rollout-controller-manager-696bdb76f8\r\n    uid: aba76833-b6e3-4231-bf2e-539c81be9278\r\n  ...\r\n```\r\n\r\nTherefore, we can reversely build any resource-dependent link through the resource's Owner Reference. However, there are two difficulties here:\r\n\r\n1. The resource links in our experience may not necessarily be constructed through Owner Reference, such as the HelmRelease resource, which is the resource API defined by FluxCD to deliver the Helm Chart artifacts. In KubeVela, we use this API to deliver Helm Chart artifacts. Helm Chart can theoretically include any Kubernetes cluster resource type. From user experience, these resources are subordinate resources of HelmRelease, but HelmRelease cannot be the owner of these resources at present. That is to say, we cannot build a tracking link similar to the HelmRelease resource through Owner Reference.\r\n\r\n2. When forward tracing resources, if you do not know the subordinate resource types, you need to traverse and query all types of resources and then filter them according to the Owner Reference, which results in a large amount of computation and puts a lot of pressure on the Kubernetes API.\r\n\r\nThe cascading relationship of application resources is often the link when we troubleshoot application failures or configuration errors. If your Kubernetes experience cannot build such a link, it will be very difficult to troubleshoot Kubernetes application failures. For example, HelmRelease, once encountering a failure, maybe we need to check the definition source code of Chart to know which subordinate resources it will generate. This threshold may hinder 90% of developers and users.\r\n\r\nThere are many types resources that do not follow the Owner Reference in Kubernetes and continue to grow. Therefore, we need to enable the system to have the ability to learn to speed up forward queries and adapt to resources that do not follow the Owner Reference mechanism.\r\n\r\n```yaml\r\napiVersion: v1\r\nkind: ConfigMap\r\nmetadata:\r\n  name: clone-set-relation\r\n  namespace: vela-system\r\n  labels:\r\n    \"rules.oam.dev/resources\": \"true\"\r\ndata:\r\n  rules: |-\r\n    - parentResourceType:\r\n        group: apps.kruise.io\r\n        kind: CloneSet\r\n      childrenResourceType:\r\n        - apiVersion: v1\r\n          kind: Pod\r\n```\r\n\r\nThe above is a configuration use case that informs KubeVela that the `CloneSet` resource cascades down the `Pod`, KubeVela will query the Pod that satisfies the Owner Reference condition from the same Namespace, so that the query complexity is O(1). A rule defines multiple resource types that are associated with a parent type down, this way is called static configuration learning. We need to implement more rules:\r\n\r\nFor example, if we need to pay attention to whether the Service is correctly associated with the Pod, and whether the PVC is correctly associated with the PV, there is no Owner Reference filter rule between these resources. If only configuring the type is not enough, we also need to have filter conditions, such as through labels, name, etc. At present, this part of the logic of KubeVela has some built-in rules. In addition, to simplify the user's burden of configuring static learning rules, we plan to implement dynamic learning capabilities. Based on all resource types in the current cluster, the system can dynamically analyze a certain type of resource as the owner of which types of resources. The result rules can be shared.\r\n\r\n## Visualize the resource status and key information\r\n\r\nIf there is only a resource relationship, it cannot solve our difficulties. We also need to be able to directly reflect exceptions, so that developers can have O(1) complexity when troubleshooting errors. Different resources have slightly different state calculations, but the general resource state has a Condition field that represents its final state. Based on this, the resource state calculation logic is formed by adding the state calculation method of a specific resource. We divide the resource state into normal, in-progress and abnormal, which are represented by blue, yellow and red border colors on the topology graph node, which is convenient for users to distinguish.\r\n\r\n![multiple-cluster-graph](https://static.kubevela.net/images/1.4/multiple-cluster-graph.jpg)\r\n\r\nIn addition, different resources have different key information, such as whether PVC is bound, whether Pod instance is started, whether Service resource is associated with external IP and so on. These are called key information, and some information is displayed on the resource node bottom, others display when the mouse moves over the node. The information helps you quickly determine whether the resource configuration is correct and its status is normal.\r\n\r\nFurther, if you want to query the detailed configuration of ConfigMap, whether the capacity and access method of PersistentVolumes are correct, whether the RBAC authorization rules are correct, etc., you do not need to leave the VelaUX and manually dig through YAML files. Initiate a query by clicking the Detail button in the node extension area. KubeVela will securely query and display the latest configurations from the target cluster through the cluster gateway.\r\n\r\n![resource-detail](https://kubevela.net/assets/images/resouce-detail-0919c787c88e6b38f00ea490d558a927.jpg)\r\n\r\n## What's next?\r\n\r\n* More intelligent\r\n\r\nWe will continue to refine the default rules and find more ways to intelligently judge resource diagrams and key information so that developers can troubleshoot errors without too much experience.\r\n\r\n* Application topology graph\r\n\r\nWhat we are currently building is the Kubernetes resource map, which is actually not our original intention. We prefer that business developers only pay attention to the application and all the relationships and states of its components, and at the same time combine business metrics analysis and monitoring data to reflect the operating pressure of the service.\r\n\r\n* More runtimes\r\n\r\nThe currently established resource relationship graph is mainly based on Kubernetes resources. KubeVela also has a core runtime that is a cloud service platform. We need to associate the resource system of cloud services with the resource topology graph. Makes it easier for developers to manage multi-cloud applications."
    },
    {
      "id": "/2022/05/30/abstraction-kubevela",
      "metadata": {
        "permalink": "/blog/2022/05/30/abstraction-kubevela",
        "editUrl": "https://github.com/kubevela/kubevela.io/tree/main/blog/2022-05-30-abstraction-kubevela.md",
        "source": "@site/blog/2022-05-30-abstraction-kubevela.md",
        "title": "Build flexible abstraction for any Kubernetes Resources with CUE and KubeVela",
        "description": "",
        "date": "2022-05-30T00:00:00.000Z",
        "formattedDate": "May 30, 2022",
        "tags": [
          {
            "label": "KubeVela",
            "permalink": "/blog/tags/kube-vela"
          },
          {
            "label": "Abstraction",
            "permalink": "/blog/tags/abstraction"
          }
        ],
        "readingTime": 9.28,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Jianbo Sun",
            "title": "KubeVela Team",
            "url": "https://github.com/kubevela/KubeVela",
            "imageURL": "https://KubeVela.io/img/logo.svg"
          }
        ],
        "frontMatter": {
          "title": "Build flexible abstraction for any Kubernetes Resources with CUE and KubeVela",
          "author": "Jianbo Sun",
          "author_title": "KubeVela Team",
          "author_url": "https://github.com/kubevela/KubeVela",
          "author_image_url": "https://KubeVela.io/img/logo.svg",
          "tags": [
            "KubeVela",
            "Abstraction"
          ],
          "description": "",
          "image": "https://raw.githubusercontent.com/oam-dev/KubeVela.io/main/docs/resources/KubeVela-03.png",
          "hide_table_of_contents": false
        },
        "prevItem": {
          "title": "Trace and visualize the relationships between the kubernetes resources with KubeVela",
          "permalink": "/blog/2022/06/10/visualize-resources"
        },
        "nextItem": {
          "title": "KubeVela v1.3 released, CNCF's Next Generation of Cloud Native Application Delivery Platform",
          "permalink": "/blog/2022/04/24/blog-1.3"
        }
      },
      "content": "This blog will introduce how to use CUE and KubeVela to build you own abstraction API to reduce the complexity of Kubernetes resources. As a platform builder, you can dynamically customzie the abstraction, build a path from shallow to deep for your developers per needs, adapt to growing number of different scenarios, and meet the iterative demands of the company's long-term business development.\r\n\r\n<!--truncate-->\r\n\r\n## Convert Kubernetes API Objects Into Custom Components\r\n\r\nLet's start the journey by using the [Kubernetes StatefulSet](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/) as example, we will convert it to be a customized module and provide capabilities.\r\n\r\nSave the YAML example of StatefulSet in the official document locally and name it as `my-stateful.yaml`, then execute command as below:\r\n\r\n\t vela def init my-stateful -t component --desc \"My StatefulSet component.\" --template-yaml ./my-stateful.yaml -o my-stateful.cue\r\n\r\nView the generated \"my-stateful.cue\" file:\r\n\r\n\t$ cat my-stateful.cue\r\n\t\"my-stateful\": {\r\n\t\tannotations: {}\r\n\t\tattributes: workload: definition: {\r\n\t\t\tapiVersion: \"<change me> apps/v1\"\r\n\t\t\tkind:       \"<change me> Deployment\"\r\n\t\t}\r\n\t\tdescription: \"My StatefulSet component.\"\r\n\t\tlabels: {}\r\n\t\ttype: \"component\"\r\n\t}\r\n\t\r\n\ttemplate: {\r\n\t\toutput: {\r\n\t\t\tapiVersion: \"v1\"\r\n\t\t\tkind:       \"Service\"\r\n\t\t\t\t... // omit non-critical info\r\n\t\t}\r\n\t\toutputs: web: {\r\n\t\t\tapiVersion: \"apps/v1\"\r\n\t\t\tkind:       \"StatefulSet\"\r\n\t\t\t\t... // omit non-critical info\r\n\t\t}\r\n\t\tparameter: {}\r\n\t}\r\n\r\nModify the generated file as follows:\r\n\r\n1. The example of the official StatefulSet website is a composite component composed of two objects `StatefulSet` and `Service`. According to KubeVela [Rules for customize component](https://kubevela.io/docs/platform-engineers/components/custom-component), in composite components, one of the resources such (StatefulSet in our example) need to be represented by the `template.output` field as a core workload, and other auxiliary objects are represented by `template.outputs`, so we make some adjustments and all the automatically generated output and outputs are switched.\r\n2. Then we fill in the apiVersion and kind data of the core workload into the part marked as `<change me>`\r\n\r\nAfter modification, you can use `vela def vet` to do format check and verification.\r\n\r\n\t$ vela def vet my-stateful.cue\r\n\tValidation succeed.\r\n\r\nThe file after two steps of changes is as follows:\r\n\r\n\t$ cat my-stateful.cue\r\n\t\"my-stateful\": {\r\n\t\tannotations: {}\r\n\t\tattributes: workload: definition: {\r\n\t\t\tapiVersion: \"apps/v1\"\r\n\t\t\tkind:       \"StatefulSet\"\r\n\t\t}\r\n\t\tdescription: \"My StatefulSet component.\"\r\n\t\tlabels: {}\r\n\t\ttype: \"component\"\r\n\t}\r\n\t\r\n\ttemplate: {\r\n\t\toutput: {\r\n\t\t\tapiVersion: \"apps/v1\"\r\n\t\t\tkind:       \"StatefulSet\"\r\n\t\t\tmetadata: name: \"web\"\r\n\t\t\tspec: {\r\n\t\t\t\tselector: matchLabels: app: \"nginx\"\r\n\t\t\t\treplicas:    3\r\n\t\t\t\tserviceName: \"nginx\"\r\n\t\t\t\ttemplate: {\r\n\t\t\t\t\tmetadata: labels: app: \"nginx\"\r\n\t\t\t\t\tspec: {\r\n\t\t\t\t\t\tcontainers: [{\r\n\t\t\t\t\t\t\tname: \"nginx\"\r\n\t\t\t\t\t\t\tports: [{\r\n\t\t\t\t\t\t\t\tname:          \"web\"\r\n\t\t\t\t\t\t\t\tcontainerPort: 80\r\n\t\t\t\t\t\t\t}]\r\n\t\t\t\t\t\t\timage: \"k8s.gcr.io/nginx-slim:0.8\"\r\n\t\t\t\t\t\t\tvolumeMounts: [{\r\n\t\t\t\t\t\t\t\tname:      \"www\"\r\n\t\t\t\t\t\t\t\tmountPath: \"/usr/share/nginx/html\"\r\n\t\t\t\t\t\t\t}]\r\n\t\t\t\t\t\t}]\r\n\t\t\t\t\t\tterminationGracePeriodSeconds: 10\r\n\t\t\t\t\t}\r\n\t\t\t\t}\r\n\t\t\t\tvolumeClaimTemplates: [{\r\n\t\t\t\t\tmetadata: name: \"www\"\r\n\t\t\t\t\tspec: {\r\n\t\t\t\t\t\taccessModes: [\"ReadWriteOnce\"]\r\n\t\t\t\t\t\tresources: requests: storage: \"1Gi\"\r\n\t\t\t\t\t\tstorageClassName: \"my-storage-class\"\r\n\t\t\t\t\t}\r\n\t\t\t\t}]\r\n\t\t\t}\r\n\t\t}\r\n\t\toutputs: web: {\r\n\t\t\tapiVersion: \"v1\"\r\n\t\t\tkind:       \"Service\"\r\n\t\t\tmetadata: {\r\n\t\t\t\tname: \"nginx\"\r\n\t\t\t\tlabels: app: \"nginx\"\r\n\t\t\t}\r\n\t\t\tspec: {\r\n\t\t\t\tclusterIP: \"None\"\r\n\t\t\t\tports: [{\r\n\t\t\t\t\tname: \"web\"\r\n\t\t\t\t\tport: 80\r\n\t\t\t\t}]\r\n\t\t\t\tselector: app: \"nginx\"\r\n\t\t\t}\r\n\t\t}\r\n\t\tparameter: {}\r\n\t}\r\n\r\nInstall ComponentDefinition into the Kubernetes cluster:\r\n\r\n\t$ vela def apply my-stateful.cue\r\n\tComponentDefinition my-stateful created in namespace vela-system.\r\n\r\nYou can see that a `my-stateful` component  via `vela components` command:\r\n\r\n\t$ vela components\r\n\tNAME       \tNAMESPACE  \tWORKLOAD                             \tDESCRIPTION\r\n\t...\r\n\tmy-stateful\tvela-system\tstatefulsets.apps                    \tMy StatefulSet component.\r\n\t... \r\n\r\nWhen you put this customized component into `Application`, it looks like:\r\n\r\n\tcat <<EOF | vela up -f -\r\n\tapiVersion: core.oam.dev/v1beta1\r\n\tkind: Application\r\n\tmetadata:\r\n\t  name: website\r\n\tspec:\r\n\t  components:\r\n\t    - name: my-component\r\n\t      type: my-stateful\r\n\tEOF\r\n\r\n\r\n\r\n## Define Customized Parameters For Component\r\n\r\nIn previous section we have defined a ComponentDefinition that has no parameter. In this section we will show how to expose parameters.\r\n\r\nIn this example, we expose the following parameters to the user:\r\n\r\n* Image name, allowing users to customize the image\r\n* Instance name, allowing users to customize the instance name of the generated StatefulSet object and Service object\r\n\r\n\r\n\t\t... # Omit other unmodified fields\r\n\t\ttemplate: {\r\n\t\t\toutput: {\r\n\t\t\t\tapiVersion: \"apps/v1\"\r\n\t\t\t\tkind:       \"StatefulSet\"\r\n\t\t\t\tmetadata: name: parameter.name\r\n\t\t\t\tspec: {\r\n\t\t\t\t\tselector: matchLabels: app: \"nginx\"\r\n\t\t\t\t\treplicas:    3\r\n\t\t\t\t\tserviceName: \"nginx\"\r\n\t\t\t\t\ttemplate: {\r\n\t\t\t\t\t\tmetadata: labels: app: \"nginx\"\r\n\t\t\t\t\t\tspec: {\r\n\t\t\t\t\t\t\tcontainers: [{\r\n\t\t\t\t\t\t\t\timage: parameter.image\r\n\t\t\t\r\n\t\t\t\t\t\t\t    ... // Omit other unmodified fields\r\n\t\t\t\t\t\t\t}]\r\n\t\t\t\t\t\t}\r\n\t\t\t\t\t}\r\n\t\t\t\t\t    ... // Omit other unmodified fields\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t\toutputs: web: {\r\n\t\t\t\tapiVersion: \"v1\"\r\n\t\t\t\tkind:       \"Service\"\r\n\t\t\t\tmetadata: {\r\n\t\t\t\t\tname: \"nginx\"\r\n\t\t\t\t\tlabels: app: \"nginx\"\r\n\t\t\t\t}\r\n\t\t\t\tspec: {\r\n\t\t\t\t\t... // Omit other unmodified fields\t\r\n\t\t\t    }\r\n\t\t\t}\r\n\t\t\tparameter: {\r\n\t\t\t\timage: string\r\n\t\t\t\tname: string\r\n\t\t\t}\r\n\t\t}\r\n\r\nAfter modification, use `vela def apply` to install to the cluster:\r\n\r\n\t$ vela def apply my-stateful.cue\r\n\tComponentDefinition my-stateful in namespace vela-system updated.\r\n\r\nThen as a platform builder, you have finished your setup. Let's see what's the developer experience now.\r\n\r\n## Developer Experience\r\n\r\nThe only thing your developer need to learn is the [Open Application Model](https://oam.dev/) which always follow a unified format.\r\n\r\n### Discover the Component\r\n\r\nThe developers can discover and check the parameters of `my-stateful` component as follows:\r\n\r\n```\r\n$ vela def list\r\nNAME                            TYPE                    NAMESPACE   DESCRIPTION\r\nmy-stateful                     ComponentDefinition     vela-system My StatefulSet component.\r\n...snip...\r\n```\r\n\r\n\t$ vela show my-stateful\r\n\t# Properties\r\n\t+----------+-------------+--------+----------+---------+\r\n\t|   NAME   | DESCRIPTION |  TYPE  | REQUIRED | DEFAULT |\r\n\t+----------+-------------+--------+----------+---------+\r\n\t| name     |             | string | true     |         |\r\n\t| image    |             | string | true     |         |\r\n\t+----------+-------------+--------+----------+---------+\r\n\r\nUpdating the ComponentDefinition will not affect existing Applications. It will take effect only after updating the Applications next time.\r\n\r\n### Use the Component in Application\r\n\r\nThe developers can easily specify the three new parameters in the application:\r\n\r\n```\r\n\tapiVersion: core.oam.dev/v1beta1\r\n\tkind: Application\r\n\tmetadata:\r\n\t  name: website\r\n\tspec:\r\n\t  components:\r\n\t    - name: my-component\r\n\t      type: my-stateful\r\n\t      properties:\r\n\t        image: nginx:latest\r\n\t        name: my-component\r\n```\r\n\r\nThe only thing left is to deploy the yaml file ( assume the name `app-stateful.yaml`) by executing `vela up -f app-stateful.yaml`.\r\n\r\nThen you can see that the name, image, and number of instances of the StatefulSet object have been updated.\r\n\r\n## Dry-run for diagnose or integration\r\n\r\nIn order to ensure that the developer's application can run correctly with the parameters, you can also use the `vela dry-run` command to verify the trial run of your template.\r\n\r\n```shell\r\nvela dry-run -f app-stateful.yaml\r\n```\r\n\r\nBy viewing the output, you can compare whether the generated object is consistent with the object you actually expect. You can even execute this YAML directly into the Kubernetes cluster and use the results of the operation for verification.\r\n\r\n<details>\r\n\r\n```\r\n# Application(website) -- Component(my-component)\r\n---\r\n\r\napiVersion: v1\r\nkind: Service\r\nmetadata:\r\n  labels:\r\n    app: nginx\r\n    app.oam.dev/appRevision: \"\"\r\n    app.oam.dev/component: my-component\r\n    app.oam.dev/name: website\r\n    workload.oam.dev/type: my-stateful\r\n  name: nginx\r\n  namespace: default\r\nspec:\r\n  clusterIP: None\r\n  ports:\r\n  - name: web\r\n    port: 80\r\n  selector:\r\n    app: nginx\r\n  template:\r\n    spec:\r\n      containers:\r\n      - image: saravak/fluentd:elastic\r\n        name: my-sidecar\r\n\r\n---\r\napiVersion: apps/v1\r\nkind: StatefulSet\r\nmetadata:\r\n  labels:\r\n    app.oam.dev/appRevision: \"\"\r\n    app.oam.dev/component: my-component\r\n    app.oam.dev/name: website\r\n    trait.oam.dev/resource: web\r\n    trait.oam.dev/type: AuxiliaryWorkload\r\n  name: web\r\n  namespace: default\r\nspec:\r\n  replicas: 3\r\n  selector:\r\n    matchLabels:\r\n      app: nginx\r\n  serviceName: nginx\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: nginx\r\n    spec:\r\n      containers:\r\n      - image: k8s.gcr.io/nginx-slim:0.8\r\n        name: nginx\r\n        ports:\r\n        - containerPort: 80\r\n          name: web\r\n        volumeMounts:\r\n        - mountPath: /usr/share/nginx/html\r\n          name: www\r\n      terminationGracePeriodSeconds: 10\r\n  volumeClaimTemplates:\r\n  - metadata:\r\n      name: www\r\n    spec:\r\n      accessModes:\r\n      - ReadWriteOnce\r\n      resources:\r\n        requests:\r\n          storage: 1Gi\r\n      storageClassName: my-storage-class\r\n```\r\n\r\n</details>\r\n\r\n\r\nYou can also use `vela dry-run -h` to view more available function parameters.\r\n\r\n\r\n## Use `context` to avoid duplication\r\n\r\nKubeVela allows you to reference the runtime information of your application via [`context` keyword](https://kubevela.io/docs/platform-engineers/components/custom-component#cue-context).\r\n\r\nIn our example above, the field `name` in the properties and the field `name` of the Component have the same meaning, so we can use the `context` to avoid duplication. We cann use the `context.name` to reference the component name in the runtime, thus the name parameter in `parameter` is no longer needed.\r\n\r\nJust modify the definition file (my-stateful.cue) as the following \r\n\r\n\r\n\t... # Omit other unmodified fields\r\n\ttemplate: {\r\n\t\toutput: {\r\n\t\t\tapiVersion: \"apps/v1\"\r\n\t\t\tkind:       \"StatefulSet\"\r\n    -       metadata: name: parameter.name\r\n\t+\t\tmetadata: name: context.name\r\n\r\n\t\t\t\t... // Omit other unmodified field\r\n\r\n\t\t}\r\n\t    parameter: {\r\n    -       name: string\r\n\t\t\timage: string\r\n\t\t}\r\n\t}\r\n\r\nThen deploy the changes by the following:\r\n\r\n```\r\nvela def apply my-stateful.cue\r\n```\r\n\r\nAfter that, the developers can immediately run application as below:\r\n\r\n```\r\n\tapiVersion: core.oam.dev/v1beta1\r\n\tkind: Application\r\n\tmetadata:\r\n\t  name: website\r\n\tspec:\r\n\t  components:\r\n\t    - name: my-component\r\n\t      type: my-stateful\r\n\t      properties:\r\n\t        image: \"nginx:latest\"\r\n```\r\n\r\nThere's no upgrade or restart for any system, they're all running into affect dynamically per your needs.\r\n\r\n## Add Operational Traits On Demand\r\n\r\nOAM follows the principle of \"separation of concerns\", after the developers finished the component part, the operators can add traits into the application to control the rest part configuration of the deployment. For example, the operators can  \r\ncontrol replicas, adding labels/annotations, injecting environment variables/sidecars, adding persistent volumes, and so on.\r\n\r\nTechnically, the trait system works in two ways:\r\n\r\n- Patch/Override the configurations defined in component.\r\n- Generate more configuration.\r\n\r\nThe customized process works the same with the component, they both use CUE but has some different keywords for path and override, you can refer to [customize trait](https://kubevela.io/docs/platform-engineers/traits/customize-trait) for details.\r\n\r\n## Operator Experience with traits\r\n\r\nThere're already some built-in traits after KubeVela installed. The operator can use `vela traits` to view, the traits marked with `*` are general traits, which can operate on common Kubernetes resource objects.\r\n\r\n\t$ vela traits\r\n\tNAME                    \tNAMESPACE  \tAPPLIES-TO      \tCONFLICTS-WITH\tPOD-DISRUPTIVE\tDESCRIPTION\r\n\tannotations             \tvela-system\t*               \t              \ttrue          \tAdd annotations on K8s pod for your workload which follows\r\n\t                        \t           \t                \t              \t              \tthe pod spec in path 'spec.template'.\r\n\tconfigmap               \tvela-system\t*               \t              \ttrue          \tCreate/Attach configmaps on K8s pod for your workload which\r\n\t                        \t           \t                \t              \t              \tfollows the pod spec in path 'spec.template'.\r\n\tlabels                  \tvela-system\t*               \t              \ttrue          \tAdd labels on K8s pod for your workload which follows the\r\n\t                        \t           \t                \t              \t              \tpod spec in path 'spec.template'.\r\n\tscaler                  \tvela-system\t*               \t              \tfalse         \tManually scale K8s pod for your workload which follows the\r\n\t                        \t           \t                \t              \t              \tpod spec in path 'spec.template'.\r\n\tsidecar                 \tvela-system\t*               \t              \ttrue          \tInject a sidecar container to K8s pod for your workload\r\n\t                        \t           \t                \t              \t              \twhich follows the pod spec in path 'spec.template'.\r\n\t...snip...\r\n\r\nTaking sidecar as an example, you can check the usage of sidecar:\r\n\r\n\t$ vela show sidecar\r\n\t# Properties\r\n\t+---------+-----------------------------------------+-----------------------+----------+---------+\r\n\t|  NAME   |               DESCRIPTION               |         TYPE          | REQUIRED | DEFAULT |\r\n\t+---------+-----------------------------------------+-----------------------+----------+---------+\r\n\t| name    | Specify the name of sidecar container   | string                | true     |         |\r\n\t| cmd     | Specify the commands run in the sidecar | []string              | false    |         |\r\n\t| image   | Specify the image of sidecar container  | string                | true     |         |\r\n\t| volumes | Specify the shared volume path          | [[]volumes](#volumes) | false    |         |\r\n\t+---------+-----------------------------------------+-----------------------+----------+---------+\r\n\t\r\n\t\r\n\t## volumes\r\n\t+------+-------------+--------+----------+---------+\r\n\t| NAME | DESCRIPTION |  TYPE  | REQUIRED | DEFAULT |\r\n\t+------+-------------+--------+----------+---------+\r\n\t| path |             | string | true     |         |\r\n\t| name |             | string | true     |         |\r\n\t+------+-------------+--------+----------+---------+\r\n\r\nUse the sidecar directly to inject a container, the application description is as follows:\r\n\r\n\tapiVersion: core.oam.dev/v1beta1\r\n\tkind: Application\r\n\tmetadata:\r\n\t  name: website\r\n\tspec:\r\n\t  components:\r\n\t    - name: my-component\r\n\t      type: my-stateful\r\n\t      properties:\r\n\t        image: nginx:latest\r\n\t        name: my-component\r\n\t      traits:\r\n\t      - type: sidecar\r\n\t        properties:\r\n\t          name: my-sidecar\r\n\t          image: saravak/fluentd:elastic\r\n\r\nDeploy and run the application, and you can see that a fluentd sidecar has been deployed and running in the StatefulSet.\r\n\r\nBoth components and traits are re-usable on any KubeVela systems, we can package components, traits along with the CRD controllers together as an addon. There're [a growing catalog of addons](https://github.com/kubevela/catalog) in the community. \r\n\r\n## Summarize\r\n\r\nThis blog introduces how to deliver complete modular capabilities through CUE. The core is that it can dynamically increase configuration capabilities according to user needs, and gradually expose more functions and usages, so as to reduce the overall learning threshold for users and ultimately improve R&D efficient.\r\nThe out-of-the-box capabilities provided by KubeVela, including components, traits, policy, and workflow, are also designed as plugable and modifiable capabilities."
    },
    {
      "id": "/2022/04/24/blog-1.3",
      "metadata": {
        "permalink": "/blog/2022/04/24/blog-1.3",
        "editUrl": "https://github.com/kubevela/kubevela.io/tree/main/blog/2022-04-24-blog-1.3.md",
        "source": "@site/blog/2022-04-24-blog-1.3.md",
        "title": "KubeVela v1.3 released, CNCF's Next Generation of Cloud Native Application Delivery Platform",
        "description": "",
        "date": "2022-04-24T00:00:00.000Z",
        "formattedDate": "April 24, 2022",
        "tags": [
          {
            "label": "KubeVela",
            "permalink": "/blog/tags/kube-vela"
          },
          {
            "label": "release-note",
            "permalink": "/blog/tags/release-note"
          }
        ],
        "readingTime": 15.84,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "KubeVela Community",
            "title": "KubeVela Team",
            "url": "https://github.com/kubevela/KubeVela",
            "imageURL": "https://KubeVela.io/img/logo.svg"
          }
        ],
        "frontMatter": {
          "title": "KubeVela v1.3 released, CNCF's Next Generation of Cloud Native Application Delivery Platform",
          "author": "KubeVela Community",
          "author_title": "KubeVela Team",
          "author_url": "https://github.com/kubevela/KubeVela",
          "author_image_url": "https://KubeVela.io/img/logo.svg",
          "tags": [
            "KubeVela",
            "release-note"
          ],
          "description": "",
          "image": "https://raw.githubusercontent.com/oam-dev/KubeVela.io/main/docs/resources/KubeVela-03.png",
          "hide_table_of_contents": false
        },
        "prevItem": {
          "title": "Build flexible abstraction for any Kubernetes Resources with CUE and KubeVela",
          "permalink": "/blog/2022/05/30/abstraction-kubevela"
        },
        "nextItem": {
          "title": "Easily Manage your Application Shipment With Differentiated Configuration in Multi-Cluster",
          "permalink": "/blog/2022/04/06/multi-cluster-management"
        }
      },
      "content": "Thanks to the contribution of hundreds of developers from KubeVela community and around 500  PRs from more than 30 contributors, KubeVela version 1.3 is officially released. Compared to[ v1.2 released](https://kubevela.io/blog/2022/01/27/blog-1.2) three months ago, this version provides a large number of new features in three aspects as OAM engine (Vela Core), GUI dashboard (VelaUX) and addon ecosystem. These new features are derived from the in-depth practice of many end users such as Alibaba, LINE, China Merchants Bank, and iQiyi, and then finally become part of the KubeVela project that everyone can use out of the box.\r\n\r\n## Pain Points of Application Delivery\r\nSo, what challenges have we encountered in cloud-native application delivery?\r\n\r\n<!--truncate-->\r\n\r\n### Hybrid clouds and multi-clusters is the new norm\r\nOn one hand, as global cloud providers' service maturing, the way most enterprises build infrastructure has become mainly replying on cloud providers and self-built as a supplement. More and more business enterprise can directly enjoy the business convenience brought by the development of cloud technology, use the elasticity of the cloud, and reduce the cost of self-built infrastructure. Enterprises need a standardized application delivery layer, which can include containers, cloud services and various self-built services in a unified manner, so as to easily achieve cloud-to-cloud interoperability, reduce the risks brought by tedious application migration, and worry-free cloud migration.\r\n\r\nOn the other hand, for security concerns such as infrastructure stability and multi-environment isolation and due to [limitations by the maximized size of Kubernetes can handle](https://kubernetes.io/docs/setup/best-practices/cluster-large/), more and more enterprises are beginning to adopt multiple Kubernetes clusters to manage container workloads. How to manage and orchestrate container applications at the multi-cluster level, and solve problems such as scheduling, dependencies, versions, and gray releasing, while providing business developers with a low-threshold experience, is a big challenge.\r\n\r\nIt can be seen that the hybrid cloud and multi-cluster involved in modern application delivery are not only multiple Kubernetes clusters, but also diverse workloads and DevOps capabilities for managing cloud services, SaaS, and self-built services.\r\n\r\n### How to pick from more than 1000+ techniques in cloud-native era\r\nLet's take the open-source projects that have joined the CNCF ecosystem as an example, the number of which has exceeded 1,000. For teams of different scales, different industries, and different technical backgrounds, it seems that the R&D team is doing similar business application delivery and management, but with changes in requirements and usage scenarios, huge differences in technology stacks will be derived. This involves a very large learning cost and threshold for integration and migration. And CNCF's thousands of ecological projects are always tempting us to integrate new projects, add new features, and better accomplish business goals. The era of static technology stacks is long gone.\r\n\r\n![alt](/img/cncf-landscape.jpg)\r\n_Figure 1. CNCF landscape_\r\n\r\nNext-generation application delivery and management require flexible assembly capabilities. According to the needs of the team, based on the minimum capability set, new functions can be expanded at a small cost, but not significantly enlarged. The traditional PaaS solution based only on a set of fixating experiences has been proven to be difficult to meet the changing scenario needs of a team during product evolution.\r\n\r\n### Next step of DevOps, delivering and managing applications for diverse infrastructures\r\nFor more than a decade, DevOps technology has been evolving to increase productivity. Today, the production process of business applications has also undergone great changes, from the traditional way of coding, testing, packaging, deployment, maintenance, and observation, to the continuous enhancement of cloud infrastructure meaning various SaaS services based on API directly become an integral part of the application. From the diversification of development languages to the diversification of deployment environments, to the diversification of components, the traditional DevOps toolchain is gradually unable to cope with and meanwhile, the complexity of user needs is increasing exponentially.\r\n\r\nAlthough DevOps prolongs, we need some different solutions. For modern application delivery and management, we still have the same pursuit of reducing human input as much as possible and becoming more intelligent. The new generation of DevOps technology needs to have easier-to-use integration capabilities, service mesh capabilities, and management capabilities that integrate observation and maintenance. At the same time, the tools need to be simple and easy to use, and the complexity stays within the platform. When choosing, enterprises can combine their own business needs, cooperate with the new architecture and legacy systems, and assemble a platform solution suitable for their team, to avoid the new platform becoming a burden for business developers or enterprises.\r\n\r\n## The Path of KubeVela Lies Ahead\r\n\r\nTo build the next generation application delivery platform, we do:\r\n![alt](/img/overlook-of-kubevela-en.png)\r\n_Figure 2. Overlook of OAM/KubeVela ecosystem_\r\n### \r\n### OAM(Open Application Model): evolving methodology in fast pacing practice\r\nBased on the internal practical experience of Alibaba and Microsoft, we launched OAM, a brand-new application model and concept in 2019. Its core idea lies in the separation of concerns, through the unified abstraction of components and traits, it can standardize business research and development in the cloud-native era. Collaboration between development team and DevOps team becomes more efficient, and at the same time we expect to avoid the complexity caused by differences in different infrastructures. We then released KubeVela as a standardized implementation of the OAM model to help companies quickly implement OAM while ensuring that OAM-compliant applications can run anywhere. In short, OAM describes the complete components of a modern application in a declarative way, while KubeVela runs according to the final state declared by OAM. Through the reconcile loop oriented to the final state, the two jointly ensure the consistency and correctness of application delivery.\r\n\r\nRecently, we have seen a paper published by Google announcing the results of its internal learning in infrastructure construction named as \"[Prodspec and Annealing](https://www.usenix.org/publications/loginonline/prodspec-and-annealing-intent-based-actuation-google-production)\". Its design concept and practice are strikingly similar to \"OAM and KubeVela\". It can be seen that different enterprises in global shares the same vision for delivering cloud-native applications. This paper also re-confirm the correctness of the standardized model and KubeVela. In the future, we will continue to promote the development of the OAM model based on the community's practice and evolution of KubeVela, and continue to deposit best practices into methodology.\r\n\r\n### A universal hybrid environment and multi-cluster delivery control plane\r\nThe kernel of KubeVela exists in the form of a [CRD Controller](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/), which can be easily integrated with the Kubernetes ecosystem, and the OAM model is also compatible with the Kubernetes API. In addition to the abstraction and orchestration capabilities of the OAM model, KubeVela's microkernel is also a natural application delivery control plane designed for multi-cluster and hybrid cloud environments. This also means that KubeVela can seamlessly connect diverse workloads such as cloud resources and containers, and orchestrate and deliver them in different clouds and clusters.\r\n\r\nIn addition to the basic orchestration capabilities, one core feature of KubeVela is that it allows users to customize the delivery workflow. The workflow steps provide deploying components to the cluster, setting up manual approval, sending notifications, etc. When the workflow execution enters a stable state (such as waiting for manual approval), KubeVela will also automatically maintain the state. Or, through the CUE-based configuration language, you can integrate any IaC-based process, such as Kubernetes CRD, SaaS API, Terraform module, image script, etc. KubeVela's IaC extensibility enables it to integrate Kubernetes' ecological technology at a meager cost. It is very quickly for platform builders to incorporate into their own PaaS or delivery systems. Also, through KubeVela's powerful extensibility, other ecological capabilities can be standardized for enterprise users.\r\n\r\nIn addition to the advanced model and extended kernel, we've also heard a lot from the community to call out an out-of-the-box product that makes using KubeVela easier. Since version 1.2, the community has invested in developing the GUI dashboard (VelaUX) project, based on KubeVela's microkernel, running on top of the OAM model and creating a delivery platform for CI/CD scenarios. We hope that enterprises can swiftly adopt VelaUX to meet business needs and have a robust, extensible ability to meet the needs of future businesses.\r\n![alt](/img/product-architecture-en.png)\r\n_Figure 3.  Product architecture of KubeVela_\r\n\r\nAround this path, in version 1.3, the community brought the following updates:\r\n## Enhancement as a Kubernetes Multi-Cluster Control Plane \r\n### No migration and switch to multi-cluster seamlessly\r\nAfter the enterprise has completed the application transformation to a cloud-native architecture, is it still necessary to perform configuration transformation when switching to multi-cluster deployment? The answer is negative.\r\n\r\nKubeVela is naturally built upon a multi-cluster basis. As shown in Figure 4, this application YAML represents an application of the Nginx component that will be published to all clusters labeled as `region=hangzhou`. For the same application description, we only need to specify the name of the cluster to be delivered in Policy or filter specific collections by tags.\r\n![alt](/img/select-deployment-cluster.png)\r\n_Figure 4. OAM application - select deployment cluster_\r\n\r\nOf course, the application description shown in Figure 4 is entirely based on the OAM specification. If your current application has been defined in Kubernetes native resources, don't worry, we support the smooth transition from it, as shown in Figure 5 below, \"Referencing Kubernetes resources for multi-cluster deployment,\" which describes a particular application whose components depend on a Secret resource that exists in the control cluster and publishes it to all clusters labeled as `region=hangzhou`.\r\n![alt](/img/ref-k8s-native.png)\r\n_Figure 5. Reference Kubernetes native resource_\r\n\r\nIn addition to multi-cluster deployment of applications, referencing Kubernetes objects can also be used in scenarios such as multi-cluster replication of existing resources, cluster data backup, etc.\r\n\r\n### Handling multi-cluster differences\r\nAlthough the application has been described in a unified OAM Model, there may be differences in the deployment of different clusters. For example, other regions use different environment variables and image registries. Different clusters deploy various components, or a component is deployed in multiple clusters but works as high availability for all, etc. For such requirements, we provide a deployment strategy to do differentiated configuration, as shown in Figure 6 below, as part of this kind of Policy. The first and second topology types of Policy define two target strategies in two ways. The third one means to deploy only the specified components. The fourth Policy represents the deployment of the selected two kinds of components and the difference in the image configuration of one of the components.\r\n![alt](/img/diff-configuration.png)\r\n_Figure 6. Differentiated configuration of multi-clusters_\r\n\r\nKubeVela supports flexible differential configuration policies, which can be configured by component properties, traits, and other forms. As shown in the figure above, the third strategy describes the component selection ability, and the fourth strategy describes the difference between the image version. We can see that there is no target specified when describing the difference. The differentiated configuration can be patched flexibly by combining it into the workflow steps.\r\n\r\n### Configure a multi-cluster delivery process\r\nThe application delivery process to different target clusters is controllable and described by workflow. As shown in Figure 7, the steps of deploying to two clusters and the target policy and differentiation strategy were adopted, respectively. The above shows that policy deployment only needs to be defined atomically and can be flexibly combined in the workflow steps to meet the requirements of different scenarios.\r\n![alt](/img/customize-multi-cluster.png)\r\n_Figure 7.  Customize the multi-cluster delivery process_\r\n\r\nThere are many more usages for delivery workflow, including multi-cluster canary release, manual approval, precise release control, etc.\r\n\r\n### Version control, safe and traceable\r\nThe description of complex applications is changing at any time with agile development. To ensure the security of application release, we need to have the ability to roll back our application to a previous correct state at the time of release or after release. Therefore, we have introduced a more robust versioning mechanism in the current version.\r\n![alt](/img/query-history-verison.png)\r\n_Figure 8. Querying historical version of the application_\r\n\r\nWe can query every past version of an application, including its release time and whether it was successful or not. We can compare the changes between versions and quickly roll back based on the snapshot rendered by the previous successful version when we encounter a failure during the release. After releasing a new version, you don't need to change the configuration source if it fails. You can directly re-release based on a history version.\r\nThe version control mechanism is the centralized idea of application configuration management. After the complete description of the application is rendered uniformly, it is checked, stored, and distributed.\r\n### See more Vela Core usages\r\n\r\n- Multi-cluster application delivery: [https://kubevela.io/docs/case-studies/multi-cluster](https://kubevela.net/zh/docs/case-studies/multi-cluster)\r\n- References external Kubernetes objects: [https://kubevela.io/docs/end-user/components/ref-objects](https://kubevela.net/zh/docs/end-user/components/ref-objects)\r\n- Application version management: [https://kubevela.io/docs/end-user/version-control](https://kubevela.net/docs/end-user/version-control)\r\n\r\n## VelaUX Introduces Multi-Tenancy Isolation and User  Authentication\r\n### Multi-tenancy and isolation for enterprises\r\nIn VelaUX, we introduce the concept of a Project that separate multi-tenancy for safety, including application delivery targets, environments, team members and permissions, etc. Figure 9 shows the project list page. Project administrators can create different projects on this page according to the team's needs to allocate corresponding resources. This capability becomes very important when there are multiple teams or multiple project groups in the enterprise publishing their business applications using the VelaUX platform simultaneously.\r\n![alt](/img/project-management.png)\r\n_Figure 9. Project management page_\r\n### Open Authentication & RBAC\r\nAs a vital platform, user authentication is one of the basic capabilities that must be possessed. Since version 1.3, we have supported user authentication and RBAC authentication.\r\n\r\nWe believe that most enterprises have built a unified authentication platform (Oauth or LDAP) for user authentication. Therefore, VelaUX prioritizes Dex getting through the single sign-on capability, supports LDAP, OIDC, Gitlab/Github, and other user authentication methods, and regards VelaUX as one of the sub portals that let access get through. Of course, if your team does not need unified authentication, we also provide basic local user authentication capabilities.\r\n![alt](/img/local-user-management.png)\r\n_Figure 10. Local user management_\r\n\r\nFor authentication, we use the RBAC model. Still, we also saw that the primary RBAC mode could not handle more precise permission control scenarios, such as authorizing the operation rights of an application to specific users. We inherit the design concept of IAM and expand the permissions to the policy composition of resource + action + condition + behavior. The authentication system (front-end UI authentication/back-end API authentication) has implemented policy-oriented fine-grained authentication. However, in terms of authorization, the current version only has some built-in standard permission policies, and subsequent versions provide the ability to create custom permissions.\r\n\r\nAt the same time, we have also seen that some large enterprises have built independent IAM platforms. The RBAC data model of VelaUX is the same as that of common IAM platforms. Therefore, users who wish to connect VelaUX to their self-built IAM can extend seamlessly.\r\n### More secure centralized DevOps \r\nThere will inevitably be some configuration management of operation and maintenance requirements in application delivery, primarily based on multi-cluster. The configuration management requirements are particularly prominent, such as the authentication configuration of the private image repository, the authentication configuration of the Helm product library, or the SSL certificate Wait. We need to uniformly manage the validity of these configurations and securely synchronize them where they are needed, preferably without business developer awareness.\r\n\r\nIn version 1.3, we introduced a module for integrated configuration management in VelaUX. Its bottom layer also uses component templates and application resource distribution links to manage and distribute configurations. Currently, Secret is used for configuration storage and distribution. The configuration lifecycle is independent of business applications, and we maintain the configuration distribution process independently in each project. You only need to fill in the configuration information for administrator users according to the configuration template.\r\n![alt](/img/integration-configuration.png)\r\n_Figure 11. Integration configuration_\r\n\r\nVarious Addons provide different configuration types, and users can define more configuration types according to their needs and manage them uniformly. For business-level configuration management, the community is also planning.\r\n\r\n### See more VelaUX usages\r\n\r\n- Project management: [https://kubevela.io/docs/how-to/dashboard/user/project](https://kubevela.net/zh/docs/how-to/dashboard/user/project)\r\n- User management: [https://kubevela.io/docs/how-to/dashboard/user/user](https://kubevela.net/zh/docs/how-to/dashboard/user/user)\r\n- RBAC authorization: [https://kubevela.io/docs/how-to/dashboard/user/rbac](https://kubevela.net/zh/docs/how-to/dashboard/user/rbac)\r\n- Use single sign-on: [https://kubevela.io/docs/tutorials/sso](https://kubevela.net/zh/docs/tutorials/sso)\r\n- Configuration management: [https://kubevela.io/docs/how-to/dashboard/config/dex-connectors](https://kubevela.net/zh/docs/how-to/dashboard/config/dex-connectors)\r\n\r\n## Introducing version control in Addon ecosystem\r\nThe Addon function was introduced in version 1.2, providing an extended plug-in specification, installation, operation, and maintenance management capabilities. The community can expand the ecological capacities of KubeVela by making different Addons. When our plug-ins and frameworks are constantly iterating, the problem of version compatibility gradually emerges, and we urgently need a version management mechanism.\r\n\r\n- Addon version distribution: We develop and manage the community's official Addon on Github. In addition to the integrated third-party product version, each Addon also includes a Definition and other configurations. Therefore, after each Addon is released, we define it according to its Definition. The version number is packaged, and the history is preserved. At the same time, we reused Helm Chart's product distribution API specification to distribute Addon.\r\n- Addon version distribution: We develop and manage the community's official Addon on Github. In addition to the integrated third-party product version, each Addon also includes a Definition and other configurations. Therefore, after each Addon is released, we define it according to its Definition. The version number is packaged, and the history is preserved. At the same time, we reused Helm Chart's product distribution API specification to distribute Addon.\r\n\r\n### Multi-cluster Addon controllable installation\r\nA type of Addon needs to be installed in the subcluster when installing, such as the FluxCD plug-in shown in Figure 12, which provides Helm Chart rendering and deployment capabilities. We need to deploy it to sub-clusters, and in the past, this process was distributed to all sub-clusters. However, according to community feedback, different plug-ins do not necessarily need to be installed in all clusters. We need a differential processing mechanism to install extensions to specified clusters on demand.\r\n![alt](/img/addon-configuration.png)\r\nFigure 12 Addon configuration\r\n\r\nThe user can specify the cluster to be deployed when enabling Addon, and the system will deploy the Addon according to the user's configuration.\r\n\r\n### New members to Addon ecosystem\r\nWhile iteratively expanding the framework's capabilities, the existing Addons in the community are also continuously being added and upgraded. The number of supported vendors has increased to seven at the cloud service support level. Ecological technology, AI training and service plug-ins, Kruise Rollout plug-ins, Dex plug-ins, etc., have been added. At the same time, the Helm Chart plug-in and the OCM cluster management plug-in have also been updated for user experience.\r\n\r\n### More Addon usages\r\n\r\n- Addon intro：[https://kubevela.io/docs/how-to/cli/addon/addon](https://kubevela.net/zh/docs/how-to/cli/addon/addon)\r\n\r\n## Recent roadmap\r\nAs KubeVela core becomes more and more stable, its scalability is unleashed gradually. The evolution of the 1.2/1.3 version of the community has been accelerated. In the future, we will iterate progressively new versions in a two-month cycle. In the next 1.4 release, we will add the following features:\r\n\r\n- Observability: Provide a complete observability solution around logs, metrics, and traces, provide out-of-the-box observability of the KubeVela system, allow custom observability configuration, and integrate existing observability components or cloud resources.\r\n- Offline installation: Provide relatively complete offline installation tools and solutions to facilitate more users to use KubeVela in an offline environment.\r\n- Multi-cluster permission management: Provides in-depth permission management capabilities for Kubernetes multi-cluster.\r\n- More out-of-the-box Addon capabilities.\r\n\r\nThe KubeVela community is looking forward to your joining to build an easy-to-use and standardized next-generation cloud-native application delivery and management platform!"
    },
    {
      "id": "/2022/04/06/multi-cluster-management",
      "metadata": {
        "permalink": "/blog/2022/04/06/multi-cluster-management",
        "editUrl": "https://github.com/kubevela/kubevela.io/tree/main/blog/2022-04-06-multi-cluster-management.md",
        "source": "@site/blog/2022-04-06-multi-cluster-management.md",
        "title": "Easily Manage your Application Shipment With Differentiated Configuration in Multi-Cluster",
        "description": "",
        "date": "2022-04-06T00:00:00.000Z",
        "formattedDate": "April 6, 2022",
        "tags": [
          {
            "label": "KubeVela",
            "permalink": "/blog/tags/kube-vela"
          },
          {
            "label": "use-case",
            "permalink": "/blog/tags/use-case"
          }
        ],
        "readingTime": 6.185,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Wei Duan",
            "title": "KubeVela Team",
            "url": "https://github.com/kubevela/kubevela",
            "imageURL": "https://KubeVela.io/img/logo.svg"
          }
        ],
        "frontMatter": {
          "title": "Easily Manage your Application Shipment With Differentiated Configuration in Multi-Cluster",
          "author": "Wei Duan",
          "author_title": "KubeVela Team",
          "author_url": "https://github.com/kubevela/kubevela",
          "author_image_url": "https://KubeVela.io/img/logo.svg",
          "tags": [
            "KubeVela",
            "use-case"
          ],
          "description": "",
          "image": "https://raw.githubusercontent.com/oam-dev/KubeVela.io/main/docs/resources/KubeVela-03.png",
          "hide_table_of_contents": false
        },
        "prevItem": {
          "title": "KubeVela v1.3 released, CNCF's Next Generation of Cloud Native Application Delivery Platform",
          "permalink": "/blog/2022/04/24/blog-1.3"
        },
        "nextItem": {
          "title": "China Merchants Bank's Practice on Offline Installation with KubeVela",
          "permalink": "/blog/2022/04/01/offline-deployment-practice"
        }
      },
      "content": "Under today's multi-cluster business scene, we often encounter these typical requirements: distribute to multiple specific clusters, specific group distributions according to business need, and differentiated configurations for multi-clusters.\r\n\r\nKubeVela v1.3 iterates based on the previous multi-cluster function. This article will reveal how to use it to do swift multiple clustered deployment and management to address all your anxieties.\r\n\r\n<!--truncate-->\r\n\r\n### Before Starting\r\n\r\n1. Prepare a Kubernetes cluster as the control plane of KubeVela.\r\n1. Make sure [KubeVela v1.3](https://github.com/kubevela/kubevela/releases/tag/v1.3.0) and KubeVela CLI v1.3.0 have been installed successfully.\r\n2. The list of Kubeconfig from sub clusters that you want to manage. We will take three clusters naming beijing-1, beijing-2 and us-west-1 as examples.\r\n3. Download and combine with [Multi-Cluster-Demo](https://github.com/kubevela/sample/tree/master/12.multi_cluster_demo) to better understand how to use the KubeVela multi-cluster capabilities.\r\n\r\n### Distribute to Multiple Specified Clusters\r\nDistributing multiple specified clusters is the most basic multi-cluster management operation. In KubeVela, you will use a policy called `topology` to implement it. The cluster will be listed in the attribute `clusters`, an array.\r\n\r\nFirst let's make sure switching kubeconfig to the control plane cluster, go with `vela cluster join` to include in the 3 clusters of Beijing-1, Beijing-2 and us-west-1:\r\n```\r\n➜   vela cluster join beijing-1.kubeconfig --name beijing-1\r\n➜   vela cluster join beijing-2.kubeconfig --name beijing-2\r\n➜   vela cluster join us-west-1.kubeconfig --name us-west-1\r\n➜   vela cluster list\r\nCLUSTER        \tTYPE           \tENDPOINT                 \tACCEPTED\tLABELS\r\nbeijing-1      \tX509Certificate\thttps://47.95.22.71:6443 \ttrue\r\nbeijing-2      \tX509Certificate\thttps://47.93.117.83:6443\ttrue\r\nus-west-1      \tX509Certificate\thttps://47.88.31.118:6443\ttrue\r\n```\r\nThen open multi-cluster-demo, look into `Basic.yaml`:\r\n```\r\napiVersion: core.oam.dev/v1beta1\r\nkind: Application\r\nmetadata:\r\n  name: example-app\r\n  namespace: default\r\nspec:\r\n  components:\r\n    - name: hello-world-server\r\n      type: webservice\r\n      properties:\r\n        image: oamdev/hello-world\r\n        port: 8000\r\n      traits:\r\n        - type: scaler\r\n          properties:\r\n            replicas: 3\r\n        - type: gateway\r\n          properties:\r\n            domain: testsvc-mc.example.com\r\n            # classInSpec : true   If the sub clusters has Kubernetes versions below v1.20 installed, please add this field\r\n            http:\r\n              \"/\": 8000\r\n  policies:\r\n    - type: topology\r\n      name: beijing-clusters\r\n      properties:\r\n        clusters: [\"beijing-1\",\"beijing-2\"]\r\n```\r\nIt can be seen that this app uses the component of type `webservice` and distributes 3 Deployments to beijing-1 and beijing-2 clusters through the `topology` policy.\r\n\r\nPlease note that the premise of successfully distributing resource into managed clusters is that it must contain the exactly same namespace as control plane did.  Since each cluster has the **`default`** namespace by default, we won't be worry in this case. But suppose we change the namespace in **`basic.yaml`** to be **`multi-cluster`**, we will receive an error:\r\n```\r\n... \r\n Status:    \trunningWorkflow\r\n\r\nWorkflow:\r\n\r\n  mode: DAG\r\n  finished: false\r\n  Suspend: false\r\n  Terminated: false\r\n  Steps\r\n  - id:9fierfkhsc\r\n    name:deploy-beijing-clusters\r\n    type:deploy\r\n    phase:failed\r\n    message:step deploy: step deploy: run step(provider=oam,do=components-apply): Found 1 errors. [(failed to apply component beijing-1-multi-cluster-0: HandleComponentsRevision: failed to create componentrevision beijing-1/multi-cluster/hello-world-server-v1: namespaces \"multi-cluster\" not found)]\r\n\r\nServices:\r\n...\r\n```\r\n\r\n**In future versions of KubeVela, we plan to support a comprehensive Authentication System, more convenient and more securely to: create namespaces in managed cluster through the hub cluster in quick moves.**\r\n\r\nAfter creating the sub cluster's namespace, come back to the control plane cluster to create the application and ship out resources:\r\n```\r\n➜   vela up -f basic.yaml\r\nApplying an application in vela K8s object format...\r\n\"patching object\" name=\"example-app\" resource=\"core.oam.dev/v1beta1, Kind=Application\"\r\n✅ App has been deployed 🚀🚀🚀\r\n    Port forward: vela port-forward example-app\r\n             SSH: vela exec example-app\r\n         Logging: vela logs example-app\r\n      App status: vela status example-app\r\n  Service status: vela status example-app --svc hello-world-server\r\n```\r\nWe use `vela status <App Name>` to view detailed infos about this app:\r\n```\r\n➜   vela status example-app\r\nAbout:\r\n\r\n  Name:      \texample-app\r\n  Namespace: \tdefault\r\n  Created at:\t2022-03-25 17:42:33 +0800 CST\r\n  Status:    \trunning\r\n\r\nWorkflow:\r\n\r\n  mode: DAG\r\n  finished: true\r\n  Suspend: false\r\n  Terminated: false\r\n  Steps\r\n  - id:wftf9d4exj\r\n    name:deploy-beijing-clusters\r\n    type:deploy\r\n    phase:succeeded\r\n    message:\r\n\r\nServices:\r\n\r\n  - Name: hello-world-server\r\n    Cluster: beijing-1  Namespace: default\r\n    Type: webservice\r\n    Healthy Ready:3/3\r\n    Traits:\r\n      ✅ scaler      ✅ gateway: Visiting URL: testsvc-mc.example.com, IP: 60.205.222.30\r\n  - Name: hello-world-server\r\n    Cluster: beijing-2  Namespace: default\r\n    Type: webservice\r\n    Healthy Ready:3/3\r\n    Traits:\r\n      ✅ scaler      ✅ gateway: Visiting URL: testsvc-mc.example.com, IP: 182.92.222.128\r\n```\r\nBoth the beijing-1 and beijing-2 have issued the corresponding resources, they also displayed external access IP addresses, and you can therefore make it public for your users.\r\n\r\n### Use Cluster Labels to Do Grouping\r\nIn addition to the above basic need, we often encounter additional situations: cross-regional deployment to certain clusters, specify which cloud provider's cluster, etc. In order to achieve a similar goal, the `labels` feature can be used.\r\n\r\nHere, suppose the us-west-1 cluster comes from AWS, we must additionally apply to the AWS cluster. You can use the `vela cluster labels add` command to tag the cluster. Of course, if there is more of AWS related clusters such as us-west-2, it will be handled as well after they were labeled:\r\n```\r\n➜  ~ vela cluster labels add us-west-1 provider=AWS\r\nSuccessfully update labels for cluster us-west-1 (type: X509Certificate).\r\nprovider=AWS\r\n➜  ~ vela cluster list\r\nCLUSTER        \tTYPE           \tENDPOINT                 \tACCEPTED\tLABELS\r\nbeijing-1      \tX509Certificate\thttps://47.95.22.71:6443 \ttrue\r\nbeijing-2      \tX509Certificate\thttps://47.93.117.83:6443\ttrue\r\nus-west-1      \tX509Certificate\thttps://47.88.31.118:6443\ttrue    \tprovider=AWS\r\n```\r\nNext we update the `basic.yaml` to add an application policy `topology-aws`:\r\n```\r\n...\r\n  policies:\r\n    - type: topology\r\n      name: beijing-clusters\r\n      properties:\r\n        clusters: [\"beijing-1\",\"beijing-2\"]\r\n    - type: topology\r\n      name: topology-aws\r\n      properties:\r\n        clusterLabelSelector:\r\n          provider: AWS\r\n```\r\nIn order save your time, please deploy `intermediate.yaml` directly:\r\n```\r\n➜  ~ vela up -f intermediate.yaml\r\n```\r\nReview the status of the application again:\r\n```\r\n➜   vela status example-app\r\n\r\n...\r\n\r\n  - Name: hello-world-server\r\n    Cluster: us-west-1  Namespace: default\r\n    Type: webservice\r\n    Healthy Ready:3/3\r\n    Traits:\r\n      ✅ scaler      ✅ gateway: Visiting URL: testsvc-mc.example.com, IP: 192.168.40.10\r\n\r\n```\r\n### Differentiated Configuration\r\n\r\nApart from above scenarios, we tend to have more application strategic needs, such as high availability of hoping to distribute 5 replicas. In this case, use the `override` policy:\r\n```\r\n...        \r\n        clusterLabelSelector:\r\n          provider: AWS\r\n    -  type: override\r\n       name: override-high-availability\r\n       properties:\r\n          components:\r\n            - type: webservice\r\n              traits:\r\n              - type: scaler\r\n                properties:\r\n                  replicas: 5\r\n```\r\nAt the same time, we hope that only AWS clusters can get high availability. Then we can expect KubeVela's workflow give us a hand. We use the following workflow: it aims to deploy this app by, first distributing to Beijing's clusters through the `deploy-beijing` policy, then distributing 5 copies to clusters which were labeled as AWS:\r\n```\r\n...                \r\n                properties:\r\n                  replicas: 5\r\n  workflow:\r\n    steps:\r\n      - type: deploy\r\n        name: deploy-beijing\r\n        properties:\r\n          policies: [\"beijing-clusters\"]\r\n      - type: deploy\r\n        name: deploy-aws\r\n        properties:\r\n          policies: [\"override-high-availability\",\"topology-aws\"]\r\n```\r\nThen we attach the above policy and workflow to `intermediate.yaml` and make it to `advanced.yaml`:\r\n```\r\n...\r\n  policies:\r\n    - type: topology\r\n      name: beijing-clusters\r\n      properties:\r\n        clusters: [\"beijing-1\",\"beijing-2\"]\r\n    - type: topology\r\n      name: topology-aws\r\n      properties:\r\n        clusterLabelSelector:\r\n          provider: AWS\r\n    -  type: override\r\n       name: override-high-availability\r\n       properties:\r\n          components:\r\n            - type: webservice\r\n              traits:\r\n              - type: scaler\r\n                properties:\r\n                  replicas: 5\r\n  workflow:\r\n    steps:\r\n      - type: deploy\r\n        name: deploy-beijing\r\n        properties:\r\n          policies: [\"beijing-clusters\"]\r\n      - type: deploy\r\n        name: deploy-aws\r\n        properties:\r\n          policies: [\"override-high-availability\",\"topology-aws\"]\r\n```\r\nThen deploy it, view the status of the application:\r\n```\r\n➜   vela up -f advanced.yaml\r\nApplying an application in vela K8s object format...\r\n\"patching object\" name=\"example-app\" resource=\"core.oam.dev/v1beta1, Kind=Application\"\r\n✅ App has been deployed 🚀🚀🚀\r\n    Port forward: vela port-forward example-app\r\n             SSH: vela exec example-app\r\n         Logging: vela logs example-app\r\n      App status: vela status example-app\r\n  Service status: vela status example-app --svc hello-world-serverapplication.core.oam.dev/podinfo-app configured\r\n  \r\n➜   vela status example-app\r\n\r\n...\r\n\r\n  - Name: hello-world-server\r\n    Cluster: us-west-1  Namespace: default\r\n    Type: webservice\r\n    Healthy Ready:5/5\r\n    Traits:\r\n      ✅ scaler      ✅ gateway: Visiting URL: testsvc-mc.example.com, IP: 192.168.40.10\r\n\r\n```\r\n\r\nThe above all are what we'd like to share with you for this time, thank you for reading and trying them out.\r\n\r\n[We invite you to explore KubeVela v1.3 for more](https://kubevela.io/docs/install) to meet further complex requirements on business, such as [dig deep](https://kubevela.io/docs/next/case-studies/multi-cluster#override-default-configurations-in-clusters) in differentiated configurations to use `override` application policy to either override all resources on one type or only certain specific components."
    },
    {
      "id": "/2022/04/01/offline-deployment-practice",
      "metadata": {
        "permalink": "/blog/2022/04/01/offline-deployment-practice",
        "editUrl": "https://github.com/kubevela/kubevela.io/tree/main/blog/2022-04-01-offline-deployment-practice.md",
        "source": "@site/blog/2022-04-01-offline-deployment-practice.md",
        "title": "China Merchants Bank's Practice on Offline Installation with KubeVela",
        "description": "",
        "date": "2022-04-01T00:00:00.000Z",
        "formattedDate": "April 1, 2022",
        "tags": [
          {
            "label": "KubeVela",
            "permalink": "/blog/tags/kube-vela"
          },
          {
            "label": "use-case",
            "permalink": "/blog/tags/use-case"
          }
        ],
        "readingTime": 5.345,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Xiangbo Ma",
            "title": "(Cloud platform development team)",
            "url": "http://www.cmbchina.com/",
            "imageURL": "/img/china-merchants-bank.jpg"
          }
        ],
        "frontMatter": {
          "title": "China Merchants Bank's Practice on Offline Installation with KubeVela",
          "author": "Xiangbo Ma",
          "author_title": "(Cloud platform development team)",
          "author_url": "http://www.cmbchina.com/",
          "author_image_url": "/img/china-merchants-bank.jpg",
          "tags": [
            "KubeVela",
            "use-case"
          ],
          "description": "",
          "image": "https://raw.githubusercontent.com/oam-dev/KubeVela.io/main/docs/resources/KubeVela-03.png",
          "hide_table_of_contents": false
        },
        "prevItem": {
          "title": "Easily Manage your Application Shipment With Differentiated Configuration in Multi-Cluster",
          "permalink": "/blog/2022/04/06/multi-cluster-management"
        },
        "nextItem": {
          "title": "Use Nocalhost and KubeVela for cloud debugging and multi-cluster hybrid cloud deployment",
          "permalink": "/blog/2022/03/27/kubevela-with-nocalhost"
        }
      },
      "content": "The cloud platform development team of China Merchants Bank has been trying out KubeVela since 2021 internally and aims to using it for enhancing our primary application delivery and management capabilities. Due to the specific security concern for financial insurance industry, network control measurements are relatively strict, and our intranet cannot directly pull Docker Hub image, and there is no Helm image source available as well. Therefore, in order to landing KubeVela in the intranet, you must perform a complete offline installation.\r\n\r\nThis article will take the KubeVela V1.2.5 version as an example, introduce the offline installation practice to help other users easier to complete KubeVela's deployment in offline environment.\r\n\r\n<!--truncate-->\r\n\r\n## KubeVela Offline Installation Solution\r\n\r\nWe divide the offline installation of KubeVela in three parts, which are Vela CLI, Vela Core, and Addon offline installation. Each part mainly involves the loading of the relevant Docker image and Helm's package, which can greatly speed up deployment process in offline environment.\r\n\r\nBefore doing so, please ensure that Kubernetes cluster version is  `>= v1.19 && < v1.22`. One way of KubeVela as a control plane relies on Kubernetes, which can be placed in any product or in any cloud provider. At the same time, you can also use Kind or Minikube to deploy KubeVela locally.\r\n\r\n### Vela CLI Offline Installation\r\n\r\n- First, you need to download of the binary version of `vela` that you want by checking KubeVela [Release Log](https://github.com/kubevela/kubevela/releases)\r\n- Unzip binary files and configure the appropriate environment variables in `$PATH`\r\n   - Unzip binary file\r\n      - `tar -zxvf vela-v1.2.5-linux-amd64.tar.gz`\r\n      - `mv ./linux-amd64/vela /usr/local/bin/vela`\r\n   - Set environment variables\r\n      - `vi /etc/profile`\r\n      - `export PATH=\"$PATH:/usr/local/bin\"`\r\n      - `source /etc/profile`\r\n   - Verify the installation of Vela CLI through `vela version`\r\n```shell\r\nCLI VERSION: V1.2.5\r\nCore Version:\r\nGitRevision: git-ef80b66\r\nGOLANGVERSION: Go1.17.7\r\n```\r\n \r\n- At this point, Vela CLI has been deployed offline!\r\n\r\n### Vela Core Offline Installation\r\n\r\n- Before deploying Vela Core offline, first you need to install [Helm](https://helm.sh/docs/intro/install/) in an offline environment and its version needs to meet `v3.2.0+`\r\n- Prepare Docker image. Vela Core's deployment mainly involves 5 images, you need to first visit the Docker Hub in extranet to download the corresponding images, then load them to offline environment\r\n   - Pull the image from Docker Hub\r\n      - `docker pull oamdev/vela-core:v1.2.5`\r\n      - `docker pull oamdev/cluster-gateway:v1.1.7`\r\n      - `docker pull oamdev/kube-webhook-certgen:v2.3`\r\n      - `docker pull oamdev/alpine-k8s:1.18.2`\r\n      - `docker pull oamdev/hello-world:v1`\r\n   - Save image to local disks\r\n      - `docker save -o vela-core.tar oamdev/vela-core:v1.2.5`\r\n      - `docker save -o cluster-gateway.tar oamdev/cluster-gateway:v1.1.7`\r\n      - `docker save -o kube-webhook-certgen.tar oamdev/kube-webhook-certgen:v2.3`\r\n      - `docker save -o alpine-k8s.tar oamdev/alpine-k8s:1.18.2`\r\n      - `docker save -o hello-world.tar oamdev/hello-world:v1`\r\n   - Re-load the image in the offline environment\r\n      - `docker load vela-core.tar`\r\n      - `docker load cluster-gateway.tar`\r\n      - `docker load kube-webhook-certgen.tar`\r\n      - `docker load alpine-k8s.tar`\r\n      - `docker load hello-world.tar`\r\n- Download [KubeVela Core](https://github.com/kubevela/KubeVela/releases), copy it to offline environment and use Helm to repackage\r\n   - Repackage the KubeVela source code and install the chart package to the control cluster offline\r\n      - `helm package kubevela/charts/vela-core --destination kubevela/charts`\r\n      - `helm install --create-namespace -n vela-system kubevela kubevela/charts/vela-core-0.1.0.tgz --wait`\r\n   - Check the output\r\n```shell\r\nKubeVela Control Plane Has Been successfully set up on your cluster.\r\n```\r\n\r\n- At this point, Vela Core has been deployed offline!\r\n\r\n### Addon Offline Installation\r\n\r\n- First download [Catalog Source](https://github.com/kubevela/catalog) and copy it to offline environment\r\n- Here, we will take VelaUX, one of many more addons, as an example. First prepare its Docker image, VelaUX mainly involve 2 images, you need to first access the extranet to download the corresponding image from Docker Hub, then load it to offline environment\r\n   - Pull the image from Docker Hub\r\n      - `docker pull oamdev/vela-apiserver:v1.2.5`\r\n      - `docker pull oamdev/velaux:v1.2.5`\r\n   - Save image to local disks\r\n      - `docker save -o vela-apiserver.tar oamdev/vela-apiserver:v1.2.5`\r\n      - `docker save -o velaux.tar oamdev/velaux:v1.2.5`\r\n   - Re-load the image in the offline environment\r\n      - `docker load vela-apiserver.tar`\r\n      - `docker load velaux.tar`\r\n- Install VelaUX\r\n   - Install VelaUX via Vela CLI\r\n      - `vela addon enable catalog-master/addons/velaux`\r\n   - Check the output\r\n```shell\r\n  Addon: velaux enabled Successfully.\r\n```\r\n \r\n   - If there is a cluster installed route Controller or Nginx Ingress Controller and also linked with an available domain, you can deploy external routing to make VelaUX accessible. Here present Openshift Route as an example, you can also choose Ingress if you wish\r\n```yaml\r\napiVersion: route.openshift.io/v1\r\nkind: Route\r\nmetadata:\r\nname: velaux-route\r\nnamespace: vela-system\r\nspec:\r\nhost: velaux.xxx.xxx.cn\r\nport:\r\n  targetPort: 80\r\nto:\r\n  kind: Service\r\n  name: velaux\r\n  weight: 100\r\nwildcardPolicy: None\r\n```\r\n\r\n   - Check the installation\r\n```shell\r\ncurl -I -m 10 -o /dev/null -s -w %{http_code} http://velaux.xxx.xxx.cn/applications\r\n```\r\n\r\n- At this point, VelaUX has been deployed offline! At the same time, for other types of Addon's offline deployment, access to the corresponding directory of the [Catalog Source](https://github.com/kubevela/catalog) and repeat the above moves, you would complete all the addons' offline deployments for good.\r\n\r\n## Summarize\r\n\r\nDuring offline deployment, we also try to save Vela Core and Addon's resource that generated to be YAML files after deploying in extranet and re-deploy them in an offline environment, but because of all different kinds of resource involved in and it requires many other authorization issues to resolve, this way is more than cumbersome.\r\n\r\nWith this practice of KubeVela's offline deployment, we hope it help you build a complete set of KubeVela in offline environment much faster. Offline installation is pretty much a pain point for most developers, we also see that the KubeVela community is introducing the brand new [velad](https://github.com/kubevela/velad), a fully offline, highly accountable installation tool. Velad can help automate completion by making many steps as one, such as preparing clusters, downloading and packing image, installing and etc. Further more, it do support many features: In Linux machine (such as Alibaba Cloud ECS) we can locally spin up a cluster to install Vela-Core; while starting a KubeVela control plane, do not have to worry about its data to be lost when machine behind it accidentally was shutdown; Velad can stores all the data from control plane cluster into a traditional database (such as MySQL deployed on another ECS).\r\n\r\nIn the recent version to come, China Merchants Bank will increase the efforts in the open source community of KubeVela, actively building: enterprise-level capacity, enhancement on multi-cluster, offline deployment and application-level observability. We'll also be contributing the financial industry's user scenarios and business needs, driving cloud-native ecology achieve more easily and efficient application management experience, and at last but not at least, welcome you the community member to join us together in this journey."
    },
    {
      "id": "/2022/03/27/kubevela-with-nocalhost",
      "metadata": {
        "permalink": "/blog/2022/03/27/kubevela-with-nocalhost",
        "editUrl": "https://github.com/kubevela/kubevela.io/tree/main/blog/2022-03-27-kubevela-with-nocalhost.md",
        "source": "@site/blog/2022-03-27-kubevela-with-nocalhost.md",
        "title": "Use Nocalhost and KubeVela for cloud debugging and multi-cluster hybrid cloud deployment",
        "description": "",
        "date": "2022-03-27T00:00:00.000Z",
        "formattedDate": "March 27, 2022",
        "tags": [
          {
            "label": "kubevela",
            "permalink": "/blog/tags/kubevela"
          },
          {
            "label": "use-case",
            "permalink": "/blog/tags/use-case"
          }
        ],
        "readingTime": 9.875,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Tianxin Dong and Yicai Yu",
            "title": "KubeVela and Nocalhost team",
            "url": "https://github.com/kubevela/kubevela",
            "imageURL": "https://kubevela.io/img/logo.svg"
          }
        ],
        "frontMatter": {
          "title": "Use Nocalhost and KubeVela for cloud debugging and multi-cluster hybrid cloud deployment",
          "author": "Tianxin Dong and Yicai Yu",
          "author_title": "KubeVela and Nocalhost team",
          "author_url": "https://github.com/kubevela/kubevela",
          "author_image_url": "https://kubevela.io/img/logo.svg",
          "tags": [
            "kubevela",
            "use-case"
          ],
          "description": "",
          "image": "https://raw.githubusercontent.com/kubevela/kubevela.io/main/docs/resources/KubeVela-03.png",
          "hide_table_of_contents": false
        },
        "prevItem": {
          "title": "China Merchants Bank's Practice on Offline Installation with KubeVela",
          "permalink": "/blog/2022/04/01/offline-deployment-practice"
        },
        "nextItem": {
          "title": "Machine Learning Practice with KubeVela",
          "permalink": "/blog/2022/03/02/kubevela-with-machine-learning"
        }
      },
      "content": "With the rapid development of cloud-native, how can we use cloud to empower business development? When launching applications, how can cloud developers easily develop and debug applications in a multi-cluster and hybrid cloud environment? In the deployment process, how to make the application deployment have sufficient verification and reliability?\r\n\r\nThese crucial issues are urgently needed to be resolved.\r\n\r\nIn this article, we will use KubeVela and Nocalhost to provide a solution for cloud debugging and multi-cluster hybrid cloud deployment.\r\n\r\nWhen a new application needs to be developed and launched, we hope that the results of debugging in the local IDE can be consistent with the final deployment state in the cloud. Such a consistent posture gives us the greatest confidence in deployment and allows us to iteratively apply updates in a more efficient and agile way like GitOps. That is: when new code is pushed to the code repository, the applications in the environment are automatically updated in real time.\r\n\r\nBased on KubeVela and Nocalhost, we can deploy application like below:\r\n\r\n![alt](/img/nocalhost/0.png)\r\n\r\nAs shown in the figure: Use KubeVela to create an application, deploy the application to the test environment, and pause it. Use Nocalhost to debug the application in the cloud. After debugging, push the debugged code to the code repository, use GitOps to deploy with KubeVela, and then update it to the production environment after verification in the test environment.\r\n\r\nIn the following, we will introduce how to use KubeVela and Nocalhost for cloud debugging and multi-cluster hybrid cloud deployment.\r\n\r\n<!--truncate-->\r\n\r\n## What is KubeVela\r\n\r\nKubeVela is an easy-to-use and highly scalable application delivery platform built on Kubernetes and OAM. Its core capability is to allow developers to easily and quickly define and deliver modern microservice applications on Kubernetes without knowing any details related to Kubernetes itself.\r\n\r\nKubeVela also provides VelaUX, which can visualize the entire application distribution process, making the process of application easier.\r\n\r\nKubeVela provides the following capabilities in this scenario:\r\n\r\n1. The full GitOps capability:\r\n  * KubeVela supports both Pull mode and Push mode for GitOps: we only need to push the updated code to the code repository, and KubeVela can automatically re-deploy applications based on the latest code. In this article, we will use GitOps in Push mode. For GitOps support in Pull mode, you can check [this article](https://kubevela.io/blog/2021/10/10/kubevela-gitops).\r\n2. Powerful workflow capabilities, including cross-environment (cluster) deployment, approval, and notification:\r\n  * With its workflow capabilities, KubeVela can easily deploy applications across environments, and supports users to add workflow steps such as manual approval and message notification.\r\n3. Application abstraction capabilities, make developers can understand, use and customize infrastructure capabilities easily:\r\n  * KubeVela follows OAM and provides a set of simple and easy-to-use application abstraction capabilities, enabling developers easy to understand application and customize infrastructure capabilities. For example, for a simple application, we can divide it into three parts: components, traits and workflow. In the example in this article, the component is a simple FE application; in the traits, we bind the Nocalhost trait to this component, so that this component can use Nocalhost to debug in the cloud; In the workflow, we can first deploy this component in the test environment, and automatically suspend the workflow, then deploy to the production environment until the manual verification and approval are passed.\r\n\r\n## What is Nocalhost\r\n\r\nNocalhost is a tool that allows developers to develop applications directly within a Kubernetes cluster.\r\n\r\nThe core capability of Nocalhost is to provide Nocalhost IDE plugins (including VSCode and Jetbrains plugins) to change remote workloads to development mode. In development mode, the container's image will be replaced with a development image containing development tools (e.g. JDK, Go, Python environment, etc.). When developers write code locally, any changes will be synchronized to the remote development container in real time, and the application will be updated immediately (depending on the application's hot reload mechanism or re-running the application), and the development container will inherit all the original workload's configurations (ConfigMap, Secret, Volume, Env, etc.).\r\n\r\nNocalhost also provides: debug and HotReload for VSCode and Jetbrains IDE; the terminal of the development container in the IDE to obtain a consistent experience with local development; a development space and mesh development space based on Namespace isolation. In addition, Nocalhost also provides a server side to help enterprises manage Kubernetes applications, developers and development spaces, so that enterprises can manage various development and testing environments in a unified way.\r\n\r\nIn the process of using Nocalhost to develop Kubernetes applications, image building, updating image versions, and waiting for the cluster to schedule Pods is eliminated, and the code/test/debug cycle is reduced from minutes to seconds.\r\n\r\n## Debug application in the cloud\r\n\r\nLet's take a simple front-end application as an example. First, we use VelaUX to deploy it in the multi-environment.\r\n\r\n> If you don't know how to enable KubeVela's VelaUX addon, please check the [official documentation](https://kubevela.io/docs/install#2-install-velaux).\r\n\r\n### Use VelaUX to deploy application\r\n\r\nCreate an environment in VelaUX, each environment can have multiple delivery targets, let's take an environment which contains a test and production delivery targets as an example.\r\n\r\nFirst, create two delivery targets, one for test and one for production. The delivery target here will deliver resources to the test and prod namespaces of the local cluster respectively. You can also add new clusters for deployment through VelaUX's cluster management capabilities.\r\n\r\n![alt](/img/nocalhost/1.png)\r\n\r\nAfter creating the delivery targets, create a new environment which contains these two delivery targets.\r\n\r\n![alt](/img/nocalhost/2.png)\r\n\r\nThen, create a new application for cloud debugging. This front-end application will expose services on port 80, so we open port 80 for this application.\r\n\r\n![alt](/img/nocalhost/3.png)\r\n\r\nAfter the application is created, the application comes with a workflow by default, which will automatically deploy the application to two delivery targets. But we don't want the un-debugged app to be deployed directly to production target. So let's edit this default workflow: add a suspend step between the two deploy steps. In this way, after deploying to the test environment, we can suspend the workflow, wait for the user to debug and verify, and then continue to deploy to the production environment.\r\n\r\n![alt](/img/nocalhost/4.png)\r\n\r\nAfter completing these configurations, let's add a Nocalhost Trait for this application for cloud debugging.\r\n\r\nWe'll introduce a few parameters in Nocalhost Trait here in detail:\r\n\r\n![alt](/img/nocalhost/5.png)\r\n\r\nThere's two types of commands, Debug and Run. During development, right-clicking Remote Debug and Remote Run on the plug-in will run the corresponding command in the remote Pod. We are using a front-end application here, so set the command to yarn serve.\r\n\r\n![alt](/img/nocalhost/6.png)\r\n\r\n![alt](/img/nocalhost/7.png)\r\n\r\nImage here refers to the debug image. Nocalhost provides images in five languages by default (go/java/python/ruby/node). You can use the built-in image by filling in the language name, you can also fill in the full image name to use custom image.\r\nTurning on HotReload means turning on the hot reload capability, and you can see the effect directly after modifying the code. PortForward will forward the cloud application's port 80 to the local port 8080.\r\n\r\n![alt](/img/nocalhost/8.png)\r\n\r\nIn the Sync section, you can set type to sendReceive (two-way sync), or set to send (one-way send). After completing the configuration, deploy the app. As you can see, the application will automatically suspend after it is deployed to the test target.\r\n\r\n![alt](/img/nocalhost/9.png)\r\n\r\nAt this point, open the Nocalhost plugin in VSCode or Jetbrains IDE, you can see our deployed application under the test namespace, click the hammer button next to the application to enter the debug mode:\r\n\r\n![alt](/img/nocalhost/10.png)\r\n\r\nAfter entering Nocalhost debug mode, you can see that the terminal in the IDE has been replaced by the terminal of the container. With the ls command, you can see all the files in the container.\r\n\r\n![alt](/img/nocalhost/11.png)\r\n\r\nRight-click the application in Nocalhost, and you can choose to enter Remote Debug or Remote Run mode. These two keys will automatically execute the Debug and Run commands we configured earlier.\r\n\r\n![alt](/img/nocalhost/12.png)\r\n\r\nAfter entering Debug mode, we can see that our cloud application is forwarded to the local port 8080:\r\n\r\n![alt](/img/nocalhost/13.png)\r\n\r\nOpen the local browser and you can see that the version of the front-end application we are currently deploying is v1.0.0:\r\n\r\n![alt](/img/nocalhost/14.png)\r\n\r\nNow, we can modify the code in the local IDE to change the version to v2.0.0:\r\n\r\n![alt](/img/nocalhost/15.png)\r\n\r\nIn the previous Nocalhost configuration, we have enabled hot reloading. Therefore, if we refresh the local 8080 port page again, we can see that the application version has become v2.0.0:\r\n\r\n![alt](/img/nocalhost/16.png)\r\n\r\nNow, we can terminate Nocalhost's debug mode and push the debugged code to the code repository.\r\n\r\n![alt](/img/nocalhost/17.png)\r\n\r\n## Multi-Environment Publishing with GitOps\r\n\r\nAfter we finish debugging, the application on the environment is still the previous v1.0.0 version. So, what is the way to update the applications in the environment?\r\n\r\nDuring the entire cloud debugging process, we only modify the source code. Therefore, we can use the GitOps to use code as the update source to complete the update of the application in the environment.\r\n\r\nLooking at the applications deployed in VelaUX, you can see that each application will have a default Trigger:\r\n\r\n![alt](/img/nocalhost/18.png)\r\n\r\nClick Manual Trigger to view the details, you can see that VelaUX provides a Webhook URL for each application, request this address, and bring the fields that need to be updated (such as: image, etc.), the the application can be easily and quickly updated. (Note: Since we need to expose addresses externally, you need to use LoadBalancer or other methods to expose VelaUX services when deploying VelaUX).\r\n\r\n![alt](/img/nocalhost/19.png)\r\n\r\nIn Curl Command, an example is also provided. Let's parse the request body in detail:\r\n\r\n```json\r\n{\r\n  // Required, the update information triggered this time\r\n  \"upgrade\": {\r\n    // Application name is the key\r\n    \"<application-name>\": {\r\n      // The value that needs to be updated, the content here will be patched to the application\r\n      \"image\": \"<image-name>\"\r\n    }\r\n  },\r\n  // Optional, the code information carried in this trigger\r\n  \"codeInfo\": {\r\n    \"commit\": \"<commit-id>\",\r\n    \"branch\": \"<branch>\",\r\n    \"user\": \"<user>\",\r\n  }\r\n}\r\n```\r\n\r\n`upgrade` is the update information to be carried in this trigger, in `<application-name` is the value that needs to be patched. The default recommendation is to update the image, or you can extend the fields here to update other properties of the application.\r\n\r\n`codeInfo` is code information, which can be optionally carried, such as commit ID, branch, committer, etc. Generally, these values can be specified by using variable substitution in the CI system.\r\n\r\nWhen our updated code is merged into the code repository, we can add a new step in CI to integrated with VelaUX in the code repository. Taking GitLab CI as an example, the following steps can be added:\r\n\r\n```json\r\nwebhook-request:\r\n  stage: request\r\n  before_script:\r\n    - apk add --update curl && rm -rf /var/cache/apk/*\r\n  script:\r\n    - |\r\n      curl -X POST -H \"Content-Type: application/json\" -d '{\"upgrade\":{\"'\"$APP_NAME\"'\":{\"image\":\"'\"$BUILD_IMAGE\"'\"}},\"codeInfo\":{\"user\":\"'\"$CI_COMMIT_AUTHOR\"'\",\"commit\":\"'\"$CI_COMMIT_SHA\"'\",\"branch\":\"'\"$CI_COMMIT_BRANCH\"'\"}}' $WEBHOOK_URL\r\n```\r\n\r\nAfter the configuration is complete, when the code is updated, the CI will be automatically triggered and the corresponding application in VelaUX will be updated.\r\n\r\n![alt](/img/nocalhost/20.png)\r\n\r\nWhen the image is updated, check the application page again, and you can see that the application in the test environment has become the version v2.0.0.\r\n\r\nAfter verification in the test delivery target, we can click `Continue` in the workflow to deploy the latest version of the application to the production delivery target.\r\n\r\n![alt](/img/nocalhost/21.png)\r\n\r\nCheck the application in the production environment, you can see that the latest v2.0.0 version is already in the production environment:\r\n\r\n![alt](/img/nocalhost/22.png)\r\n\r\nAt this point, we first used Nocalhost in the test environment for cloud debugging through KubeVela. After passing the verification, we updated the code, used GitOps to complete the deployment update, and continued to update the application in the production environment, thus completing an application from deployment to launch.\r\n\r\n## Summary\r\n\r\nUsing KubeVela + Nocalhost, it is not only convenient for cloud debugging in the development environment, but also easy to update and deploy to the production environment after the test is completed, making the entire development and process stable and reliable."
    },
    {
      "id": "/2022/03/02/kubevela-with-machine-learning",
      "metadata": {
        "permalink": "/blog/2022/03/02/kubevela-with-machine-learning",
        "editUrl": "https://github.com/kubevela/kubevela.io/tree/main/blog/2022-03-02-kubevela-with-machine-learning.md",
        "source": "@site/blog/2022-03-02-kubevela-with-machine-learning.md",
        "title": "Machine Learning Practice with KubeVela",
        "description": "",
        "date": "2022-03-02T00:00:00.000Z",
        "formattedDate": "March 2, 2022",
        "tags": [
          {
            "label": "KubeVela",
            "permalink": "/blog/tags/kube-vela"
          },
          {
            "label": "use-case",
            "permalink": "/blog/tags/use-case"
          }
        ],
        "readingTime": 9.375,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Tianxin Dong",
            "title": "KubeVela team",
            "url": "https://github.com/kubevela/kubevela",
            "imageURL": "https://kubevela.io/img/logo.svg"
          }
        ],
        "frontMatter": {
          "title": "Machine Learning Practice with KubeVela",
          "author": "Tianxin Dong",
          "author_title": "KubeVela team",
          "author_url": "https://github.com/kubevela/kubevela",
          "author_image_url": "https://kubevela.io/img/logo.svg",
          "tags": [
            "KubeVela",
            "use-case"
          ],
          "description": "",
          "image": "https://raw.githubusercontent.com/kubevela/kubevela.io/main/docs/resources/KubeVela-03.png",
          "hide_table_of_contents": false
        },
        "prevItem": {
          "title": "Use Nocalhost and KubeVela for cloud debugging and multi-cluster hybrid cloud deployment",
          "permalink": "/blog/2022/03/27/kubevela-with-nocalhost"
        },
        "nextItem": {
          "title": "Generate top 50 popular resources of AWS using 100 lines of code",
          "permalink": "/blog/2022/03/01/kubevela-generate-top-50-popular-resources-of-aws-using-100-lines-of-code"
        }
      },
      "content": "At the background of Machine learning goes viral, AI engineers not only need to train and debug their models, but also need to deploy them online to verify how it looks(of course sometimes, this part of the work is done by AI platform engineers. ). It is very tedious and draining AI engineers.\r\n\r\nIn the cloud-native era, our model training and model serving are also usually performed on the cloud. Doing so not only improves scalability, but also improves resource utility. This is very effective for machine learning scenarios that consume a lot of computing resources.\r\n\r\nBut it is often difficult for AI engineers to use cloud-native techniques. The concept of cloud native has become more complex over time. Even to deploy a simple model serving on cloud native architecture, AI engineers may need to learn several additional concepts: Deployment, Service, Ingress, etc.\r\n\r\nAs a simple, easy-to-use, and highly scalable cloud-native application management tool, KubeVela enables developers to quickly and easily define and deliver applications on Kubernetes without knowing any details about the underlying cloud-native infrastructure. KubeVela's rich extensibility extends to AI addons and provide functions such as model training, model serving, and A/B testing, covering the basic needs of AI engineers and helping AI engineers quickly conduct model training and model serving in a cloud-native environment.\r\n\r\nThis article mainly focus on how to use KubeVela's AI addon to help engineers complete model training and model serving more easily.\r\n\r\n<!--truncate-->\r\n\r\n## KubeVela AI Addon\r\n\r\nThe KubeVela AI addon is divided into two: model training and model serving. The model training addon is based on KubeFlow's training-operator and can support distributed model training in different frameworks such as TensorFlow, PyTorch, and MXNet. The model serving addon is based on Seldon Core, which can easily use the model to start the model serving, and also supports advanced functions such as traffic distribution and A/B testing.\r\n\r\n![alt](/img/ai/ai-addon-en.png)\r\n\r\nThrough the KubeVela AI addon, the deployment of model training and serving tasks can be significantly simplified. At the same time, the process of model training and serving can be combined with KubeVela's own workflow, multi-cluster and other functions to complete production-level services.\r\n\r\n> Note: You can find all source code and YAML files in [KubeVela Samples](https://github.com/kubevela/samples/tree/master/11.Machine_Learning_Demo). If you want to use the model pretrained in this example, `style-model.yaml` and `color-model.yaml` in the folder will do that and copy the model into the PVC.\r\n\r\n## Model Training\r\n\r\nFirst enable the two addons for model training and model serving.\r\n\r\n```bash\r\nvela addon enable model-training\r\nvela addon enable model-serving\r\n```\r\n\r\nModel training includes two component types, `model-training` and `jupyter-notebook`, and model serving includes the `model-serving` component type. The specific parameters of these three components can be viewed through the `vela show` command.\r\n\r\n> You can also read [KubeVela AI Addon Documentation](https://kubevela.io/en/docs/next/reference/addons/ai) for more information.\r\n\r\n```bash\r\nvela show model-training\r\nvela show jupyter-notebook\r\nvela show model-serving\r\n```\r\n\r\nLet's train a simple model using the TensorFlow framework that turns gray images into colored ones. Deploy the following YAML file:\r\n\r\n> Note: The source code for model training comes from: [emilwallner/Coloring-greyscale-images](https://github.com/emilwallner/Coloring-greyscale-images)\r\n\r\n```yaml\r\napiVersion: core.oam.dev/v1beta1\r\nkind: Application\r\nmetadata:\r\n  name: training-serving\r\n  namespace: default\r\nspec:\r\n  components:\r\n  # Train the model\r\n  - name: demo-training\r\n    type: model-training\r\n    properties:\r\n      # Mirror of the trained model\r\n      image: fogdong/train-color:v1\r\n      # A framework for model training\r\n      framework: tensorflow\r\n      # Declare storage to persist models. Here, the default storage class in the cluster will be used to create the PVC\r\n      storage:\r\n        - name: \"my-pvc\"\r\n          mountPath: \"/model\"\r\n```\r\n\r\nAt this point, KubeVela will pull up a `TFJob` for model training.\r\n\r\nIt's hard to see what's going on just by training the model. Let's modify this YAML file and put the model serving after the model training step. At the same time, because the model serving will directly start the model, and the input and output of the model are not intuitive (ndarray or Tensor), therefore, we deploy a test service to call the service and convert the result into an image.\r\n\r\nDeploy the following YAML file:\r\n```yaml\r\napiVersion: core.oam.dev/v1beta1\r\nkind: Application\r\nmetadata:\r\n  name: training-serving\r\n  namespace: default\r\nspec:\r\n  components:\r\n  # Train the model\r\n  - name: demo-training\r\n    type: model-training\r\n    properties:\r\n      image: fogdong/train-color:v1\r\n      framework: tensorflow\r\n      storage:\r\n        - name: \"my-pvc\"\r\n          mountPath: \"/model\"\r\n  \r\n  # Start the model serving\r\n  - name: demo-serving\r\n    type: model-serving\r\n    # The model serving will start after model training is complete\r\n    dependsOn:\r\n      - demo-training\r\n    properties:\r\n      # The protocol used to start the model serving can be left blank. By default, seldon's own protocol is used.\r\n      protocol: tensorflow\r\n      predictors:\r\n        - name: model\r\n          # The number of replicas for the model serving\r\n          replicas: 1\r\n          graph:\r\n            # model name\r\n            name: my-model\r\n            # model frame\r\n            implementation: tensorflow\r\n            # Model address, the previous step will save the trained model to the pvc of my-pvc, so specify the address of the model through pvc://my-pvc\r\n            modelUri: pvc://my-pvc\r\n\r\n  # test model serving\r\n  - name: demo-rest-serving\r\n    type: webservice\r\n    # The test service will start after model training is complete\r\n    dependsOn:\r\n      - demo-serving\r\n    properties:\r\n      image: fogdong/color-serving:v1\r\n      # Use LoadBalancer to expose external addresses for easy to access\r\n      exposeType: LoadBalancer\r\n      env:\r\n        - name: URL\r\n          # The address of the model serving\r\n          value: http://ambassador.vela-system.svc.cluster.local/seldon/default/demo-serving/v1/models/my-model:predict\r\n      ports:\r\n        # Test service port\r\n        - port: 3333\r\n          expose: true\r\n```\r\n\r\nAfter deployment, check the status of the application with `vela ls`:\r\n\r\n```bash\r\n$ vela ls\r\n\r\ntraining-serving      \tdemo-training      \tmodel-training\t       \trunning\thealthy\tJob Succeeded\t2022-03-02 17:26:40 +0800 CST\r\n├─                  \tdemo-serving       \tmodel-serving \t       \trunning\thealthy\tAvailable    \t2022-03-02 17:26:40 +0800 CST\r\n└─                  \tdemo-rest-serving  \twebservice    \t       \trunning\thealthy\tReady:1/1    \t2022-03-02 17:26:40 +0800 CST\r\n```\r\n\r\nAs you can see, the application has started normally. Use `vela status <app-name> --endpoint` to view the service address of the application.\r\n\r\n```bash\r\n$ vela status training-serving --endpoint\r\n\r\n+---------+-----------------------------------+---------------------------------------------------+\r\n| CLUSTER |     REF(KIND/NAMESPACE/NAME)      |                     ENDPOINT                      |\r\n+---------+-----------------------------------+---------------------------------------------------+\r\n|         | Service/default/demo-rest-serving | tcp://47.251.10.177:3333                          |\r\n|         | Service/vela-system/ambassador    | http://47.251.36.228/seldon/default/demo-serving  |\r\n|         | Service/vela-system/ambassador    | https://47.251.36.228/seldon/default/demo-serving |\r\n+---------+-----------------------------------+---------------------------------------------------+\r\n```\r\n\r\nThe application has three service addresses, the first is the address of our test service, the second and third are the addresses of the native model.\r\n\r\nWe can call the test service to see the effect of the model: the test service will read the content of the image, convert it into a `Tensor` and request the model serving, and finally convert the `Tensor` returned by the model serving into an image to return.\r\n\r\nWe choose a black and white female image as input:\r\n\r\n![alt](/img/ai/woman-grey.png)\r\n\r\nAfter the request, you can see that a color image is output:\r\n\r\n![alt](/img/ai/test-request.png)\r\n\r\n## Model Servings: Canary Testing\r\n\r\nIn addition to starting the model serving directly, we can also use multiple versions of the model in one model serving and assign different traffic to them for canary testing.\r\n\r\nDeploy the following YAML, you can see that both the v1 version of the model and the v2 version of the model are set to 50% traffic. Again, we deploy a test service behind the model serving:\r\n\r\n```yaml\r\napiVersion: core.oam.dev/v1beta1\r\nkind: Application\r\nmetadata:\r\n  name: color-serving\r\n  namespace: default\r\nspec:\r\n  components:\r\n  - name: color-model-serving\r\n    type: model-serving\r\n    properties:\r\n      protocol: tensorflow\r\n      predictors:\r\n        - name: model1\r\n          replicas: 1\r\n          # v1 version model traffic is 50\r\n          traffic: 50\r\n          graph:\r\n            name: my-model\r\n            implementation: tensorflow\r\n            # Model address, our v1 version model is stored under the /model/v1 path in the pvc of color-model, so specify the address of the model through pvc://color-model/model/v1\r\n            modelUri: pvc://color-model/model/v1\r\n        - name: model2\r\n          replicas: 1\r\n          # v2 version model traffic is 50\r\n          traffic: 50\r\n          graph:\r\n            name: my-model\r\n            implementation: tensorflow\r\n            # Model address, our v2 version model is stored under the /model/v2 path in the pvc of color-model, so specify the address of the model through pvc://color-model/model/v2\r\n            modelUri: pvc://color-model/model/v2\r\n  - name: color-rest-serving\r\n    type: webservice\r\n    dependsOn:\r\n      - color-model-serving\r\n    properties:\r\n      image: fogdong/color-serving:v1\r\n      exposeType: LoadBalancer\r\n      env:\r\n        - name: URL\r\n          value: http://ambassador.vela-system.svc.cluster.local/seldon/default/color-model-serving/v1/models/my-model:predict\r\n      ports:\r\n        - port: 3333\r\n          expose: true\r\n```\r\n\r\nWhen the model deployment is complete, use `vela status <app-name> --endpoint` to view the address of the model serving:\r\n\r\n```bash\r\n$ vela status color-serving --endpoint\r\n\r\n+---------+------------------------------------+----------------------------------------------------------+\r\n| CLUSTER |      REF(KIND/NAMESPACE/NAME)      |                         ENDPOINT                         |\r\n+---------+------------------------------------+----------------------------------------------------------+\r\n|         | Service/vela-system/ambassador     | http://47.251.36.228/seldon/default/color-model-serving  |\r\n|         | Service/vela-system/ambassador     | https://47.251.36.228/seldon/default/color-model-serving |\r\n|         | Service/default/color-rest-serving | tcp://47.89.194.94:3333                                  |\r\n+---------+------------------------------------+----------------------------------------------------------+\r\n```\r\n\r\nRequest the model with a black and white city image:\r\n\r\n![alt](/img/ai/chicago-grey.png)\r\n\r\nAs you can see, the result of the first request is as follows. While the sky and ground are rendered in color, the city itself is black and white:\r\n\r\n![alt](/img/ai/canary-request1.png)\r\n\r\nRequest again, you can see that in the result of this request, the sky, ground and city are rendered in color:\r\n\r\n![alt](/img/ai/canary-request2.png)\r\n\r\nBy distributing traffic to different versions of the model, it can help us better judge the model results.\r\n\r\n## Model Serving: A/B Testing\r\n\r\nFor a black and white image, we can turn it into color through the model. Or in another way, we can transfer the style of the original image by uploading another style image.\r\n\r\nDo our users love colorful pictures more or pictures of different styles more? We can explore this question by conducting A/B testing.\r\n\r\nDeploy the following YAML, by setting `customRouting`, forward the request with `style: transfer` in the `Header` to the model of style transfer. At the same time, make this style transfer model share the same address as the colorized model.\r\n\r\n> Note: The model for style transfer comes from [TensorFlow Hub](https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2)\r\n\r\n```yaml\r\napiVersion: core.oam.dev/v1beta1\r\nkind: Application\r\nmetadata:\r\n  name: color-style-ab-serving\r\n  namespace: default\r\nspec:\r\n  components:\r\n  - name: color-ab-serving\r\n    type: model-serving\r\n    properties:\r\n      protocol: tensorflow\r\n      predictors:\r\n        - name: model1\r\n          replicas: 1\r\n          graph:\r\n            name: my-model\r\n            implementation: tensorflow\r\n            modelUri: pvc://color-model/model/v2\r\n  - name: style-ab-serving\r\n    type: model-serving\r\n    properties:\r\n      protocol: tensorflow\r\n      # The model of style migration takes a long time, set the timeout time so that the request will not be timed out\r\n      timeout: \"10000\"\r\n      customRouting:\r\n        # Specify custom Header\r\n        header: \"style: transfer\"\r\n        # Specify custom routes\r\n        serviceName: \"color-ab-serving\"\r\n      predictors:\r\n        - name: model2\r\n          replicas: 1\r\n          graph:\r\n            name: my-model\r\n            implementation: tensorflow\r\n            modelUri: pvc://style-model/model\r\n  - name: ab-rest-serving\r\n    type: webservice\r\n    dependsOn:\r\n      - color-ab-serving\r\n      - style-ab-serving\r\n    properties:\r\n      image: fogdong/style-serving:v1\r\n      exposeType: LoadBalancer\r\n      env:\r\n        - name: URL\r\n          value: http://ambassador.vela-system.svc.cluster.local/seldon/default/color-ab-serving/v1/models/my-model:predict\r\n      ports:\r\n        - port: 3333\r\n          expose: true\r\n```\r\n\r\nAfter successful deployment, view the address of the model serving through `vela status <app-name> --endpoint`:\r\n\r\n```bash\r\n$ vela status color-style-ab-serving --endpoint\r\n\r\n+---------+---------------------------------+-------------------------------------------------------+\r\n| CLUSTER |    REF(KIND/NAMESPACE/NAME)     |                       ENDPOINT                        |\r\n+---------+---------------------------------+-------------------------------------------------------+\r\n|         | Service/vela-system/ambassador  | http://47.251.36.228/seldon/default/color-ab-serving  |\r\n|         | Service/vela-system/ambassador  | https://47.251.36.228/seldon/default/color-ab-serving |\r\n|         | Service/vela-system/ambassador  | http://47.251.36.228/seldon/default/style-ab-serving  |\r\n|         | Service/vela-system/ambassador  | https://47.251.36.228/seldon/default/style-ab-serving |\r\n|         | Service/default/ab-rest-serving | tcp://47.251.5.97:3333                                |\r\n+---------+---------------------------------+-------------------------------------------------------+\r\n```\r\n\r\nIn this application, the two services have two addresses each, but the model service address of the second `style-ab-serving` is invalid because the model service is already pointed to the address of `color-ab-serving` . Again, we see how it works by requesting the test service.\r\n\r\nFirst, without the header, the image changes from black and white to color:\r\n\r\n![alt](/img/ai/ab-request1.png)\r\n\r\nLet's add an image of an ocean wave as a style render:\r\n\r\n![alt](/img/ai/wave.jpg)\r\n\r\nWe add the Header of `style: transfer` to this request, and you can see that the city has become a wave style:\r\n\r\n![alt](/img/ai/ab-request2.png)\r\n\r\nWe can also use an ink painting image as a style rendering:\r\n\r\n![alt](/img/ai/chinese-style.jpg)\r\n\r\nIt can be seen that this time the city has become an ink painting style:\r\n\r\n![alt](/img/ai/ab-request3.png)\r\n\r\n## Summary\r\n\r\nThrough KubeVela's AI plug-in, it can help you to perform model training and model serving more conveniently.\r\n\r\nIn addition, together with KubeVela, we can also deliver the tested model to different environments through KubeVela's multi-environment function, so as to realize the flexible deployment of the model."
    },
    {
      "id": "/2022/03/01/kubevela-generate-top-50-popular-resources-of-aws-using-100-lines-of-code",
      "metadata": {
        "permalink": "/blog/2022/03/01/kubevela-generate-top-50-popular-resources-of-aws-using-100-lines-of-code",
        "editUrl": "https://github.com/kubevela/kubevela.io/tree/main/blog/2022-03-01-kubevela-generate-top-50-popular-resources-of-aws-using-100-lines-of-code.md",
        "source": "@site/blog/2022-03-01-kubevela-generate-top-50-popular-resources-of-aws-using-100-lines-of-code.md",
        "title": "Generate top 50 popular resources of AWS using 100 lines of code",
        "description": "",
        "date": "2022-03-01T00:00:00.000Z",
        "formattedDate": "March 1, 2022",
        "tags": [
          {
            "label": "Terraform",
            "permalink": "/blog/tags/terraform"
          }
        ],
        "readingTime": 5.515,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Avery Qi (Tongji University) Zhengxi Zhou (Alibaba Cloud)",
            "title": "KubeVela Team",
            "url": "https://github.com/kubevela/kubevela",
            "imageURL": "https://kubevela.io/img/logo.svg"
          }
        ],
        "frontMatter": {
          "title": "Generate top 50 popular resources of AWS using 100 lines of code",
          "author": "Avery Qi (Tongji University) Zhengxi Zhou (Alibaba Cloud)",
          "author_title": "KubeVela Team",
          "author_url": "https://github.com/kubevela/kubevela",
          "author_image_url": "https://kubevela.io/img/logo.svg",
          "tags": [
            "Terraform"
          ],
          "description": "",
          "hide_table_of_contents": false
        },
        "prevItem": {
          "title": "Machine Learning Practice with KubeVela",
          "permalink": "/blog/2022/03/02/kubevela-with-machine-learning"
        },
        "nextItem": {
          "title": "KubeVela v1.2 - Focused on Developer Experience, Simplified Multi-Cluster Application Delivery",
          "permalink": "/blog/2022/01/27/blog-1.2"
        }
      },
      "content": "KubeVela currently supports AWS, Azure, GCP,  AliCloud, Tencent Cloud, Baidu Cloud, UCloud and other cloud vendors, and also provides [a quick and easy command line tool](https://kubevela.io/docs/next/platform-engineers/components/component-terraform) to introduce cloud resources from cloud providers. But supporting cloud resources from cloud providers one by one in KubeVela is not conducive to quickly satisfying users' needs for cloud resources. This doc provides a solution to quickly introduce the top 50 most popular cloud resources from AWS in less than 100 lines of code.\r\n\r\nWe also expect users to be inspired by this article to contribute cloud resources for other cloud providers.\r\n\r\n<!--truncate-->\r\n\r\n## Where are the most popular cloud resources on AWS?\r\n\r\nThe official Terraform website provides Terraform modules for each cloud provider, for example, [AWS cloud resource Terraform modules](https://registry.terraform.io/namespaces/terraform-aws-modules). And the cloud resources are sorted by popularity of usage (downloads), for example, AWS VPC has 18.7 million downloads.\r\n\r\nThrough a simple analysis, we found that the data for the top 50 popular Terraform modules for AWS can be obtained by requesting [https://registry.terraform.io/v2/modules?filter%5Bprovider%5D=aws&include=latest-version&page%5Bsize%5D=50&page%5Bnumber%5D=1](https://registry.terraform.io/v2/modules?filter%5Bprovider%5D=aws&include=latest-version&page%5Bsize%5D=50&page%5Bnumber%5D=1).\r\n\r\n\r\n\r\n## Prerequisites\r\n\r\nThe code accepts two parameters.\r\n\r\n* provider Name\r\n* The URL of the Terraform Modules corresponding to the provider\r\n\r\nFor AWS, Provider Name should be “aws”，corresponding Terraform modules URL is[Terraform Modules json API](https://registry.terraform.io/v2/modules?filter%5Bprovider%5D=aws&include=latest-version&page%5Bsize%5D=50&page%5Bnumber%5D=1)(Searching top 50 popular resources for provider aws in [Terraform Registry](https://registry.terraform.io/)).\r\n\r\nYou need to make sure the providerName(aws) and Modules links are correct before executing the code.\r\n\r\n\r\n## Executing the code\r\n\r\nThen you can quickly bring in the top 50 most popular AWS cloud resources in bulk with the following 100 lines of code (filename gen.go).\r\n\r\n```\r\nimport (\r\n  \"encoding/json\"\r\n  \"fmt\"\r\n  \"io\"\r\n  \"log\"\r\n  \"net/http\"\r\n  \"os\"\r\n  \"os/exec\"\r\n  \"path/filepath\"\r\n  \"strings\"\r\n\r\n  \"github.com/pkg/errors\"\r\n)\r\n\r\ntype TFDownload struct {\r\n  Data     []DataItem     `json:\"data\"`\r\n  Included []IncludedItem `json:\"included\"`\r\n}\r\n\r\ntype IncludedItem struct {\r\n  Id         string     `json:\"id\"`\r\n  Attributes Attributes `json:\"attributes\"`\r\n}\r\n\r\ntype DataItem struct {\r\n  Attributes    Attributes    `json:\"attributes\"`\r\n  Relationships Relationships `json:\"relationships\"`\r\n}\r\n\r\ntype Relationships struct {\r\n  LatestVersion RelationshipLatestVersion `json:\"latest-version\"`\r\n}\r\n\r\ntype RelationshipLatestVersion struct {\r\n  Data RelationshipData `json:\"data\"`\r\n}\r\n\r\ntype RelationshipData struct {\r\n  Id string `json:\"id\"`\r\n}\r\n\r\nvar errNoVariables = errors.New(\"failed to find main.tf or variables.tf in Terraform configurations\")\r\n\r\ntype Attributes struct {\r\n  Name        string `json:\"name\"`\r\n  Downloads   int    `json:\"downloads\"`\r\n  Source      string `json:\"source\"`\r\n  Description string `json:\"description\"`\r\n  Verified    bool   `json:\"verified\"`\r\n}\r\n\r\nfunc main() {\r\n  if len(os.Args) < 2 {\r\n     fmt.Println(\"Please provide the cloud provider name and an official Terraform modules URL\")\r\n     os.Exit(1)\r\n  }\r\n  providerName := os.Args[1]\r\n  terraformModulesUrl := os.Args[2]\r\n  resp, err := http.Get(terraformModulesUrl)\r\n  if err != nil {\r\n     log.Fatal(err)\r\n  }\r\n  defer resp.Body.Close()\r\n  body, err := io.ReadAll(resp.Body)\r\n  if err != nil {\r\n     log.Fatal(err)\r\n  }\r\n\r\n  var modules TFDownload\r\n  if err := json.Unmarshal(body, &modules); err != nil {\r\n     fmt.Println(err.Error())\r\n     os.Exit(1)\r\n  }\r\n\r\n  if _, err = os.Stat(providerName); err == nil {\r\n     if err := os.RemoveAll(providerName); err != nil {\r\n        log.Fatal(err)\r\n     }\r\n     fmt.Printf(\"Successfully deleted existed directory %s\\n\", providerName)\r\n  }\r\n  if _, err = os.Stat(providerName); os.IsNotExist(err) {\r\n     if err := os.Mkdir(providerName, 0755); err != nil {\r\n        if !os.IsExist(err) {\r\n           log.Fatal(err)\r\n        }\r\n        fmt.Printf(\"Successfully created directory %s\\n\", providerName)\r\n     }\r\n  }\r\n\r\n  for _, module := range modules.Data {\r\n     var description string\r\n     for _, attr := range modules.Included {\r\n        if module.Relationships.LatestVersion.Data.Id == attr.Id {\r\n           description = attr.Attributes.Description\r\n        }\r\n     }\r\n     if description == \"\" {\r\n        description = strings.ToUpper(providerName) + \" \" + strings.Title(module.Attributes.Name)\r\n     }\r\n\r\n     outputFile := fmt.Sprintf(\"%s/terraform-%s-%s.yaml\", providerName, providerName, module.Attributes.Name)\r\n     if _, err := os.Stat(outputFile); !os.IsNotExist(err) {\r\n        continue\r\n     }\r\n     if providerName == \"aws\" && (module.Attributes.Name == \"rds\" || module.Attributes.Name == \"s3-bucket\" ||\r\n        module.Attributes.Name == \"subnet\" || module.Attributes.Name == \"vpc\") {\r\n        continue\r\n     }\r\n     if err := generateDefinition(providerName, module.Attributes.Name, module.Attributes.Source, \"\", description); err != nil {\r\n        fmt.Println(err.Error())\r\n        os.Exit(1)\r\n     }\r\n  }\r\n}\r\n\r\nfunc generateDefinition(provider, name, gitURL, path, description string) error {\r\n  defYaml := filepath.Join(provider, fmt.Sprintf(\"terraform-%s-%s.yaml\", provider, name))\r\n\r\n  cmd := fmt.Sprintf(\"vela def init %s --type component --provider %s --git %s.git --desc \\\"%s\\\" -o %s\",\r\n     name, provider, gitURL, description, defYaml)\r\n  if path != \"\" {\r\n     cmd = fmt.Sprintf(\"%s --path %s\", cmd, path)\r\n  }\r\n  fmt.Println(cmd)\r\n  stdout, err := exec.Command(\"bash\", \"-c\", cmd).CombinedOutput()\r\n  if err != nil {\r\n     return errors.Wrap(err, string(stdout))\r\n  }\r\n  fmt.Println(string(stdout))\r\n  return nil\r\n}\r\n```\r\n\r\n\r\nExecuting the following command:\r\n\r\n\r\n```\r\ngo run gen.go aws \"https://registry.terraform.io/v2/modules?filter%5Bprovider%5D=aws&include=latest-version&page%5Bsize%5D=50&page%5Bnumber%5D=1\"\r\n```\r\n\r\n\r\n\r\n## Explanation for the code\r\n\r\n\r\n### Unmarshal the json data for the resources\r\n\r\nAccess the URL passed in by the user and parse the returned json data into the Go structure.\r\n\r\nThe json format corresponding to the resource is as follows.\r\n\r\n\r\n```\r\n{\r\n  \"data\": [\r\n    {\r\n      \"type\": \"modules\",\r\n      \"id\": \"23\",\r\n      \"attributes\": {\r\n        \"downloads\": 18440513,\r\n        \"full-name\": \"terraform-aws-modules/vpc/aws\",\r\n        \"name\": \"vpc\",\r\n        \"namespace\": \"terraform-aws-modules\",\r\n        \"owner-name\": \"\",\r\n        \"provider-logo-url\": \"/images/providers/aws.png\",\r\n        \"provider-name\": \"aws\",\r\n        \"source\": \"https://github.com/terraform-aws-modules/terraform-aws-vpc\",\r\n        \"verified\": true\r\n      },\r\n      \"relationships\": {\r\n        \"latest-version\": {\r\n          \"data\": {\r\n            \"id\": \"142143\",\r\n            \"type\": \"module-versions\"\r\n          }\r\n        }\r\n      },\r\n      \"links\": {\r\n        \"self\": \"/v2/modules/23\"\r\n      }\r\n    },\r\n    ...\r\n  ],\r\n  \"included\": [\r\n    {\r\n      \"type\": \"module-versions\",\r\n      \"id\": \"36806\",\r\n      \"attributes\": {\r\n        \"created-at\": \"2020-01-03T11:35:36Z\",\r\n        \"description\": \"Terraform module Terraform module for creating AWS IAM Roles with heredocs\",\r\n        \"downloads\": 260030,\r\n        \"published-at\": \"2020-02-06T06:26:08Z\",\r\n        \"source\": \"\",\r\n        \"tag\": \"v2.0.0\",\r\n        \"updated-at\": \"2022-02-22T00:45:44Z\",\r\n        \"version\": \"2.0.0\"\r\n      },\r\n      \"links\": {\r\n        \"self\": \"/v2/module-versions/36806\"\r\n      }\r\n    },\r\n    ...\r\n  ],\r\n  ...\r\n}\r\n```\r\n\r\n\r\nIn the json data corresponding to Modules, we only care about two key-value pairs, viz.\r\n\r\n* data: A list containing the names and properties of Modules\r\n* Included: Information about the specific version of Modules filtered out\r\n\r\nIn this case, for each Module element in data, resolve its attributes, Id and the id corresponding to the latest-version in relationship; for each Module version element in Included, resolve its attributes and Id.\r\n\r\nThe attributes are further resolved as follows five items: \r\n\r\n* Name\r\n* Downloads\r\n* Source\r\n* Description\r\n* Verified\r\n\r\nThe Go structure is named as `TFDownload `, The http library gets the json data and then parses the structure of the Terraform modules through the `json.Unmarshal`.\r\n\r\n\r\n### generating ComponentDefinitions in batch\r\n\r\n1. creating directory and component definitions\r\n\r\nAfter parsing, create a new folder in the current directory and name the folder as \\<provider name\\>.\r\n\r\nIterate through the parsed data, and for each Module element, perform the following operations to generate the corresponding definition and documentation for it.\r\n\r\n2. Generate definition files\r\n\r\nGenerate the definition file by reading the corresponding information from the module's github repository using the following vela command.\r\n\r\n\r\n```\r\nvela def init {ModuleName} --type component --provider {providerName} --git {gitURL} --desc {description} -o {yamlFileName}\r\n```\r\n\r\n\r\nSeveral items to be filled in the instruction are passed in from the parsed Module structure.\r\n\r\n* gitURL: \t{Module.Attributes.Source}.git\r\n* description: If there are elements in `Included` which have the same ID with relationship.latest-version.ID, set the description as the corresponding description in `Included` elements, otherwise set the description as providerName+ModuleName. \r\n* yamlFileName：terraform-{providerName}-{Module.Attributes.Name}.yaml\r\n\r\n\r\n\r\n## Have a try?\r\n\r\nThere are also a number of cloud providers that offer a wealth of Terraform modules, such as\r\n\r\nGCP: [https://registry.terraform.io/namespaces/terraform-google-modules](https://registry.terraform.io/namespaces/terraform-google-modules)\r\n\r\nAlibaba Cloud: [https://registry.terraform.io/namespaces/terraform-alicloud-modules](https://registry.terraform.io/namespaces/terraform-alicloud-modules)\r\n\r\nDo you want to extend cloud resources for your current or favorite cloud provider for KubeVela as well?"
    },
    {
      "id": "/2022/01/27/blog-1.2",
      "metadata": {
        "permalink": "/blog/2022/01/27/blog-1.2",
        "editUrl": "https://github.com/kubevela/kubevela.io/tree/main/blog/2022-01-27-blog-1.2.md",
        "source": "@site/blog/2022-01-27-blog-1.2.md",
        "title": "KubeVela v1.2 - Focused on Developer Experience, Simplified Multi-Cluster Application Delivery",
        "description": "",
        "date": "2022-01-27T00:00:00.000Z",
        "formattedDate": "January 27, 2022",
        "tags": [
          {
            "label": "DevOps",
            "permalink": "/blog/tags/dev-ops"
          },
          {
            "label": "release-note",
            "permalink": "/blog/tags/release-note"
          }
        ],
        "readingTime": 11.79,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "KubeVela Maintainers",
            "title": "KubeVela Team",
            "url": "https://github.com/kubevela/kubevela",
            "imageURL": "https://kubevela.io/img/logo.svg"
          }
        ],
        "frontMatter": {
          "title": "KubeVela v1.2 - Focused on Developer Experience, Simplified Multi-Cluster Application Delivery",
          "author": "KubeVela Maintainers",
          "author_title": "KubeVela Team",
          "author_url": "https://github.com/kubevela/kubevela",
          "author_image_url": "https://kubevela.io/img/logo.svg",
          "tags": [
            "DevOps",
            "release-note"
          ],
          "description": "",
          "hide_table_of_contents": false
        },
        "prevItem": {
          "title": "Generate top 50 popular resources of AWS using 100 lines of code",
          "permalink": "/blog/2022/03/01/kubevela-generate-top-50-popular-resources-of-aws-using-100-lines-of-code"
        },
        "nextItem": {
          "title": "Using GitOps + KubeVela for Application Continuous Delivery",
          "permalink": "/blog/2021/10/10/kubevela-gitops"
        }
      },
      "content": "As the cloud native technologies grows continuously, more and more infrastructure capabilities are becoming standardized PaaS or SaaS products. To build a product you don't need a whole team to do it nowadays. Because there are so many services that can take roles from software developing, testing to infrastructure operations. As driven the culture of agile development and cloud native technologies, more and more roles can be shifted left to developers, e.g. testing, monitoring, security. As emphasized by the DevOps concepts, it can be done in the development phase for the work of monitoring, security, and operations via open source projects and cloud services. Nonetheless, this also creates huge challenges to developers, as they might lack the control of diverse products and complex APIs. Not only do they have to make choices, but also they need to understand and coordinate the complex, heterogeneous infrastructure capabilities in order to satisfy the fast-changing requirements of the business.\r\n\r\nThis complexity and uncertainty has exacerbated the developer experience undoubtedly, reducing the delivery efficiency of business system, increasing the operational risks. The tenet of developer experience is simplicity and efficiency. Not only the developers but also the enterprises have to choose the better developer tools and platforms to achieve this goal. This is also the focus of KubeVela v1.2 and upcoming release that to build a modern platform based on cloud native technologies and covering development, delivery, and operations. We can see from the following diagram of KubeVela architecture that developers only need to focus on applications per se, and use differentiated operational and delivery capabilities around the applications.\r\n\r\n![image.png](https://static.kubevela.net/images/1.2/architecture.jpg)\r\n\r\n<!--truncate-->\r\n\r\npic 1. KubeVela Architecture\r\n\r\n## OAM & KubeVela History\r\nLet's retrospect the history of OAM and KubeVela to understand how it is formed this way:\r\n\r\n- **OAM（Open Application Model）birth and growth**\r\n\r\nTo create simplicity in a complex world, the first problem we need to solve is how to make standard abstractions? OAM creatively proposes two ways of separation: separation between applications and resources, and separation between development and operation (in ideal world the operation can be fully automated). It is a cloud native application specification which provides everything-as-a-service, complete modularity design. The spec has been getting tractions among major vendors all over the world since it's been announced. Because we all share a common goal -- To reduce learning curve and provide application lego-style invention for developers.\r\n\r\n- **v1.0 release of KubeVela, bringing the OAM spec implementation**\r\n\r\nWith the application specification as the guidance, advanced community users can create their own tools to build practical solutions. But it is unaccessible to most developers though. KubeVela was born as the community standard implementation to solve this problem. It absorbs the good parts from latest Kubernetes community development. It provides automated, idempotent, reliable application rollout controllers. With its features, KubeVela can empower developers to quickly deploy OAM-compliant applications.\r\n\r\n- **v1.1 release of KubeVela, provides delivery workflow, making multi-cluster rollout controlled and simplified**\r\n\r\nAs more and more enterprises adopt the cloud, hybrid and distributed cloud will certainly become the future norm. KubeVela has been designed and built based on hybrid cloud infrastructure as a modern application management system. We anticipate that the architecture of modern enterprise applications will be heterogenous considering factors of availability, performance, data security, etc. In KubeVela 1.1, we adds new feature to achieve programmable delivery workflow. It natively fits the multi-cluster architecture to provide modern multi-cluster application rollout.\r\n\r\nBy the time of 2022, on the road to serve developers, KubeVela has gone to the fourth phase. It is going to empower developers to do multi-cluster rollout way more easily. In the following we will dissect its changes:\r\n\r\n## Core Features in v1.2 Release\r\n### The new GUI project: VelaUX\r\nIt is the best choice to reduce developer learning curve by  providing an easy-to-use UI console. Since the inception, KubeVela community has been asking for UI. With the v1.2 release, it has finally come. Providing GUI will help developers organize and compose heterogeneous aplications in a standard way. This will help them analyze and discover business obstacles quicker.\r\n\r\n[VelaUX](https://github.com/kubevela/velaux) is the frontend project of KubeVela with extensible core design. It introduces low-code experience for users to drag-and-drop form that takes user input based on dynamic components. To achieve this we have designed the frontend description spec [UISchema](https://kubevela.io/docs/reference/ui-schema) with [X-Definition](https://kubevela.io/docs/platform-engineers/oam/x-definition), and multi-dimensional query language [VelaQL](https://kubevela.io/docs/platform-engineers/system-operation/velaql). This design makes the foundation for the heterogenous application delivery architecture of KubeVela.\r\n\r\nFrom GUI, users can manage addons, connect Kubernetes clusters, distribute delivery targets, set up environments and deploy all kinds of apps, monitor runtime status, achieve full lifecycle management of application delivery.\r\n\r\n![image.png](https://static.kubevela.net/images/1.2/dashboard.jpeg)\r\npic 2. KubeVela Application Dashboard\r\n\r\nFor the new terms in GUI, please refer to [Core Concepts](https://kubevela.io/docs/next/getting-started/core-concept) documentation to learn more details.\r\n### Unified Multi-Cluster Control\r\nKubeVela will manage N Kubernetes clusters, N cloud vendor services in a big unified infrastructure pool. From that our developers can set up different environments based on business requirements, workflow policies, team collaboration needs, etc. This will create separate environment workspaces from big infrastructure resource pool. One application can be deployed into multiple environments, and environments are isolated from each other in both management and runtime.\r\n\r\n![image.png](https://static.kubevela.net/images/1.2/multiple-cluster.png)\r\npic 3. KubeVela Application Status\r\n\r\nAs shown above, an application can be deployed to default environments and other custom environments such as test or prod. Each environment can include multiple delivery targets. Each delivery target indicates an independent, separate Kubernetes cluster.\r\n\r\n### Heterogeneous Application Architecture\r\nIn terms of cloud native technologies, we have many options to pick for build application delivery solutions. Based on Kubernetes, we can use mature technologies like Helm Chart to delivery middleware and third-party open source softwares. We can deliver enterprise business applications via container images. We can also use OpenYurt to deliver and mange edge applications. Based on the open technologies of cloud services, we can deliver database, message queues, cache, etc. middleware, including operational features like logging, monitoring.\r\n\r\nWith so many options, KubeVela adopts OAM as the standard application definition to manage heterogeneous application architecture uniformly. KubeVela provides highly extensible delivery core engine. Users can use built-in or install more plugins to extend the platform, and manage application deliveries in a consistent way. On top of KubeVela, what users see is the modular, everything-as-a-service control plane.\r\n\r\n![image.png](https://static.kubevela.net/images/1.2/cloud-resource.png)\r\npic 4. Cloud Resources Deploy\r\n\r\nAs shown above, we can tell that in the application management page users can conveniently deliver cloud resources. Developers can read the following docs to understand the full delivery process of heterogeneous application architecture:\r\n\r\n1. [Deliver Docker Image](https://kubevela.io/docs/next/deliver-app/webservice)\r\n2. [Deliver Helm Chart](https://kubevela.io/docs/next/deliver-app/helm)\r\n3. [Deliver Kubernetes Resources](https://kubevela.io/docs/next/deliver-app/k8s-object)\r\n4. [Deliver cloud resources](https://kubevela.io/docs/next/deliver-app/consume-cloud-services)\r\n\r\n### Extension System\r\nKubeVela has been designed as an extensible system from the very beginning. The aforementioned heterogeneous application architecture can be achieved via KubeVela's extension system. It can be extended via standard interfaces and plugin as many capabilities as you want. This will match the differentiated requirements of enterprises while reducing the cognitive burden incurred in learning new things. KubeVela's extension system includes component types, operational traits, workflow types, delivery policies, etc. In current release, we have the addon management system. An addon packages the extension capabilities for easy distribution.\r\n\r\n![image.png](https://static.kubevela.net/images/1.2/addon.png)\r\npic 5. KubeVela Addons\r\n\r\nCurrently we provide an official catalog with pre-packaged addons shown as above. Meanwhile in the experimental catalog repo we can collaborating with community users to create more capabilities.\r\n\r\nBy now, KubeVela has grown into an application delivery platform that serve developers directly. What enterprise scenarios can we use KubeVela for? In the following we list a couple of common scenarios:\r\n\r\n## Enterprise Software Delivery Solutions\r\n### Multi-Cluster DevOps\r\nToday many enterprise software delivery looks like the following diagram. They use the compute resources from cloud vendors for both the demo and production environments. But they use their in-house server farm for the development or testing environments. If any business applications have multi-region disaster recovery requirements, then production environments can span multiple regions or even clouds.\r\n\r\n![image.png](https://static.kubevela.net/images/1.2/devops-en.png)\r\npic 6. DevOps Pipeline\r\n\r\nFor basic DevOps workflow, it includes code hosting, CI/CD process. KubeVela can provide support for CD process. To enterprises the following are the practical steps:\r\n\r\n1. Prepare local and cloud resources according to real needs. Make sure local and cloud resources are connected in the same network plane for unified resource management.\r\n2. Deploy KubeVela into the production environment and ensure its accessibility.\r\n3. Install DevOps toolchain like Gitlab, Jenkins, Sonar via KubeVela. Usually the accessability of code hosting and developer toolchain are critical and we must deploy them to production environments. Unless you local clusters can ensure accessibility, can hope the business code to exist in local environment, then you can deploy them to local clusters.\r\n4. Setup local development environments via KubeVela, deploy testing middleware in local. Setup cloud middleware in production environments. \r\n5. Setup business code CI piplines via Jenkins. Generates Docker image and send it to KubeVela to do multi-environment deployment. This will make up an end-to-end application delivery workflow.\r\n\r\nUsing KubeVela multi-cluster DevOps solution will provide the following advantages:\r\n\r\n1. Developers do not need to know any Kubernetes knowledge to achieve heterogeneous cloud-native application delivery.\r\n2. Unified multi-cluster, multi-environment management in a single control plane. Natively deploy multi-cluster applications.\r\n3. Unified application management mode, regardless of business applications or developer toolchain.\r\n4. Flexible workflow to help enterprises to glue various software delivery processes in a single workflow.\r\n\r\n\r\n### Unified Management of Heterogeneous Environments\r\nDifferent enterprises face different problems and requirements of infrastructure and business. On the infrastructure side, enterprises could build in-house private cloud, yet buy some public cloud resources, and own some edge devices. On the business side, the variance of scale and requirements will lead to multi-cloud, multi-region application architecture, while keeping some legacy systems. On the developer side, developing software will need various environments such as development, testing, staging, production. On the management side, different business teams need isolation from each other, while opening up connection between some business applications.\r\n​\r\n\r\nIn the past, it was very easy to become fragmented between different business teams inside enterprises. This fragmentation exists in: toolchain, technical architecture, business management. We take this into account while being innovative in technologies. KubeVela brings a new solution that pursues unified management and extensible architecture with good compatibility.\r\n\r\n- On the infrastructure side, we support different API formats including Kubernetes API, cloud APIs, and custom APIs to model all kinds of the infrastructure.\r\n- On the business architecture side, the application model is open and platform agnostic. KubeVela provides the ability to connect and empower businesses.\r\n- On the Developer toolchain side, there might be different toolchain and artifacts in the enterprises. KubeVela provides the extension mechanism and standard models to combine different kinds of artifacts into a standardized delivery workflow. Surely, its standards are shifting left and empowering enterprises to unify toolchain management. You don't need to concern whether you are using Gitlab or Jenkins because KubeVela can integrate them both.\r\n- On the operations side, the operational capabilities and toolchain solutions can be unified under KubeVela standards in the enterprises. Moreover, the community operational capabilities can be shared and reused easily via KubeVela extensions.\r\n\r\nThus, KubeVela can be used to connect different stages inside the enterprises, and unify all capabilities in a single platform. It is a practical and future-proof solution.\r\n\r\n### Enterprise Internal Application Platform\r\nMany enterprises that has enough development power will choose to build internal application platforms. The main reason is that they can customize the platform to make it very easy for their use cases. In the past we can see there are many PaaS platforms born out of Cloud Foundry. We all know the stereotypes of application platforms will not satisfy all enterprises. If the application package format and delivery workflow can standardized inside enterprises, then all users need to do is to fill the image name. However, in traditional PaaS platforms developers have to understand a bunch of so-called general concepts. For example, if an enterprise want to deploy AI applications, and there is some difference for AI application architecture, then we have to create such AI PaaS, and enterprises have to pay more fees and learn more concepts.\r\n\r\nTherefore, when general products couldn't satisfy the needs of enterprises, they will consider develop one on their own. But it takes so much resources to build an internal platform from scratch. Sometimes it even surpasses the investment of their core business. This is not a feasible solution.\r\n\r\nWith above introduction, are you more familiar with the motivations and history of KubeVela? There is no such a product to be the silver bullet. But our goal is to create a standardized model to empower more and more enterprises and developers to participate in the path towards building simple and efficient developer tools. KubeVela is still in early development phase. We still hope you can join us to develop it together. We want to thank [the 100+ contributors](https://github.com/kubevela/kubevela/graphs/contributors) who contributed to KubeVela.\r\n\r\n## Join the Community\r\n### Collaborate on OAM Specification\r\nOAM spec is the cornerstone of modern application platform architecture. Currently, OAM spec is driven by implementation of KubeVela for future improvement while the spec didn't rely on KubeVela. We highly encourage cloud vendors, platform builders, and end users to join us to define OAM spec together. We highly appreciate that vendors like Tencent, China Telecom, China Unicom have supported OAM spec and started collaborative work. Every person and organization are welcome to share your ideas, suggestions, and thoughts.\r\n\r\nGo the [Community repo](https://github.com/oam-dev/spec).\r\n\r\n### Collaborate on Addon ecosystem\r\nAs mentioned above, we have created the addon extension system, and encourage community developers to contribute your tools, and share your thoughts.\r\n\r\n- Go to the [Community repo](https://github.com/kubevela/catalog)\r\n- [How to contribute an addon](https://kubevela.io/docs/platform-engineers/addon/intro)\r\n\r\n### Contribute Cloud Resources\r\nKubeVela integrated Terraform Module with [Terraform controller](https://github.com/oam-dev/terraform-controller) to extend cloud resources. We have supported [several cloud resources](https://kubevela.io/docs/end-user/components/cloud-services/provider-and-consume-cloud-services), and encourage community developers or cloud providers to contribute more.\r\n\r\nGo to [contribute cloud resource](https://kubevela.io/docs/platform-engineers/components/component-terraform).\r\n\r\n### Provide Your Feedback\r\nWe highly welcome everyone to participate in the KubeVela community discussion whether you want to know more or contribute code!\r\n\r\nGo to [Community repo](https://github.com/kubevela/kubevela).\r\n\r\n> KubeVela  is a CNCF sandbox project. Learn more by reading the [official documentation](https://kubevela.io/docs/install)"
    },
    {
      "id": "/2021/10/10/kubevela-gitops",
      "metadata": {
        "permalink": "/blog/2021/10/10/kubevela-gitops",
        "editUrl": "https://github.com/kubevela/kubevela.io/tree/main/blog/2021-10-10-kubevela-gitops.md",
        "source": "@site/blog/2021-10-10-kubevela-gitops.md",
        "title": "Using GitOps + KubeVela for Application Continuous Delivery",
        "description": "",
        "date": "2021-10-10T00:00:00.000Z",
        "formattedDate": "October 10, 2021",
        "tags": [
          {
            "label": "KubeVela",
            "permalink": "/blog/tags/kube-vela"
          },
          {
            "label": "use-case",
            "permalink": "/blog/tags/use-case"
          }
        ],
        "readingTime": 12.375,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Tianxin Dong",
            "title": "KubeVela Team",
            "url": "https://github.com/kubevela/kubevela",
            "imageURL": "https://kubevela.io/img/logo.svg"
          }
        ],
        "frontMatter": {
          "title": "Using GitOps + KubeVela for Application Continuous Delivery",
          "author": "Tianxin Dong",
          "author_title": "KubeVela Team",
          "author_url": "https://github.com/kubevela/kubevela",
          "author_image_url": "https://kubevela.io/img/logo.svg",
          "tags": [
            "KubeVela",
            "use-case"
          ],
          "description": "",
          "image": "https://raw.githubusercontent.com/kubevela/kubevela.io/main/docs/resources/KubeVela-03.png",
          "hide_table_of_contents": false
        },
        "prevItem": {
          "title": "KubeVela v1.2 - Focused on Developer Experience, Simplified Multi-Cluster Application Delivery",
          "permalink": "/blog/2022/01/27/blog-1.2"
        },
        "nextItem": {
          "title": "KubeVela Releases 1.1, Reaching New Peaks in Cloud-Native Continuous Delivery",
          "permalink": "/blog/2021/10/08/blog-1.1"
        }
      },
      "content": "KubeVela is a simple, easy-to-use, and highly extensible cloud-native application platform. It can make developers deliver microservices applications easily, without knowing Kubernetes details.\r\n\r\nKubeVela is based on OAM model, which naturally solves the orchestration problems of complex resources. It means that KubeVela can manage complex large-scale applications with GitOps. Convergence of team and system size after the system complexity problem.\r\n\r\n<!--truncate-->\r\n\r\n## What is GitOps\r\n\r\nGitOps is a modern way to do continuous delivery. Its core idea is to have a Git repository which contains environmental and application configurations. An automated process is also needed for sync the config to cluster.\r\n\r\nBy changing the files in repository, developers can apply the applications automatically. The benefits of applying GitOps include:\r\n- Increased productivity. Continuous delivery can speed up the time of deployment.\r\n- Lower the barrier for developer to deploy. By pushing code instead of container configuration, developers can easily deploy Kubernetes without knowing its internal implementation.\r\n- Trace the change records. Managing the cluster with Git makes every change traceable, enhancing the audit trail.\r\n- Recover the cluster with Git's rollback and branch.\r\n\r\n## GitOps with KubeVela\r\n\r\nKubeVela as an declarative application delivery control plane can be naturally used in GitOps approach, and this will provide below extra bonus to end users alongside with GitOps benefits:\r\n- application delivery workflow (CD pipeline)\r\n  - i.e. KubeVela supports pipeline style application delivery process in GitOps, instead of simply declaring final status;\r\n- handling deployment dependencies and designing typologies (DAG);\r\n- unified higher level abstraction atop various GitOps tools' primitives;\r\n- declare, provision and consume cloud resources in unified application definition;\r\n- various out-of-box deployment strategies (Canary, Blue-Green ...);\r\n- various out-of-box hybrid/multi-cloud deployment policies (placement rule, cluster selectors etc.);\r\n- Kustomize-style patch for multi-env deployment without the need to learn Kustomize at all;\r\n- ... and much more.\r\n\r\nIn this section, we will introduce steps of using KubeVela directly in GitOps approach.\r\n\r\n## GitOps workflow\r\n\r\nThe GitOps workflow is divided into CI and CD:\r\n\r\n* CI(Continuous Integration): Continuous integration builds code and images, and pushes images to the registry. There are many CI tools like GitHub Action, Travis, Jenkins and so on. In this article, we use GitHub Action for CI. You can also use other CI tools. KubeVela can connect CI processes under any tool around GitOps.\r\n* CD(Continuous Delivery): Continuous delivery automatically updates the configuration in the cluster. For example, update the latest images in the registry to the cluster.\r\n  * Currently there are two main CD modes:\r\n    * Push-based: Push mode CD is mainly accomplished by configuring CI pipeline. In this way, the access key of the cluster is shared with CI so that the CI pipeline can push changes to the cluster. For this mode, please refer to our previous blog post: [Using Jenkins + KubeVela for Application Continuous Delivery](/blog/2021/09/02/kubevela-jenkins-cicd).\r\n    * Pull-based: Pull mode CD listens for changes to the repository (code repository or configuration repository) in the cluster and synchronizes those changes to the cluster. In this way, the cluster actively pulls the update, thus avoiding the problem of exposing the secret key. This article will introduce using KubeVela and GitOps in pull mode.\r\n\r\nThis article will separate into two perspectives:\r\n\r\n1. For platform administrators/SREs, they can update the config in Git repo. It will trigger automated re-deployment.\r\n\r\n2. For developers, they can update the app source code and then push it to Git. It will trigger building latest image and re-deployment.\r\n\r\n## For platform administrators/SREs\r\n\r\nPlatform administrators/SREs prepares the Git repo for operational config. Every config change will be traceable by that. KubeVela will watch the repo and apply changes to the clusters.\r\n\r\n![alt](/img/gitops/ops-flow.jpg)\r\n\r\n## Setup Config Repository\r\n\r\n> The configuration files are from the [Example Repo](https://github.com/kubevela/samples/tree/master/09.GitOps_Demo/for-SREs).\r\n\r\nIn this example, we will deploy an application and a database, the application uses the database to store data.\r\n\r\nThe structure of the config repository looks below:\r\n\r\n* The `clusters/` contains the GitOps config. It will command KubeVela to watch the specified repo and apply latest changes.\r\n* The `apps/` contains the Application yaml for deploying the user-facing app.\r\n* The `infrastructure/` contains infrastructure tools, i.e. MySQL database.\r\n\r\n```shell\r\n├── apps\r\n│   └── my-app.yaml\r\n├── clusters\r\n│   ├── apps.yaml\r\n│   └── infra.yaml\r\n└── infrastructure\r\n    └── mysql.yaml\r\n```\r\n\r\n> KubeVela recommends using the directory structure above to manage your GitOps repository. `clusters/` holds the associated KubeVela GitOps configuration that need to be applied to cluster manually, `apps/` holds your application and `infrastructure/` holds your base configuration. By separating applications from basic configurations, you can manage your deployment environment more reasonably and isolate application changes.\r\n\r\n#### Directory `clusters/`\r\n\r\nThe `clusters/` is the initialize configuration directory for KubeVela GitOps.\r\n\r\nBelow is how the `clusters/infra.yaml` looks like:\r\n\r\n```yaml\r\napiVersion: core.oam.dev/v1beta1\r\nkind: Application\r\nmetadata:\r\n  name: infra\r\nspec:\r\n  components:\r\n  - name: database-config\r\n    type: kustomize\r\n    properties:\r\n      repoType: git\r\n      # replace it with your repo url\r\n      url: https://github.com/FogDong/KubeVela-GitOps-Infra-Demo\r\n      # replace it with your git secret if it's a private repo\r\n      # secretRef: git-secret\r\n      # the pull interval time, set to 10m since the infrastructure is steady\r\n      pullInterval: 10m\r\n      git:\r\n        # the branch name\r\n        branch: main\r\n      # the path to sync\r\n      path: ./infrastructure\r\n```\r\n\r\n`apps.yaml` and `infra.yaml` in `clusters/` are similar. Their difference is to watch different directories. In `apps.yaml`, the `properties.path` will be `./apps`.\r\n\r\nApply the files in `clusters/` manually. They will sync the files in `infrastructure/` and `apps/` dir of the Git repo.\r\n\r\n#### Directory `apps/`\r\n\r\nThe file in `apps/` is a simple application with database information and Ingress. The app serves HTTP service and connects to a MySQL database. In the '/' path, it will display the version in the code; in the `/db` path, it will list the data in database.\r\n\r\n```yaml\r\napiVersion: core.oam.dev/v1beta1\r\nkind: Application\r\nmetadata:\r\n  name: my-app\r\n  namespace: default\r\nspec:\r\n  components:\r\n    - name: my-server\r\n      type: webservice\r\n      properties:\r\n        image: <your image address> # {\"$imagepolicy\": \"default:apps\"}\r\n        port: 8088\r\n        env:\r\n          - name: DB_HOST\r\n            value: mysql-cluster-mysql.default.svc.cluster.local:3306\r\n          - name: DB_PASSWORD\r\n            valueFrom:\r\n              secretKeyRef:\r\n                name: mysql-secret\r\n                key: ROOT_PASSWORD\r\n      traits:\r\n        - type: ingress\r\n          properties:\r\n            domain: testsvc.example.com\r\n            http:\r\n              /: 8088\r\n```\r\n\r\nThis is an Application binds with Traits Ingress. In this way, the underlying Deployment, Service, and Ingress can be brought together in a single file, making it easier to manage the application.\r\n\r\n#### Directory `infrastructure/`\r\n\r\nThe `infrastructure/` contains the config of some infrastructures like database. In the following, we will use [MySQL operator](https://github.com/bitpoke/mysql-operator) to deploy a MySQL cluster.\r\n\r\n> Notice that there must be a secret in your cluster with MySQL password specified in key `ROOT_PASSWORD`.\r\n\r\n```yaml\r\napiVersion: core.oam.dev/v1beta1\r\nkind: Application\r\nmetadata:\r\n  name: mysql\r\n  namespace: default\r\nspec:\r\n  components:\r\n    - name: mysql-controller\r\n      type: helm\r\n      properties:\r\n        repoType: helm\r\n        url: https://presslabs.github.io/charts\r\n        chart: mysql-operator\r\n        version: \"0.4.0\"\r\n    - name: mysql-cluster\r\n      type: raw\r\n      dependsOn:\r\n        - mysql-controller\r\n      properties:\r\n        apiVersion: mysql.presslabs.org/v1alpha1\r\n        kind: MysqlCluster\r\n        metadata:\r\n          name: mysql-cluster\r\n        spec:\r\n          replicas: 1\r\n          # replace it with your secret\r\n          secretName: mysql-secret\r\n```\r\n\r\nWe use workflow in this Application. The first step is to deploy the MySQL controller, after the controller is running, the second step will deploy the MySQL cluster.\r\n\r\n#### Apply the files in `clusters/`\r\n\r\nAfter storing bellow files in the Git config repo, we need to apply the GitOps config files in `clusters/` manually.\r\n\r\nFirst, apply the `clusters/infra.yaml` to cluster, we can see that the MySQL in `infrastructure/` is automatically deployed:\r\n\r\n```shell\r\nkubectl apply -f clusters/infra.yaml\r\n```\r\n\r\n```shell\r\n$ vela ls\r\n\r\nAPP   \tCOMPONENT       \tTYPE      \tTRAITS \tPHASE  \tHEALTHY\tSTATUS\tCREATED-TIME\r\ninfra \tdatabase-config \tkustomize \t       \trunning\thealthy\t      \t2021-09-26 20:48:09 +0800 CST\r\nmysql \tmysql-controller\thelm      \t       \trunning\thealthy\t      \t2021-09-26 20:48:11 +0800 CST\r\n└─  \tmysql-cluster   \traw       \t       \trunning\thealthy\t      \t2021-09-26 20:48:11 +0800 CST\r\n```\r\n\r\nApply the `clusters/apps.yaml` to cluster, we can see that the application in `apps/` is automatically deployed:\r\n\r\n```shell\r\nkubectl apply -f clusters/apps.yaml\r\n```\r\n\r\n```shell\r\nAPP   \tCOMPONENT       \tTYPE      \tTRAITS \tPHASE  \tHEALTHY\tSTATUS\tCREATED-TIME\r\napps  \tapps            \tkustomize \t       \trunning\thealthy\t      \t2021-09-27 16:55:53 +0800 CST\r\ninfra \tdatabase-config \tkustomize \t       \trunning\thealthy\t      \t2021-09-26 20:48:09 +0800 CST\r\nmy-app\tmy-server       \twebservice\tingress\trunning\thealthy\t      \t2021-09-27 16:55:55 +0800 CST\r\nmysql \tmysql-controller\thelm      \t       \trunning\thealthy\t      \t2021-09-26 20:48:11 +0800 CST\r\n└─  \tmysql-cluster   \traw       \t       \trunning\thealthy\t      \t2021-09-26 20:48:11 +0800 CST\r\n```\r\n\r\nBy deploying the KubeVela GitOps config files, we now automatically apply the application and database in cluster.\r\n\r\n`curl` the Ingress of the app, we can see that the current version is `0.1.5` and the application is connected to the database successfully:\r\n\r\n```shell\r\n$ kubectl get ingress\r\nNAME        CLASS    HOSTS                 ADDRESS         PORTS   AGE\r\nmy-server   <none>   testsvc.example.com   <ingress-ip>    80      162m\r\n\r\n$ curl -H \"Host:testsvc.example.com\" http://<ingress-ip>\r\nVersion: 0.1.5\r\n\r\n$ curl -H \"Host:testsvc.example.com\" http://<ingress-ip>/db\r\nUser: KubeVela\r\nDescription: It's a test user\r\n```\r\n\r\n## Modify the config for GitOps trigger\r\n\r\nAfter the first deployment, we can modify the files in config repo to update the applications in the cluster.\r\n\r\nModify the domain of the application's Ingress:\r\n\r\n```yaml\r\n...\r\n      traits:\r\n        - type: ingress\r\n          properties:\r\n            domain: kubevela.example.com\r\n            http:\r\n              /: 8089\r\n```\r\n\r\nCheck the Ingress in cluster after a while:\r\n\r\n```shell\r\nNAME        CLASS    HOSTS                 ADDRESS         PORTS   AGE\r\nmy-server   <none>   kubevela.example.com  <ingress-ip>    80      162m\r\n```\r\n\r\nThe host of the Ingress has been updated successfully!\r\n\r\nIn this way, we can edit the files in the Git repo to update the cluster.\r\n\r\n## For developers\r\n\r\nDevelopers writes the application source code and push it to a Git repo (aka app repo). Once app repo updates, the CI will build the image and push it to the image registry. KubeVela watches the image registry, and updates the image in config repo. Finally, it will apply the config to the cluster.\r\n\r\nUser can update the configuration in the cluster automatically when the code is updated.\r\n\r\n![alt](/img/gitops/dev-flow.jpg)\r\n\r\n### Setup App Code Repository\r\n\r\nSetup a Git repository with source code and Dockerfile.\r\n\r\nThe app serves HTTP service and connects to a MySQL database. In the '/' path, it will display the version in the code; in the `/db` path, it will list the data in database.\r\n\r\n```go\r\n\thttp.HandleFunc(\"/\", func(w http.ResponseWriter, r *http.Request) {\r\n\t\t_, _ = fmt.Fprintf(w, \"Version: %s\\n\", VERSION)\r\n\t})\r\n\thttp.HandleFunc(\"/db\", func(w http.ResponseWriter, r *http.Request) {\r\n\t\trows, err := db.Query(\"select * from userinfo;\")\r\n\t\tif err != nil {\r\n\t\t\t_, _ = fmt.Fprintf(w, \"Error: %v\\n\", err)\r\n\t\t}\r\n\t\tfor rows.Next() {\r\n\t\t\tvar username string\r\n\t\t\tvar desc string\r\n\t\t\terr = rows.Scan(&username, &desc)\r\n\t\t\tif err != nil {\r\n\t\t\t\t_, _ = fmt.Fprintf(w, \"Scan Error: %v\\n\", err)\r\n\t\t\t}\r\n\t\t\t_, _ = fmt.Fprintf(w, \"User: %s \\nDescription: %s\\n\\n\", username, desc)\r\n\t\t}\r\n\t})\r\n\r\n\tif err := http.ListenAndServe(\":8088\", nil); err != nil {\r\n\t\tpanic(err.Error())\r\n\t}\r\n```\r\n\r\nIn this tutorial, we will setup a CI pipeline using GitHub Actions to build the image and push it to a registry. The code and configuration files are from the [Example Repo](https://github.com/kubevela/samples/tree/master/09.GitOps_Demo/for-developers/app-code).\r\n\r\n## Create Git Secret for KubeVela committing to Config Repo\r\n\r\nAfter the new image is pushed to the image registry, KubeVela will be notified and update the `Application` file in the Git repository and cluster. Therefore, we need a secret with Git information for KubeVela to commit to the Git repository. Fill the following yaml files with your password and apply it to the cluster:\r\n\r\n```yaml\r\napiVersion: v1\r\nkind: Secret\r\nmetadata:\r\n  name: my-secret\r\ntype: kubernetes.io/basic-auth\r\nstringData:\r\n  username: <your username>\r\n  password: <your password>\r\n```\r\n\r\n## Setup Config Repository\r\n\r\nThe configuration repository is almost the same as the previous configuration, you only need to add the image registry config to the file. For more details, please refer to [Example Repository](https://github.com/kubevela/samples/tree/master/09.GitOps_Demo/for-developers/kubevela-config).\r\n\r\nAdd the config of image registry in `clusters/apps.yaml`, it listens for image updates in the image registry:\r\n\r\n```yaml\r\n...\r\n  imageRepository:\r\n    image: <your image>\r\n    # if it's a private image registry, use `kubectl create secret docker-registry` to create the secret\r\n    # secretRef: imagesecret\r\n    filterTags:\r\n      # filter the image tag\r\n      pattern: '^master-[a-f0-9]+-(?P<ts>[0-9]+)'\r\n      extract: '$ts'\r\n    # use the policy to sort the latest image tag and update\r\n    policy:\r\n      numerical:\r\n        order: asc\r\n    # add more commit message\r\n    commitMessage: \"Image: {{range .Updated.Images}}{{println .}}{{end}}\"\r\n```\r\n\r\nModify the image field in `apps/my-app.yaml` and add annotation `# {\"$imagepolicy\": \"default:apps\"}`.\r\nNotice that KubeVela will only be able to modify the image field if the annotation is added after the field. `default:apps` is `namespace:name` of the GitOps config file above.\r\n\r\n```yaml\r\nspec:\r\n  components:\r\n    - name: my-server\r\n      type: webservice\r\n      properties:\r\n        image: ghcr.io/fogdong/test-fog:master-cba5605f-1632714412 # {\"$imagepolicy\": \"default:apps\"}\r\n```\r\n\r\nAfter update the files in `clusters/` to cluster, we can then update the application by modifying the code.\r\n\r\n## Modify the code\r\n\r\nChange the `Version` to `0.1.6` and modify the data in database:\r\n\r\n```go\r\nconst VERSION = \"0.1.6\"\r\n\r\n...\r\n\r\nfunc InsertInitData(db *sql.DB) {\r\n\tstmt, err := db.Prepare(insertInitData)\r\n\tif err != nil {\r\n\t\tpanic(err)\r\n\t}\r\n\tdefer stmt.Close()\r\n\r\n\t_, err = stmt.Exec(\"KubeVela2\", \"It's another test user\")\r\n\tif err != nil {\r\n\t\tpanic(err)\r\n\t}\r\n}\r\n```\r\n\r\nCommit the change to the Git Repository, we can see that our CI pipelines has built the image and push it to the image registry.\r\n\r\nKubeVela will listen to the image registry and update the `apps/my-app.yaml` in Git Repository with the latest image tag.\r\n\r\nWe can see that there is a commit form `kubevelabot`, the commit message is always with a prefix `Update image automatically.` You can use format like `{{range .Updated.Images}}{{println .}}{{end}}` to specify the image name in the `commitMessage` field.\r\n\r\n![alt](/img/gitops/gitops-commit.png)\r\n\r\n> Note that if you want to put the code and config in the same repository, you need to filter out the commit from KubeVela in CI configuration like below to avoid the repeat build of pipeline.\r\n> \r\n> ```shell\r\n> jobs:\r\n>  publish:\r\n>    if: \"!contains(github.event.head_commit.message, 'Update image automatically')\"\r\n> ```\r\n\r\nRe-check the `Application` in cluster, we can see that the image of the `my-app` has been updated after a while.\r\n\r\n> KubeVela polls the latest information from the code and image repo periodically (at an interval that can be customized):\r\n> * When the `Application` file in the Git repository is updated, KubeVela will update the `Application` in the cluster based on the latest configuration.\r\n> * When a new tag is added to the image registry, KubeVela will filter out the latest tag based on your policy and update it to Git repository. When the files in the repository are updated, KubeVela repeats the first step and updates the files in the cluster, thus achieving automatic deployment.\r\n\r\nWe can `curl` to `Ingress` to see the current version and data:\r\n\r\n```shell\r\n$ kubectl get ingress\r\nNAME        CLASS    HOSTS                 ADDRESS         PORTS   AGE\r\nmy-server   <none>   kubevela.example.com  <ingress-ip>    80      162m\r\n\r\n$ curl -H \"Host:kubevela.example.com\" http://<ingress-ip>\r\nVersion: 0.1.6\r\n\r\n$ curl -H \"Host:kubevela.example.com\" http://<ingress-ip>/db\r\nUser: KubeVela\r\nDescription: It's a test user\r\n\r\nUser: KubeVela2\r\nDescription: It's another test user\r\n```\r\n\r\nThe `Version` has been updated successfully! Now we're done with everything from changing the code to automatically applying to the cluster.\r\n\r\n## Summary\r\n\r\nFor platform admins/SREs, they update the config repo to operate the application and infrastructure. KubeVela will synchronize the config to the cluster, simplifying the deployment process.\r\n\r\nFor end users/developers, they write the source code, push it to Git, and then re-deployment will happen. It will make CI to build the image. KubeVela will then update the image field and apply the deployment config.\r\n\r\nBy integrating with GitOps, KubeVela helps users speed up deployment and simplify continuous deployment."
    },
    {
      "id": "/2021/10/08/blog-1.1",
      "metadata": {
        "permalink": "/blog/2021/10/08/blog-1.1",
        "editUrl": "https://github.com/kubevela/kubevela.io/tree/main/blog/2021-10-08-blog-1.1.md",
        "source": "@site/blog/2021-10-08-blog-1.1.md",
        "title": "KubeVela Releases 1.1, Reaching New Peaks in Cloud-Native Continuous Delivery",
        "description": "",
        "date": "2021-10-08T00:00:00.000Z",
        "formattedDate": "October 8, 2021",
        "tags": [
          {
            "label": "DevOps",
            "permalink": "/blog/tags/dev-ops"
          },
          {
            "label": "release-note",
            "permalink": "/blog/tags/release-note"
          }
        ],
        "readingTime": 6.16,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "KubeVela Maintainers",
            "title": "KubeVela Team",
            "url": "https://github.com/kubevela/kubevela",
            "imageURL": "https://kubevela.io/img/logo.svg"
          }
        ],
        "frontMatter": {
          "title": "KubeVela Releases 1.1, Reaching New Peaks in Cloud-Native Continuous Delivery",
          "author": "KubeVela Maintainers",
          "author_title": "KubeVela Team",
          "author_url": "https://github.com/kubevela/kubevela",
          "author_image_url": "https://kubevela.io/img/logo.svg",
          "tags": [
            "DevOps",
            "release-note"
          ],
          "description": "",
          "hide_table_of_contents": false
        },
        "prevItem": {
          "title": "Using GitOps + KubeVela for Application Continuous Delivery",
          "permalink": "/blog/2021/10/10/kubevela-gitops"
        },
        "nextItem": {
          "title": "Using Jenkins + KubeVela for Application Continuous Delivery",
          "permalink": "/blog/2021/09/02/kubevela-jenkins-cicd"
        }
      },
      "content": "## Overview\r\n\r\nInitialized by Alibaba and currently a CNCF sandbox project, KubeVela is a modern application platform that focues on modeling the delivery workflow of micro-services on top of Kubernetes, Terraform, Flux Helm controller and beyond. This brings strong value added to the existing GitOps and IaC primitives with battle tested application delivery practices including deployment pipeline, across-environment promotion, manual approval, canary rollout and notification, etc.\r\n\r\nThis is the first open source project in CNCF that focuses on the full lifecycle continuous delivery experience from abstraction, rendering, orchestration to deployment. This reminds us of Spinnaker, but designed to be simpler, cloud native, can be used with any CI pipeline and easily extended.\r\n\r\n<!--truncate-->\r\n\r\n## Introduction\r\n\r\nKubernetes has made it easy to build application deployment infrastructure, either on cloud, on-prem, or on IoT environments. But there are still two problems for developers to manage micro-service applications. First, developers just want to deploy, but delivering application with lower level infrastructure/orchestrator primitives is too much for them. It's very hard for developers to keep up with all these details and they need a simpler abstraction to \"just deploy\". Second, application delivery workflow is a basic need for \"just deploy\", but it is inherently out of scope of Kubernetes itself. The existing workflow addons/projects are too generic, they are way more than focusing on delivering applications only. These problems makes continuous delivery complex and unscalable even with the help of Kubernetes. GitOps can help in deploying phase, but lack the capabilities of abstracting, rendering, and orchestration. This results in low SDO (software delivery and operation) performance and burnout of DevOps engineers. In worst case, it could cause production outage if users make unsafe operations due to the complexity.\r\n\r\nThe latest DORA survey [1] shows that organizations adopting continuous delivery are more likely to have processes that are more high quality, low-risk, and cost-effective. Though the question is how we can make it more focused and easy to practice. Hence, KubeVela introduces Open Application Model (OAM), a higher level abstraction for modeling application delivery workflow with app-centric, consistent and declarative approach. This empowers developers to continuously verify and deploy their applications with confidence, standing on the shoulders of Kubernetes control theory, GitOps, IaC and beyond.\r\n\r\n![](https://static.kubevela.net/images/cicd.png) \r\n\r\nKubeVela latest 1.1 release is a major milestone bringing more continuous delivery features. It highlights:\r\n\r\n- **Multi-environment, multi-cluster rollout**: KubeVela allows users to define the environments and the clusters which application components to deploy to or to promote. This makes it easier for users to manage multi-stage application rollout. For example, users can deploy applicatons to test environment and then promote to production environment.\r\n- **Canary rollout and approval gate**: Application delivery is a procedural workflow that takes multiple steps. KubeVela provides such workflow on top of Kubernetes. By default, Users can use KubeVela to build canary rollout, approval gate, notification pipelines to deliver applications confidently. Moreover, the workflow model is declarative and extensible. Workflow steps can be stored in Git to simplify management.\r\n- **Addon management**: All KubeVela capabilities (e.g. Helm chart deployment) are pluggable. They are managed as addons [2]. KubeVela provides simple experience via CLI/UI to discover, install, uninstall addons. There is an community addon registry. Users can also bring their own addon registries.\r\n- **Cloud Resource**: Users can enable Terraform addon on KubeVela to deploy cloud resources using the same abstraction to deploy applications. This enables cooperative delivery of application and its dependencies. That includes databases, redis, message queues, etc. By using KubeVela, users don't need to switch over to another interface to manage middlewares. This provides unified experience and aligns better with the upcoming trends in CNCF Cooperative-Delivery Working Group [3].\r\n\r\nThat is the introduction about KubeVela 1.1 release. In the following, we will provide deep-dive and examples for the new features.\r\n\r\n## Multi-Environment, Multi-Cluster Rollout\r\n\r\nUsers would need to deploy applications across clusters in different regions. Additionally, users would have test environment to run some automated tests first before deploying to production environment. However, it remains mysterious for many users how to do multi-environment, multi-cluster application rollout on Kubernetes. \r\n\r\nKubeVela 1.1 introduces multi-environment, multi-cluster rollout. It integrates Open Cluster Management and Karmada projects to handle multi-cluster management. Based on that, it provides EnvBinding Policy to define per-environment config patch and placement decisions. Here is an example of EnvBinding policy:\r\n\r\n```yaml\r\n  policies:\r\n    - name: example-multi-env-policy\r\n      type: env-binding\r\n      properties:\r\n        envs:\r\n          - name: staging\r\n            placement: # selecting the cluster to deploy to\r\n              clusterSelector:\r\n                name: cluster-staging\r\n            selector: # selecting which component to use\r\n              components:\r\n                - hello-world-server\r\n\r\n          - name: prod\r\n            placement:\r\n              clusterSelector:\r\n                name: cluster-prod\r\n            patch: # overlay patch on above components\r\n              components:\r\n                - name: hello-world-server\r\n                  type: webservice\r\n                  traits:\r\n                    - type: scaler\r\n                      properties:\r\n                        replicas: 3\r\n```\r\n\r\nBelow is a demo for a multi-stage application rollout from Staging to Production. The local cluster serves as the control plane and the rest two are the runtime clusters.\r\n\r\n![](https://static.kubevela.net/images/deploy_cloud.gif)\r\n\r\nNote that all the resources and statuses are aggregated and abstracted in the KubeVela Applications. Did any problems happen, it will pinpoint the problematic resources for users. This results in faster recovery time and more manageable delivery.\r\n\r\n### Canary Rollout, Approval, Notification\r\n\r\nCan you build a canary rollout pipeline in 5 minutes? Ask Kubernetes users and they would tell you it is not even enough to learn an Istio concept. We belive that as a developer you do not need to master Istio to build a canary rollout pipeline. KubeVela abstracts away the low level details and provides a simple solution as follows.\r\n\r\nFirst, installing Istio is made easy via KubeVela addons:\r\n\r\n```bash\r\nvela addon enable istio\r\n```\r\n\r\nThen, users just need to define how many batches for the rollout:\r\n\r\n```yaml\r\ntraits:\r\n  - type: rollout\r\n    properties:\r\n      targetSize: 100\r\n      rolloutBatches:\r\n        - replicas: 10\r\n        - replicas: 90\r\n```\r\n\r\nFinally, define the workflow of canary, approval, and notification:\r\n\r\n```yaml\r\n  workflow:\r\n    steps:\r\n      - name: rollout-1st-batch\r\n        type: canary-rollout\r\n        properties:\r\n          # just upgrade first batch of component\r\n          batchPartition: 0\r\n          traffic:\r\n            weightedTargets:\r\n              - revision: reviews-v1\r\n                weight: 90 # 90% to the old version\r\n              - revision: reviews-v2\r\n                weight: 10 # 10% to the new version\r\n\r\n      - name: approval-gate\r\n        type: suspend\r\n\r\n      - name: rollout-rest\r\n        type: canary-rollout\r\n        properties:\r\n          batchPartition: 1\r\n          traffic:\r\n            weightedTargets:\r\n              - revision: reviews-v2\r\n                weight: 100 # 100% shift to new version\r\n\t\t\t\t\r\n      - name: send-msg\r\n        type: webhook-notification\r\n        properties:\r\n          slack:\r\n            url: <your slack webhook url>\r\n            text: \"rollout finished\"\r\n```\r\n\r\nHere is a full demo:\r\n\r\n![](https://static.kubevela.net/images/rollout.gif)\r\n\r\n## What Comes Next\r\n\r\nIn this KubeVela release we have built the cornerstone for continuous delivery on Kubernetes. For the upcoming release our major theme will be improving user experience. We will release a dashboard that takes the user experience to another level. Besides that, we will keep improving our CLI tools, debuggability, observability. This will ensure our users can self serve to not only deploy and manage applications, but also debug and analyze the delivery pipelines.\r\n\r\nFor more project roadmap information, please see Kubevela [RoadMap](https://kubevela.io/docs/roadmap/2021-12-roadmap).\r\n\r\n## Join the Community\r\n\r\nKubeVela is a community-driven, open-source project. Dozens of leading enterprises have adopted KubeVela in production, including Alibaba, Tencent, ByteDance, XPeng Motors. You are welcome to join the community. Here are next steps:\r\n\r\n- Read documentation and try it out: [kubevela.io](https://kubevela.io)​\r\n- Join Slack channel to address your questions: [CNCF #kubevela](https://cncf.slack.com/messages/kubevela/)\r\n- Welcome to Star/Watch/Fork the project and contribute: [github.com/kubevela/kubevela](https://github.com/kubevela/kubevela)​\r\n- If you're already a user, please register in [this issue](https://github.com/kubevela/kubevela/issues/1662) and motivate the community!\r\n\r\n## References\r\n\r\n(1) DORA full report: https://cloud.google.com/blog/products/devops-sre/announcing-dora-2021-accelerate-state-of-devops-report\r\n(2) KubeVela Addon: https://github.com/kubevela/catalog/tree/master/addons/example\r\n(3) Cooperative Delivery Charter: https://github.com/cncf/tag-app-delivery/blob/master/cooperative-delivery-wg/charter.md"
    },
    {
      "id": "/2021/09/02/kubevela-jenkins-cicd",
      "metadata": {
        "permalink": "/blog/2021/09/02/kubevela-jenkins-cicd",
        "editUrl": "https://github.com/kubevela/kubevela.io/tree/main/blog/2021-09-02-kubevela-jenkins-cicd.md",
        "source": "@site/blog/2021-09-02-kubevela-jenkins-cicd.md",
        "title": "Using Jenkins + KubeVela for Application Continuous Delivery",
        "description": "",
        "date": "2021-09-02T00:00:00.000Z",
        "formattedDate": "September 2, 2021",
        "tags": [
          {
            "label": "KubeVela",
            "permalink": "/blog/tags/kube-vela"
          },
          {
            "label": "use-case",
            "permalink": "/blog/tags/use-case"
          }
        ],
        "readingTime": 11.92,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Da Yin, Yang Song",
            "title": "KubeVela Team",
            "url": "https://github.com/kubevela/kubevela",
            "imageURL": "https://kubevela.io/img/logo.svg"
          }
        ],
        "frontMatter": {
          "title": "Using Jenkins + KubeVela for Application Continuous Delivery",
          "author": "Da Yin, Yang Song",
          "author_title": "KubeVela Team",
          "author_url": "https://github.com/kubevela/kubevela",
          "author_image_url": "https://kubevela.io/img/logo.svg",
          "tags": [
            "KubeVela",
            "use-case"
          ],
          "description": "",
          "image": "https://raw.githubusercontent.com/kubevela/kubevela.io/main/docs/resources/KubeVela-03.png",
          "hide_table_of_contents": false
        },
        "prevItem": {
          "title": "KubeVela Releases 1.1, Reaching New Peaks in Cloud-Native Continuous Delivery",
          "permalink": "/blog/2021/10/08/blog-1.1"
        },
        "nextItem": {
          "title": "KubeVela Performance Test - Managing Massive Applications",
          "permalink": "/blog/2021/08/30/kubevela-performance-test"
        }
      },
      "content": "KubeVela bridges the gap between applications and infrastructures, enabling easy delivery and management of development codes. Compared to Kubernetes objects, the Application in KubeVela better abstracts and simplifies the configurations which developers care about, and leave complex infrastruature capabilities and orchestration details to platform engineers. The KubeVela apiserver further exposes HTTP interfaces, which help developers to deploy applications even without Kubernetes cluster access.\r\n\r\nThis article will use Jenkins, a popular continuous integration tool, as basis and give a brief introduction to how to build GitOps-based application continuous delivery highway.\r\n\r\n<!--truncate-->\r\n\r\n## Continuous Delivery Highway\r\n\r\nAs application developer, you might care more about whether your application is functioning correctly and if development is convenient. There will be several system components on this highway to help you achieve that.\r\n1. First, you need a git repo to place program codes, test codes and a YAML file to declare your KubeVela application.\r\n2. Second, you also need a continuous integration tool to help you automate the integration test of codes, build container images and push images to image repo.\r\n3. Finally, you need to have a Kubernetes cluster and install KubeVela in it, with its apiserver function enabled.\r\n\r\n> Currently, the access management for KubeVela apiserver is under construction. You will need to configure apiserver access in later version of KubeVela (after v1.1).\r\n\r\nIn this article, we adopt GitHub as the git repo, Jenkins as the CI tool, DockerHub as the image repo. We use a simple HTTP Server written in Golang as example. The whole process of continuous delivery is shown as below. We can see that on this highway of continuous delivery, developers only need to care about application development and managing code version with Git. The highway will help developer run integration test and deploy applications into target Kubernetes cluster automatically.\r\n\r\n![arch](/img/jenkins-cicd/arch.png)\r\n\r\n## Set-up Environment\r\n\r\n### Jenkins\r\n\r\n> This article takes Jenkins as the CI tool. Developers can choose other CI tools like Travis or GitHub Action.\r\n\r\nFirst you need to set up Jenkins to deploy CI pipelines. The installation and initialization of Jenkins could refer to the [official docs](https://www.jenkins.io/doc/book/installing/linux/).\r\n\r\nNotice that since the CI pipeline in this example is based on Docker and GitHub, you need to install related plugins in Jenkins (*Dashboard > Manage Jenkins > Manage Plugins*), including Pipeline、HTTP Request Plugin、Docker Pipeline、Docker Plugin.\r\n\r\nBesides, you also need to configure Docker environment for Jenkins to use (*Dashboard > Manage Jenkins > Configure System > Docker Builder*). If Docker has already been installed, you can set Docker URL as `unix:///var/run/docker.sock`.\r\n\r\nSince the docker image will be pushed to image repo during the running of CI pipelines, you also need to store image repo accounts in Jenkins Credintial (*Dashboard > Manage Jenkins > Manage Credentials > Add Credentials*), such as DockerHub username and password.\r\n\r\n![jenkins-credential](/img/jenkins-cicd/jenkins-credential.png)\r\n\r\n### GitHub\r\n\r\n> This example uses GitHub as git repo. Developer can change it to other repos on demand, such as Gitlab.\r\n\r\nTo enable Jenkins to retrieve GitHub updates and write pipeline status back to GitHub, you need to execute the following two steps in GitHub. \r\n\r\n1. [Configure](https://github.com/settings/tokens/new) Personal Access Token. Notice to check `repo:status` to get the permission for writing commit status.\r\n\r\n![github-pat](/img/jenkins-cicd/github-pat.png)\r\n\r\nThen fill Personal Access Token from GitHub in Jenkins Credential (with Secret Text type).\r\n\r\n![jenkins-secret-text](/img/jenkins-cicd/jenkins-secret-text.png)\r\n\r\nFinally, go to *Dashboard > Manage Jenkins > Configure System > GitHub* in Jenkins and click Add *GitHub Server* to fill the newly created credential in. You can click *Test connection* to check if the configuration is correct.\r\n\r\n![jenkins-github](/img/jenkins-cicd/jenkins-github.png)\r\n\r\n2. Add Webhook to GitHub code repo settings. Fill Jenkins Webhook address into it. For example, http://my-jenkins.example.com/github-webhook/ . In this way, all Push events in this code repo will be pushed to Jenkins.\r\n\r\n![github-webhook](/img/jenkins-cicd/github-webhook.png)\r\n\r\n### KubeVela\r\n\r\nYou need to install KubeVela in your Kubernetes cluster and enable the apiserver function. Refer to [official doc](/docs/platform-engineers/advanced-install#install-kubevela-with-apiserver) for details.\r\n\r\n## Composing Applications\r\n\r\nWe use a simple HTTP Server as example. Here, we declare a constant named `VERSION` and print it when accessing the HTTP service. A simple test is also set up, which can be used to validate the format of `VERSION`.\r\n```go\r\n// main.go\r\n\r\npackage main\r\n\r\nimport (\r\n\t\"fmt\"\r\n\t\"net/http\"\r\n)\r\n\r\nconst VERSION = \"0.1.0-v1alpha1\"\r\n\r\nfunc main() {\r\n\thttp.HandleFunc(\"/\", func(w http.ResponseWriter, r *http.Request) {\r\n\t\t_, _ = fmt.Fprintf(w, \"Version: %s\\n\", VERSION)\r\n\t})\r\n\tif err := http.ListenAndServe(\":8088\", nil); err != nil {\r\n\t\tprintln(err.Error())\r\n\t}\r\n}\r\n```\r\n```go\r\n// main_test.go\r\n\r\npackage main\r\n\r\nimport (\r\n\t\"regexp\"\r\n\t\"testing\"\r\n)\r\n\r\nconst verRegex string = `^v?([0-9]+)(\\.[0-9]+)?(\\.[0-9]+)?` +\r\n\t`(-([0-9A-Za-z\\-]+(\\.[0-9A-Za-z\\-]+)*))?` +\r\n\t`(\\+([0-9A-Za-z\\-]+(\\.[0-9A-Za-z\\-]+)*))?$`\r\n\r\nfunc TestVersion(t *testing.T) {\r\n\tif ok, _ := regexp.MatchString(verRegex, VERSION); !ok {\r\n\t\tt.Fatalf(\"invalid version: %s\", VERSION)\r\n\t}\r\n}\r\n```\r\n\r\nTo build container image for the HTTP server and publishing it as KubeVela Application into Kubernetes, we also need another two files `Dockerfile` and `app.yaml` in the code repo. They are used to describe how container image is built and configure the KubeVela Application respectively.\r\n\r\n```Dockerfile\r\n# Dockerfile\r\nFROM golang:1.13-rc-alpine3.10 as builder\r\nWORKDIR /app\r\nCOPY main.go .\r\nRUN go build -o kubevela-demo-cicd-app main.go\r\n\r\nFROM alpine:3.10\r\nWORKDIR /app\r\nCOPY --from=builder /app/kubevela-demo-cicd-app /app/kubevela-demo-cicd-app\r\nENTRYPOINT ./kubevela-demo-cicd-app\r\nEXPOSE 8088\r\n```\r\n\r\nIn `app.yaml`, we declare the application should contain 5 replica and expose the service through `Ingress`. The `labels` trait is used to tag Application Pods with current git commit id. Then the delivery pipeline in Jenkins will inject GIT_COMMIT into it and submit the Application configuration to KubeVela apiserver. Then the updates for Application will be triggered. The application will update 2 replica first, then hang and wait for manual approve. After developer confirms the change is valid, the rest 3 replica will be updated. This canary release is configured by the `rollout` trait declared in the Application.\r\n\r\n```yaml\r\n# app.yaml\r\napiVersion: core.oam.dev/v1beta1\r\nkind: Application\r\nmetadata:\r\n  name: kubevela-demo-app\r\nspec:\r\n  components:\r\n    - name: kubevela-demo-app-web\r\n      type: webservice\r\n      properties:\r\n        image: somefive/kubevela-demo-cicd-app\r\n        imagePullPolicy: Always\r\n        port: 8080\r\n      traits:\r\n        - type: rollout\r\n          properties:\r\n            rolloutBatches:\r\n              - replicas: 2\r\n              - replicas: 3\r\n            batchPartition: 0\r\n            targetSize: 5\r\n        - type: labels\r\n          properties:\r\n            jenkins-build-commit: GIT_COMMIT\r\n        - type: ingress\r\n          properties:\r\n            domain: kubevela-demo-cicd-app.cf7c0ed25b151437ebe1ef58efc29bca4.us-west-1.alicontainer.com\r\n            http:\r\n              \"/\": 8088\r\n```\r\n\r\n## Configure CI pipelines\r\n\r\nIn this article, we set up two pipelines in Jenkins. One is the test pipeline, which is for running tests for application codes. The other one is the delivery pipeline, which builds container images and uploads them to image repo. Then the application configuration will be updated.\r\n\r\n### Test Pipeline\r\nCreate a new pipeline in Jenkins. Set *Build Triggers* as *GitHub hook trigger for GITScm polling*.\r\n\r\n![test-pipeline-create](/img/jenkins-cicd/test-pipeline-create.png)\r\n![test-pipeline-config](/img/jenkins-cicd/test-pipeline-config.png)\r\n\r\nThis pipeline uses golang image as execution environment at first. Next, it checkouts the `dev` branch of the target GitHub repo, indicating that this pipeline will be triggered by push events to `dev` branch. The piepline status will be written back to GitHub after execution finished.\r\n\r\n```groovy\r\nvoid setBuildStatus(String message, String state) {\r\n  step([\r\n      $class: \"GitHubCommitStatusSetter\",\r\n      reposSource: [$class: \"ManuallyEnteredRepositorySource\", url: \"https://github.com/Somefive/KubeVela-demo-CICD-app\"],\r\n      contextSource: [$class: \"ManuallyEnteredCommitContextSource\", context: \"ci/jenkins/test-status\"],\r\n      errorHandlers: [[$class: \"ChangingBuildStatusErrorHandler\", result: \"UNSTABLE\"]],\r\n      statusResultSource: [ $class: \"ConditionalStatusResultSource\", results: [[$class: \"AnyBuildResult\", message: message, state: state]] ]\r\n  ]);\r\n}\r\npipeline {\r\n    agent {\r\n        docker { image 'golang:1.13-rc-alpine3.10' }\r\n    }\r\n    stages {\r\n        stage('Prepare') {\r\n            steps {\r\n                script {\r\n                    def checkout = git branch: 'dev', url: 'https://github.com/Somefive/KubeVela-demo-CICD-app.git'\r\n                    env.GIT_COMMIT = checkout.GIT_COMMIT\r\n                    env.GIT_BRANCH = checkout.GIT_BRANCH\r\n                    echo \"env.GIT_BRANCH=${env.GIT_BRANCH},env.GIT_COMMIT=${env.GIT_COMMIT}\"\r\n                }\r\n                setBuildStatus(\"Test running\", \"PENDING\");\r\n            }\r\n        }\r\n        stage('Test') {\r\n            steps {\r\n                sh 'CGO_ENABLED=0 GOCACHE=$(pwd)/.cache go test *.go'\r\n            }\r\n        }\r\n    }\r\n    post {\r\n        success {\r\n            setBuildStatus(\"Test success\", \"SUCCESS\");\r\n        }\r\n        failure {\r\n            setBuildStatus(\"Test failed\", \"FAILURE\");\r\n        }\r\n    }\r\n}\r\n```\r\n\r\n### Delivery Pipeline\r\n\r\nThe delivery pipeline, similar to the test pipeline, first pulls codes in `prod` branch of the git repo. Then use Docker to build images and push it to remote image repo. (Here we use DockerHub, the *withRegistry* function takes image repo location and the Credential ID of the repo as parameters). After image been built, the pipeline converts Application YAML file into JSON file, with GIT_COMMIT injected. Finally, the pipeline sends POST requests to KubeVela apiserver (here is http://47.88.24.19/) for creating or updating target application.\r\n\r\n> Currently, KubeVela apiserver takes JSON object as inputs. Therefore we do extra conversion in the delivery pipeline. In the future, the KubeVela apiserver will further improve and simplify this interaction process. The admission management will be added as well to address the security issue.\r\n\r\n> In this case we will create an application named *cicd-demo-app* in Namespace *kubevela-demo-namespace*. Notice that the Namespace need to be created in Kubernetes in advance. KubeVela apiserver will simplify it in later version.\r\n\r\n```groovy\r\nvoid setBuildStatus(String message, String state) {\r\n  step([\r\n      $class: \"GitHubCommitStatusSetter\",\r\n      reposSource: [$class: \"ManuallyEnteredRepositorySource\", url: \"https://github.com/Somefive/KubeVela-demo-CICD-app\"],\r\n      contextSource: [$class: \"ManuallyEnteredCommitContextSource\", context: \"ci/jenkins/deploy-status\"],\r\n      errorHandlers: [[$class: \"ChangingBuildStatusErrorHandler\", result: \"UNSTABLE\"]],\r\n      statusResultSource: [ $class: \"ConditionalStatusResultSource\", results: [[$class: \"AnyBuildResult\", message: message, state: state]] ]\r\n  ]);\r\n}\r\npipeline {\r\n    agent any\r\n    stages {\r\n        stage('Prepare') {\r\n            steps {\r\n                script {\r\n                    def checkout = git branch: 'prod', url: 'https://github.com/Somefive/KubeVela-demo-CICD-app.git'\r\n                    env.GIT_COMMIT = checkout.GIT_COMMIT\r\n                    env.GIT_BRANCH = checkout.GIT_BRANCH\r\n                    echo \"env.GIT_BRANCH=${env.GIT_BRANCH},env.GIT_COMMIT=${env.GIT_COMMIT}\"\r\n                    setBuildStatus(\"Deploy running\", \"PENDING\");\r\n                }\r\n            }\r\n        }\r\n        stage('Build') {\r\n            steps {\r\n                script {\r\n                    docker.withRegistry(\"https://registry.hub.docker.com\", \"DockerHubCredential\") {\r\n                        def customImage = docker.build(\"somefive/kubevela-demo-cicd-app\")\r\n                        customImage.push()\r\n                    }\r\n                }\r\n            }\r\n        }\r\n        stage('Deploy') {\r\n            steps {\r\n                sh 'wget -q \"https://github.com/mikefarah/yq/releases/download/v4.12.1/yq_linux_amd64\"'\r\n                sh 'chmod +x yq_linux_amd64'\r\n                script {\r\n                    def app = sh (\r\n                        script: \"./yq_linux_amd64 eval -o=json '.spec' app.yaml | sed -e 's/GIT_COMMIT/$GIT_COMMIT/g'\",\r\n                        returnStdout: true\r\n                    )\r\n                    echo \"app: ${app}\"\r\n                    def response = httpRequest acceptType: 'APPLICATION_JSON', contentType: 'APPLICATION_JSON', httpMode: 'POST', requestBody: app, url: \"http://47.88.24.19/v1/namespaces/kubevela-demo-namespace/applications/cicd-demo-app\"\r\n                    println('Status: '+response.status)\r\n                    println('Response: '+response.content)\r\n                }\r\n            }\r\n        }\r\n    }\r\n    post {\r\n        success {\r\n            setBuildStatus(\"Deploy success\", \"SUCCESS\");\r\n        }\r\n        failure {\r\n            setBuildStatus(\"Deploy failed\", \"FAILURE\");\r\n        }\r\n    }\r\n}\r\n```\r\n\r\n> NOTE: the deploy stage is written with KubeVela v1.1. The apiserver interaction method is updated in KubeVela v1.2, leveraging VelaUX (the UI dashboard) and webhook trigger. If you are using KubeVela v1.2.0+, you should refer to the latest documents.\r\n\r\n## Performance\r\n\r\nAfter finishing the configuration process described above, the whole process of continuous delivery has already been set up. Let's check how it works.\r\n\r\n![pipeline-overview](/img/jenkins-cicd/pipeline-overview.png)\r\n\r\nFirst, we set the `VERSION` constant in `main.go` to `Bad Version Number`, aka,\r\n```go\r\nconst VERSION = \"Bad Version Number\"\r\n```\r\nThen, we submit this change to `dev` branch. We can see that the test pipeline in Jenkins is triggered and the failure status is written back to GitHub.\r\n\r\n![test-pipeline-fail](/img/jenkins-cicd/test-pipeline-fail.png)\r\n![test-github-fail](/img/jenkins-cicd/test-github-fail.png)\r\n\r\nWe edit the `VERSION` to `0.1.1` again and resubmit it. Now we see that the test pipeline is successfully executed, with the commit in GitHub marked as succeeded.\r\n\r\n![test-pipeline-success](/img/jenkins-cicd/test-pipeline-success.png)\r\n![test-github-success](/img/jenkins-cicd/test-github-success.png)\r\n\r\nThen we issue a Pull Request to merge `dev` branch into `prod` branch.\r\n\r\n![pull-request](/img/jenkins-cicd/pull-request.png)\r\n\r\nThe Jenkins delivery pipeline is triggered once the Pull Request is accepted. After execution finished, the latest commit in prod branch is also marked as succeeded.\r\n\r\n![deploy-pipeline-success](/img/jenkins-cicd/deploy-pipeline-success.png)\r\n![deploy-github-success](/img/jenkins-cicd/deploy-github-success.png)\r\n\r\n```bash\r\n$ kubectl get app -n kubevela-demo-namespace\r\nNAME                     COMPONENT               TYPE         PHASE     HEALTHY   STATUS   AGE\r\nkubevela-demo-cicd-app   kubevela-demo-app-web   webservice   running   true               112s\r\n$ kubectl get deployment -n kubevela-demo-namespace\r\nNAME                       READY   UP-TO-DATE   AVAILABLE   AGE\r\nkubevela-demo-app-web-v1   2/2     2            2           2m1s\r\n```\r\n\r\nAs shown above, the target application is successfully accepted by KubeVela apiserver and related resources are created by KubeVela controller. The current replica number of Deployment is 2. After deleting `batchPartition : 0` in the `rollout` trait of the application, which means confirming current release, the Deployment replica is updated to 5. Now we can access the domain configured in Ingress and get the current version number.\r\n\r\n```bash\r\n$ kubectl edit app -n kubevela-demo-namespace\r\napplication.core.oam.dev/kubevela-demo-cicd-app edited\r\n$ kubectl get deployment -n kubevela-demo-namespace -w\r\nNAME                       READY   UP-TO-DATE   AVAILABLE   AGE\r\nkubevela-demo-app-web-v1   4/5     5            4           3m39s\r\nkubevela-demo-app-web-v1   5/5     5            5           3m39s\r\nkubevela-demo-app-web-v1   5/5     5            5           3m40s\r\nkubevela-demo-app-web-v1   5/5     5            5           3m40s\r\n$ curl http://kubevela-demo-cicd-app.cf7c0ed25b151437ebe1ef58efc29bca4.us-west-1.alicontainer.com/\r\nVersion: 0.1.1\r\n```\r\n\r\nRepeat the steps above. Upgrade the version number to `0.1.2`. Finish both test pipeline and delivery pipeline. Then we will see there is a version change to the Deployment managed by the target application. The replica number of the old Deployment decreases from 5 to 3 while the new one contains 2 replica at this moment. If we access the service now, we will find sometimes the old version number is returned and sometimes the new version number is displayed. This is because when rolling update the application, both new version replica and old version replica exist. The incoming traffic will be dispatched to different version replica. Therefore we can observe two different version at the same time. \r\n\r\n```bash\r\n$ kubectl get deployment -n kubevela-demo-namespace -w\r\nNAME                       READY   UP-TO-DATE   AVAILABLE   AGE\r\nkubevela-demo-app-web-v1   5/5     5            5           11m\r\nkubevela-demo-app-web-v2   0/0     0            0           0s\r\nkubevela-demo-app-web-v2   0/0     0            0           0s\r\nkubevela-demo-app-web-v2   0/0     0            0           0s\r\nkubevela-demo-app-web-v1   5/5     5            5           12m\r\nkubevela-demo-app-web-v2   0/0     0            0           0s\r\nkubevela-demo-app-web-v2   0/0     0            0           0s\r\nkubevela-demo-app-web-v2   0/0     0            0           0s\r\nkubevela-demo-app-web-v2   0/0     0            0           0s\r\nkubevela-demo-app-web-v2   0/2     0            0           0s\r\nkubevela-demo-app-web-v2   0/2     0            0           0s\r\nkubevela-demo-app-web-v2   0/2     0            0           0s\r\nkubevela-demo-app-web-v2   0/2     2            0           0s\r\nkubevela-demo-app-web-v1   5/5     5            5           12m\r\nkubevela-demo-app-web-v2   1/2     2            1           2s\r\nkubevela-demo-app-web-v2   2/2     2            2           2s\r\nkubevela-demo-app-web-v1   5/3     5            5           13m\r\nkubevela-demo-app-web-v1   5/3     5            5           13m\r\nkubevela-demo-app-web-v1   3/3     3            3           13m\r\n```\r\n```bash\r\n$ curl http://kubevela-demo-cicd-app.cf7c0ed25b151437ebe1ef58efc29bca4.us-west-1.alicontainer.com/\r\nVersion: 0.1.1\r\n$ curl http://kubevela-demo-cicd-app.cf7c0ed25b151437ebe1ef58efc29bca4.us-west-1.alicontainer.com/\r\nVersion: 0.1.1\r\n$ curl http://kubevela-demo-cicd-app.cf7c0ed25b151437ebe1ef58efc29bca4.us-west-1.alicontainer.com/\r\nVersion: 0.1.1\r\n$ curl http://kubevela-demo-cicd-app.cf7c0ed25b151437ebe1ef58efc29bca4.us-west-1.alicontainer.com/\r\nVersion: 0.1.1\r\n$ curl http://kubevela-demo-cicd-app.cf7c0ed25b151437ebe1ef58efc29bca4.us-west-1.alicontainer.com/\r\nVersion: 0.1.2\r\n$ curl http://kubevela-demo-cicd-app.cf7c0ed25b151437ebe1ef58efc29bca4.us-west-1.alicontainer.com/\r\nVersion: 0.1.2\r\n$ curl http://kubevela-demo-cicd-app.cf7c0ed25b151437ebe1ef58efc29bca4.us-west-1.alicontainer.com/\r\nVersion: 0.1.2\r\n$ curl http://kubevela-demo-cicd-app.cf7c0ed25b151437ebe1ef58efc29bca4.us-west-1.alicontainer.com/\r\nVersion: 0.1.2\r\n$ curl http://kubevela-demo-cicd-app.cf7c0ed25b151437ebe1ef58efc29bca4.us-west-1.alicontainer.com/\r\nVersion: 0.1.1\r\n```\r\n\r\nAfter confirming new services are functioning correctly, we can remove the `batchPartition: 0` as described above to complete the whole canary release process.\r\n\r\n```bash\r\n$ kubectl get deployment -n kubevela-demo-namespace -w\r\nNAME                       READY   UP-TO-DATE   AVAILABLE   AGE\r\nkubevela-demo-app-web-v1   3/3     3            3           18m\r\nkubevela-demo-app-web-v2   2/2     2            2           5m24s\r\nkubevela-demo-app-web-v2   2/5     2            2           5m36s\r\nkubevela-demo-app-web-v2   2/5     2            2           5m37s\r\nkubevela-demo-app-web-v2   2/5     2            2           5m37s\r\nkubevela-demo-app-web-v2   2/5     5            2           5m37s\r\nkubevela-demo-app-web-v2   3/5     5            3           5m38s\r\nkubevela-demo-app-web-v2   4/5     5            4           5m38s\r\nkubevela-demo-app-web-v2   5/5     5            5           5m39s\r\nkubevela-demo-app-web-v1   3/0     3            3           18m\r\nkubevela-demo-app-web-v1   3/0     3            3           18m\r\nkubevela-demo-app-web-v1   0/0     0            0           18m\r\nkubevela-demo-app-web-v1   0/0     0            0           18m\r\nkubevela-demo-app-web-v2   5/5     5            5           5m41s\r\nkubevela-demo-app-web-v2   5/5     5            5           5m41s\r\nkubevela-demo-app-web-v1   0/0     0            0           18m\r\n```\r\n\r\n## Conclusion\r\n\r\nIn summary, we executed the whole continuous delivery process successfully. In this process, developers can easily update and deploy their applications, with the help of KubeVela and Jenkins. Besides, developers can use their favourite tools in different stages, such as substituting GitHub with Gitlab, or using TravisCI instead of Jenkins.\r\n\r\nReaders might also notice that this progress can not only upgrade the application service, but also change deployment plan via editing `app.yaml`, such as scaling up or adding sidecars, which works like classical push-style GitOps. About more KubeVela GitOps content, you can refer to other related case studies."
    },
    {
      "id": "/2021/08/30/kubevela-performance-test",
      "metadata": {
        "permalink": "/blog/2021/08/30/kubevela-performance-test",
        "editUrl": "https://github.com/kubevela/kubevela.io/tree/main/blog/2021-08-30-kubevela-performance-test.md",
        "source": "@site/blog/2021-08-30-kubevela-performance-test.md",
        "title": "KubeVela Performance Test - Managing Massive Applications",
        "description": "KubeVela is demonstrated to be able to host thousands of applications effectively with limited resources.",
        "date": "2021-08-30T00:00:00.000Z",
        "formattedDate": "August 30, 2021",
        "tags": [
          {
            "label": "KubeVela",
            "permalink": "/blog/tags/kube-vela"
          }
        ],
        "readingTime": 8.06,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Da Yin, Yang Song, Zhengxi Zhou and Jianbo Sun",
            "title": "KubeVela Team",
            "url": "https://github.com/kubevela/kubevela",
            "imageURL": "https://kubevela.io/img/logo.svg"
          }
        ],
        "frontMatter": {
          "title": "KubeVela Performance Test - Managing Massive Applications",
          "author": "Da Yin, Yang Song, Zhengxi Zhou and Jianbo Sun",
          "author_title": "KubeVela Team",
          "author_url": "https://github.com/kubevela/kubevela",
          "author_image_url": "https://kubevela.io/img/logo.svg",
          "tags": [
            "KubeVela"
          ],
          "description": "KubeVela is demonstrated to be able to host thousands of applications effectively with limited resources.",
          "image": "https://raw.githubusercontent.com/kubevela/kubevela.io/main/docs/resources/KubeVela-03.png",
          "hide_table_of_contents": false
        },
        "prevItem": {
          "title": "Using Jenkins + KubeVela for Application Continuous Delivery",
          "permalink": "/blog/2021/09/02/kubevela-jenkins-cicd"
        },
        "nextItem": {
          "title": "KubeVela - The Extensible App Platform Based on Open Application Model and Kubernetes",
          "permalink": "/blog/2020/12/7/kubevela-the-extensible-app-platform-based-on-open-application-model-and-kubernetes"
        }
      },
      "content": "As an application management and integration platform, KubeVela needs to handle thousands of applications in production scenario. To evaluate the performance of KubeVela, develop team has conducted performance tests based on simultated environments and demonstrated the capability of managing a large number of applications concurrently.\r\n\r\n<!--truncate-->\r\n\r\n## Setup\r\n\r\n### Cluster Environment\r\nWorking with large clusters requires lots of resources, such as Machines, Network Bandwidth, Storages and many other devices. Therefore, KubeVela team adopts kubemark, an official tool provided by kubernetes, to simulate large clusters by mocking hundreds of kubelets. Each kubelet works like a real node except that they do not run real containers inside pods. The aim of KubeVela performance test mainly focus on whether KubeVela controller can manage thousands of applications effectively, instead of pulling images or executing commands inside pods. As a result, we only need to get resources hosting these fake nodes, also named as hollow-nodes.\r\n\r\nWe set up the Kubernetes clusters on Alibaba Cloud which includes 5 master nodes and 15 worker nodes. The master nodes wil host Kubernetes core components such as kube-apiserver and kube-controller-manager. The worker nodes need to run other pressure-test related componets, including monitoring tools, KubeVela controller and kubemark pods. Since the major target is to test the performance of KubeVela controller, we do not expect other components to be the bottleneck of the pressure test. To this end, both master nodes and worker nodes are equiped with 32 cores and 128 Gi memory. We use the combination of Prometheus, Loki and Grafana as the monitoring suites and grant them enough resources in avoid of crash caused by Out-of-Memory.\r\n\r\nNotice that KubeVela controller and monitoring tools need to be placed on real nodes to function while all pods created during performance tests should be assigned to hollow-nodes correctly. To achieve that, we give different taints to hollow-nodes and real nodes, and add corresponding tolerations to different pods.\r\n\r\n### Application\r\nTo simulate real applications in production, we design an application template with 2 components and 5 functional traits, including\r\n- 1 webservice component\r\n  - a scaler trait setting its replica to 3\r\n  - a sidecar trait attaching another container to each pod\r\n  - an ingress trait generating one ingress instance and one service instance\r\n- 1 worker component\r\n  - a scaler trait also setting its replica to 3\r\n  - a configmap trait generating a new configmap and attaching it to worker pods\r\n\r\nIn the following experiment, we test the performance of KubeVela controller managing 3,000 Applications (12,000 Pods in total) on 200 nodes. Applications are created in parallel at first, then kept running for a while, and finally deleted from the cluster. Each application will be reconciled multiple times with latencies and consumed resources recorded by monitoring tools.\r\n\r\n> In practice, we also have another trait used for adding tolerations as described above.\r\n\r\n### KubeVela Controller\r\nThe KubeVela controller is set up with a group of recommendation configurations as follows\r\n- Kubernetes Resource\r\n  - 0.5 core CPU\r\n  - 1 Gi Memory\r\n  - 1 replica\r\n- Program \r\n  - concurrent-reconciles=2 (The number of reconcile threads)\r\n  - kube-api-qps=300 (The qps of kubernetes client used in controller)\r\n  - kube-api-burst=500 (The burst of kubernetes client used in controller)\r\n  - informer-re-sync-interval=20m (The interval of routine reconciles.)\r\nWe will analyze these settings in the below sections.\r\n\r\n> To evaluate the performance of KubeVela Controller itself, we disabled the Ingress MutatingWebhook and Application ValidatingWebhook which is beyond the focus of this test but will affect the performance of KubeVela Controller by increasing the latency of creating/patching resources.\r\n\r\n## Experiments\r\n\r\n### Creation\r\nThe creation of all 3,000 applications lasted 25min. Getting all pods running takes a bit longer time, which is out of the scope of KubeVela controller. \r\n\r\nFor each application creation, it will trigger three turns of reconciling. The usage of CPU will reach 100% in the late period of creation. The memory usage will increase as the number of applications rises. It reaches around 67% at the end of creation.\r\n\r\n![create-cpu](/img/pressure-test/create-cpu.png)\r\n![create-memory](/img/pressure-test/create-memory.png)\r\n\r\nThe average time of the first turn reconciling is relatively short since it only needs patch finalizer. The second and third turn reconciling contain full reconcile cycles and need more time to process. The following charts record the time consumptions of different period during reconciling applications. The average time is generally below 200ms while 99% of reconciles uses less than 800ms.\r\n\r\n![create-avg-time](/img/pressure-test/create-avg-time.png)\r\n![create-p99-time](/img/pressure-test/create-p99-time.png)\r\n\r\n### Regular Reconciles\r\nAfter creation, applications are reconciled by controller every 20min. the monitoring of 8-hour reconcile process are displayed as below. The CPU usage will come up to 90% once reconcile happens routinely. The memory usage generally keeps a stable pattern, up to 75% memory usage.\r\n\r\n![med-cpu](/img/pressure-test/med-cpu.png)\r\n![med-memory](/img/pressure-test/med-memory.png)\r\n\r\nThe average reconcile time is under 200ms while 99% are about 800ms~900ms. Each regular reconcile for all applications generally takes around 10min.\r\n\r\n![med-avg](/img/pressure-test/med-avg.png)\r\n![med-p99](/img/pressure-test/med-p99.png)\r\n\r\n### Deletion\r\nThe application deletion process is fast and low-resource consumptive. It takes less than 3min to delete all applications. However, notice that the deletion of resources managed by application usually takes longer time. This is because the cleanup of these resources (such as deployments or pods) are not directly controlled by the KubeVela controller. KubeVela controller takes charge of deleting their owner and cleanup them by triggering cascading deletion. In addition, each deletion is associated with two turns of reconcile where the second turn returns immediately when it fails to retrieve target application (since it is deleted).\r\n\r\n![del-cpu](/img/pressure-test/del-cpu.png)\r\n![del-memory](/img/pressure-test/del-memory.png)\r\n![del-avg](/img/pressure-test/del-avg.png)\r\n![del-p99](/img/pressure-test/del-p99.png)\r\n\r\n\r\n## Analysis\r\n\r\n### Number of Applications\r\nThe experiment displayed above demonstrates a classical scenario for KubeVela. Although 3,000 applications are successfully managed by the KubeVela controller in this case, it is strongly recommended to adopt a smaller number (such as 2,000) of applications with the above configuration for the following reasons:\r\nThe time and resource consumption is closely associated with the spec of applications. If a lot of users apply larger applications with more pods and more other resources, 3,000 applications might break the resource limit more easily.\r\nThe memory and CPU usage shown above is approching resource limits. If memory drains, the KubeVela controller will crash and restart. If high CPU usage maintains for a long time, it might cause a long waiting queue in KubeVela controller which further lead to longer response time for application changes.\r\n\r\n### Configurations\r\nThere are several parameters users could config to adapt into their own scenario.\r\nUsing more replica for KubeVela controller do not scale up the ability of KubeVela controller. The leader election mechanism ensures that only one replica will work while others will wait. The aim of multi-replica is to support fast recovery when the working one crash. However, if the crash is caused by OOM, the recovery usually will not be able to fix that.\r\nThe number of qps and burst in the program configuration should be increased accordingly while scaling up KubeVela controller. These two parameters limit the capability for controller to send requests to apiserver.\r\nGenerally, in order to scale up KubeVela controller, scale up the resource limits and all the program parameters mentioned above (except the reconcile interval). If you have more applications to manage, add more memory. If you have higher operation frequency, add more CPU and threads, then increase qps and burst accordingly.\r\nLonger reconcile interval allows more applications to handle, at the cost of longer time to fix potential underlying resource problems. For example, if one deployment managed by one application disappears, the routine reconciling can discover this problem and fix it.\r\n\r\n### Scaling Up\r\nIn addition to the experiment described above, we conducted another two experiment to test how well KubeVela controller can scale to larger clusters and more applications.\r\n\r\nIn a 500-node cluster, we tried to create 5,000 applications with the same spec described above, at the speed of 4 per second and lasted for around 21min. 1 core and 2 Gi memory are granted to KubeVela controller with the use of 4 concurrent reconcile threads. The kube-api-qps and kube-api-burst are raised to 500/800 correspondingly. All 30,000 pods successfully turn into Running phase. The time costs for each reconcile is similar to the previous experiment and CPU/Memory cost is not very high compared to the given resources. The regular reconciles for 5,000 applications takes 7~8 minutes, and no significant resource cost is observed. During this scaling, we found that the throughput of kube-apiserver starts to block the creation of application, as too many resources need to be created while applying applications.\r\n\r\n![std-cpu](/img/pressure-test/std-cpu.png)\r\n![std-memory](/img/pressure-test/std-memory.png)\r\n\r\nScaling up to 12,000 applications on 1,000 nodes is much harder than previous attempts. With the same creation speed, the apiserver will be flooded by lots of pod scheduling requests and finally start to drop application creation request. To overcome this difficulty, we divide the creation process of applications into several stage. Each stage only create 1,000~3,000 applications and the next stage will not begin until all pods are ready. With this strategy, we successfully create 12,000 applications, 24,000 deployments, 12,000 services, 12,000 ingress, 12,000 configmaps and 72,000 pods. The whole process takes about 30 hours. To hold this number of applications, KubeVela controller consumes 1.7 cores and 2.45 Gi memory. It takes about 12min to finish a full turn of regular reconciles for all 12,000 applications.\r\n\r\n![large-cpu](/img/pressure-test/large-cpu.png)\r\n![large-memory](/img/pressure-test/large-memory.png)\r\n![large-all](/img/pressure-test/large-all.png)\r\n\r\n### Future\r\nThe performance test of KubeVela demonstrates its ability of managing thousands of applications with limited resource consumption. It can also scale up to over 10,000 applications on large clusters with 1,000 nodes. In addition, KubeVela team also conducted similar pressure test for non-deployment based applications such as CloneSet in OpenKruise (which is not enclosed in this report) and reach same conclusions. In the future, we will add more performance tests for more complex scenario like Workflow or MultiCluster."
    },
    {
      "id": "/2020/12/7/kubevela-the-extensible-app-platform-based-on-open-application-model-and-kubernetes",
      "metadata": {
        "permalink": "/blog/2020/12/7/kubevela-the-extensible-app-platform-based-on-open-application-model-and-kubernetes",
        "editUrl": "https://github.com/kubevela/kubevela.io/tree/main/blog/2020-12-7-kubevela-the-extensible-app-platform-based-on-open-application-model-and-kubernetes.md",
        "source": "@site/blog/2020-12-7-kubevela-the-extensible-app-platform-based-on-open-application-model-and-kubernetes.md",
        "title": "KubeVela - The Extensible App Platform Based on Open Application Model and Kubernetes",
        "description": "The Extensible App Platform Based on Open Application Model and Kubernetes",
        "date": "2020-12-07T00:00:00.000Z",
        "formattedDate": "December 7, 2020",
        "tags": [
          {
            "label": "KubeVela",
            "permalink": "/blog/tags/kube-vela"
          },
          {
            "label": "release-note",
            "permalink": "/blog/tags/release-note"
          }
        ],
        "readingTime": 7.19,
        "hasTruncateMarker": true,
        "authors": [
          {
            "name": "Lei Zhang and Fei Guo",
            "title": "CNCF TOC Member/Kubernetes",
            "url": "https://github.com/JoelMarcey",
            "imageURL": "https://avatars.githubusercontent.com/u/1701782?s=200&v=4"
          }
        ],
        "frontMatter": {
          "title": "KubeVela - The Extensible App Platform Based on Open Application Model and Kubernetes",
          "author": "Lei Zhang and Fei Guo",
          "author_title": "CNCF TOC Member/Kubernetes",
          "author_url": "https://github.com/JoelMarcey",
          "author_image_url": "https://avatars.githubusercontent.com/u/1701782?s=200&v=4",
          "tags": [
            "KubeVela",
            "release-note"
          ],
          "description": "The Extensible App Platform Based on Open Application Model and Kubernetes",
          "image": "https://tva1.sinaimg.cn/large/ad5fbf65gy1glgj5q8inej208g049aa6.jpg",
          "hide_table_of_contents": false
        },
        "prevItem": {
          "title": "KubeVela Performance Test - Managing Massive Applications",
          "permalink": "/blog/2021/08/30/kubevela-performance-test"
        }
      },
      "content": ">7 Dec 2020 12:33pm, by Lei Zhang and Fei Guo\r\n\r\n![image](https://tva1.sinaimg.cn/large/ad5fbf65gy1glgj5q8inej208g049aa6.jpg)\r\n\r\nLast month at KubeCon+CloudNativeCon 2020, the [Open Application Model (OAM)](https://github.com/oam-dev/spec) community launched [KubeVela](https://github.com/kubevela/kubevela/), an easy-to-use yet highly extensible application platform based on OAM and Kubernetes.\r\n\r\nFor developers, KubeVela is an easy-to-use tool that enables you to describe and ship applications to Kubernetes with minimal effort, yet for platform builders, KubeVela serves as a framework that empowers them to create developer-facing yet fully extensible platforms at ease.\r\n\r\nThe trend of cloud native technology is moving towards pursuing consistent application delivery across clouds and on-premises infrastructures using Kubernetes as the common abstraction layer. Kubernetes, although excellent in abstracting low-level infrastructure details, does introduce extra complexity to application developers, namely understanding the concepts of pods, port exposing, privilege escalation, resource claims, CRD, and so on. We’ve seen the nontrivial learning curve and the lack of developer-facing abstraction have impacted user experiences, slowed down productivity, led to unexpected errors or misconfigurations in production.\r\n\r\nAbstracting Kubernetes to serve developers’ requirements is a highly opinionated process, and the resultant abstractions would only make sense had the decision-makers been the platform builders. Unfortunately, the platform builders today face the following dilemma: There is no tool or framework for them to easily extend the abstractions if any.\r\n\r\nThus, many platforms today introduce restricted abstractions and add-on mechanisms despite the extensibility of Kubernetes. This makes easily extending such platforms for developers’ requirements or to wider scenarios almost impossible.\r\n\r\nIn the end, developers complain those platforms are too rigid and slow in response to feature requests or improvements. The platform builders do want to help but the engineering effort is daunting: any simple API change in the platform could easily become a marathon negotiation around the opinionated abstraction design.\r\n\r\n<!--truncate-->\r\n\r\n## Introducing KubeVela\r\n\r\nWith KubeVela, we aim to solve these two challenges in an approach that separates concerns of developers and platform builders.\r\n\r\nFor developers, KubeVela is an easy-to-use yet extensible tool that enables you to describe and deploy microservices applications with minimal effort. And instead of managing a handful of Kubernetes YAML files, a simple docker-compose style `appfile` is all you need.\r\n\r\n### A Sample Appfile\r\n\r\nIn this example, we will create a vela.yaml along with your app. This file describes how to build the image, how to deploy the image to Kubernetes, how to access the application and how the system would scale it automatically.\r\n\r\n```yaml\r\nname: testapp\r\n\r\nservices:\r\n    express-server:\r\n      image: oamdev/testapp:v1\r\n      build:\r\n        docker:\r\n          file: Dockerfile\r\n          context: .\r\n      cmd: [\"node\", \"server.js\"]\r\n      port: 8080\r\n      cpu: \"0.01\"\r\n\r\n      route:\r\n        domain: example.com\r\n        rules:\r\n          - path: /testapp\r\n            rewriteTarget: /\r\n\r\n      autoscale:\r\n        min: 1\r\n        max: 4\r\n        cpuPercent: 5\r\n```\r\n\r\nJust do: `$ vela up`, your app will then be alive on  https://example.com/testapp.\r\n\r\n### Behind the Appfile\r\n\r\nThe `appfile` in KubeVela does not have a fixed schema specification, instead, what you can define in this file is determined by what kind of workload types and traits are available in your platform. These two concepts are core concepts from OAM, in detail:\r\n\r\n- [Workload type](https://kubevela.io/%23/en/concepts?id=workload-type-amp-trait), which declares the characteristics that runtime infrastructure should take into account in application deployment. In the sample above, it defines a “Web Service” workload named `express-server` as part of your application.\r\n- [Trait](https://kubevela.io/%23/en/concepts?id=workload-type-amp-trait), which represents the operation configurations that are attached to an instance of workload type. Traits augment a workload type instance with operational features. In the sample above, it defines a route trait to access the application and an autoscale trait for the CPU based horizontal automatic scaling policy.\r\n\r\nWhenever a new workload type or trait is added, it would become immediately available to be declared in the `appfile`. Let’s say, a new trait named metrics is added, developers could check the schema of this trait by simply `$ vela show metrics` and define it in the previous sample `appfile`:\r\n\r\n```yaml\r\nname: testapp\r\n\r\nservices:\r\n    express-server:\r\n      type: webservice\r\n      image: oamdev/testapp:v1\r\n      build:\r\n        docker:\r\n          file: Dockerfile\r\n          contrxt: .\r\n      cmd: [\"node\", \"server.js\"]\r\n      port: 8080\r\n      cpu: \"0.01\"\r\n\r\n      route:\r\n        domain: example.com\r\n        rules:\r\n          - path: /testapp\r\n            rewriteTarget: /\r\n\r\n      autoscale:\r\n        min: 1\r\n        max: 4\r\n        cpuPercent: 5\r\n\r\n      metrices:\r\n        port: 8080\r\n        path: \"/metrics\"\r\n        scheme: \"http\"\r\n        enabled: true\r\n```\r\n\r\n## Vela Up\r\n\r\nThe `vela up` command deploys the application defined in `appfile` to Kubernetes. After deployment, you can use `vela status` to check how to access your application following the `route` trait declared in `appfile`.\r\n\r\n![](https://tvax2.sinaimg.cn/large/ad5fbf65gy1glf9pyhr42j20la0kiafn.jpg)\r\n\r\nApps deployed with KubeVela will receive a URL (and versioned pre-release URLs) with valid TLS certificate automatically generated via [cert-manager](https://cert-manager.io/docs/). KubeVela also provides a set of commands (i.e. `vela logs, vela exec`) to best support your application management without becoming a Kubernetes expert. [Learn more about vela up and appfile](https://kubevela.io/%23/en/developers/learn-appfile).\r\n\r\n## KubeVela for Platform Builders\r\n\r\nThe above experience cannot be achieved without KubeVela’s innovative offerings to the platform builders as an extensible platform engine. These features are the hidden gems that make KubeVela unique. In details, KubeVela relieves the pains of building developer facing platforms on Kubernetes by doing the following:\r\n\r\n- **Application Centric**. Behind the appfile, KubeVela enforces “application” as its main API and all KubeVela’s capabilities serve the applications’ requirements only. This is how KubeVela brings application-centric context to the platform by default and changes building such platforms into working around application architecture.\r\n- **Extending Natively**. As mentioned in the developer section, an application described by appfile is composed of various pluggable workload types and operation features (i.e. traits). Capabilities from Kubernetes ecosystem can be added to KubeVela as new workload types or traits through Kubernetes CRD registry mechanism at any time.\r\n- **Simple yet Extensible User Interface**. Behind the `appfile`, KubeVela uses [CUELang](https://github.com/cuelang/cue) as the “last mile” abstraction engine between user-facing schema and the control plane objects. KubeVela provides a set of built-in abstractions to start with and the platform builders are free to modify them at any time. Capability adding/updating or abstraction changes will all take effect at runtime, neither recompilation nor redeployment of KubeVela is required.\r\n\r\nUnder the hood, KubeVela core is built on top of Crossplane OAM Kubernetes Runtime with KEDA, Flagger, Prometheus, etc as dependencies, yet its feature pool is “unlimited” and can be extended at any time.\r\n\r\n![](https://tva2.sinaimg.cn/large/ad5fbf65gy1glf9sktkdxj20q00dsacl.jpg)\r\n\r\nWith KubeVela, platform builders now have the tooling support to design and ship any new capabilities with abstractions to end-users with high confidence and low turnaround time. And for a developer, you only need to learn these abstractions, describe the app with them in a single file, and then ship it.\r\n\r\n## Not Another PaaS System\r\n\r\nMost typical Platform-as-a-Service (PaaS) systems also provide full application management capabilities and aim to improve developer experience and efficiency. In this context, KubeVela shares the same goal.\r\n\r\nThough unlike most typical PaaS systems which are either inextensible or create their own addon systems maintained by their own communities. KubeVela is designed to fully leverage the Kubernetes ecosystems as its capability pool. Hence, there’s no additional addon system introduced in this project. For platform builders, a new capability can be installed in KubeVela at any time by simply registering its API resource to OAM and providing a CUE template. We hope and expect that with the help of the open source community, the number of the KubeVela’s capabilities will grow dramatically over time. [Learn more about using community capabilities by $vela cap](https://kubevela.io/%23/en/developers/cap-center).\r\n\r\nSo in a nutshell, KubeVela is a Kubernetes plugin for building application-centric abstractions. It leverages the native Kubernetes extensibility and capabilities to resolve a hard problem – making application management enjoyable on Kubernetes.\r\n\r\n## Learn More\r\n\r\nKubeVela is incubated by the OAM community as the successor of [Rudr](https://github.com/oam-dev/rudr) project, while rather than being a reference implementation, KubeVela intends to be an end-to-end implementation that could be used in wider scenarios. The design of KubeVela’s appfile is also part of the experimental attempt in OAM specification to bring a simplified user experience to developers.\r\n\r\nTo learn more about KubeVela, please visit KubeVela’s [documentation site](https://kubevela.io/). The following content are also good next steps:\r\n\r\n- Try out KubeVela following the [step-by-step tutorial](https://kubevela.io/%23/en/quick-start) in its Quick Start page.\r\n- Give us feedback! KubeVela is still in its early stage and we are happy to ask the community for feedback via OAM [Gitter](https://gitter.im/oam-dev/community) or [Slack channel](https://cloud-native.slack.com/archives/C01BLQ3HTJA).\r\n- [Extend KubeVela](https://kubevela.io/%23/en/platform-engineers/trait) to build your own platforms. If you have an idea for a new workload type, trait or try to build something more complex like a database or AI PaaS with KubeVela, post your idea as a GitHub Issue or propose it to the OAM community, we are eager to know.\r\n- [Contribute to KubeVela](https://github.com/kubevela/kubevela/blob/master/CONTRIBUTING.md). KubeVela is initialized by the open source community with [bootstrap contributors](https://github.com/kubevela/kubevela/blob/bbb2c527d96d3e1a0694e2f49b3d1d1168e72c53/OWNERS_ALIASES%23L35) from 8+ different organizations. We intend to donate this project to a neutral foundation as soon as it gets stable."
    }
  ]
}